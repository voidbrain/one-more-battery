(()=>{var I,Fn={18633:(I,$,l)=>{"use strict";var b=l(10467),r=l(41653),g=(l(62914),l(10222),l(50531),l(15293),l(91584),l(91806),l(22919)),at=(l(40083),l(31542),l(48631)),P=l(42804),x=l(67165),p=l(42946),z=l(7257),w=l(59352),c=l(71936);r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=(0,c.un)(t);let n=(0,r.VVh)(t);return null!=this.maxValue&&(n=(0,r.zQh)(n,0,this.maxValue)),n}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}return i.className="ReLU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=(0,c.un)(t);return(0,r.H8d)(n,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}return i.className="LeakyReLU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=(0,z.Fe)(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=(0,w.Bm)(t.alphaRegularizer),this.alphaConstraint=(0,x.YZ)(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new p.Qp(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=(0,c.U$)(t)).slice(1);if(null!=this.sharedAxes)for(const a of this.sharedAxes)e[a-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let a=1;a<t.length;++a)n[a]=t[a];this.inputSpec=[new g.eO({ndim:t.length,axes:n})],this.built=!0}call(t,e){return t=(0,c.un)(t),(0,r.NsG)(t,this.alpha.read())}getConfig(){const t={alphaInitializer:(0,z.zo)(this.alphaInitializer),alphaRegularizer:(0,w.R9)(this.alphaRegularizer),alphaConstraint:(0,x.uH)(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}return i.className="PReLU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new p.EH(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=(0,c.un)(t);return(0,r.Pqc)(n)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}return i.className="ELU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const n=(0,c.un)(t);return(0,r.lKK)(n,(0,r.wgE)((0,r.rhj)(n,this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}return i.className="ThresholdedReLU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new P.rF).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){return(0,r.DZQ)(()=>{let n=(0,c.un)(t);const a=e.mask;if(null!=a){const o=(0,r.lKK)((0,r.jbE)((0,r.SaS)(n.shape),(0,r.wgE)(a,n.dtype)),(0,r.d_2)(-1e9));n=(0,r.WQq)(n,o)}return this.axis instanceof Array?this.axis.length>1?(0,r.oNF)((0,r.jbE)(n,(0,r.VZ)(n,this.axis,!0))):this.softmax(n,this.axis[0]):this.softmax(n,this.axis)})}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Softmax",i})());var nt=l(17513),S=l(99198),M=l(9980),Y=l(51367),C=l(24503);function h(i,s){return(0,r.DZQ)(()=>((0,M.uM)(s),"channelsFirst"===s?r.mgz(i,[0,2,3,1]):i))}function y(i,s){return(0,r.DZQ)(()=>((0,M.uM)(s),"channelsFirst"===s?r.mgz(i,[0,2,3,4,1]):i))}function ht(i,s,t,e=[1,1],n="valid",a,o,u=null){return(0,r.DZQ)(()=>{if(null==a&&(a=(0,nt.VI)()),(0,M.uM)(a),3!==i.rank&&4!==i.rank)throw new p.Qp(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${i.rank}.`);if(3!==s.rank&&4!==s.rank)throw new p.Qp(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${i.rank}.`);let f=h(i,a);if("causal"===n)throw new p.EH("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return f=r.cZk.conv2d({x:f,filter:s,strides:e,pad:"same"===n?"same":"valid",dilations:o,dataFormat:"NHWC",bias:t,activation:u}),"channelsFirst"===a&&(f=r.mgz(f,[0,3,1,2])),f})}class zt extends g.Wd{constructor(s,t){if(super(t),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",zt.verifyArgs(t),this.rank=s,C.oo(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new p.EH(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=(0,Y.J)(t.kernelSize,s,"kernelSize"),this.strides=(0,Y.J)(null==t.strides?1:t.strides,s,"strides"),this.padding=null==t.padding?"valid":t.padding,(0,M.tB)(this.padding),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,M.uM)(this.dataFormat),this.activation=(0,P.b_)(t.activation),this.useBias=null==t.useBias||t.useBias,this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=(0,x.YZ)(t.biasConstraint),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.activityRegularizer=(0,w.Bm)(t.activityRegularizer),this.dilationRate=(0,Y.J)(null==t.dilationRate?1:t.dilationRate,s,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new p.Qp(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new p.Qp(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new p.Qp(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(s){if(C.vA("kernelSize"in s,"required key 'kernelSize' not in config"),"number"!=typeof s.kernelSize&&!C.HP(s.kernelSize,"number",1,3))throw new p.Qp(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(s.kernelSize)}.`)}getConfig(){const s={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:(0,P.Bu)(this.activation),useBias:this.useBias,biasInitializer:(0,z.zo)(this.biasInitializer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),biasConstraint:(0,x.uH)(this.biasConstraint)},t=super.getConfig();return Object.assign(s,t),s}}class At extends zt{constructor(s,t){super(s,t),this.kernel=null,At.verifyArgs(t),this.filters=t.filters,C.oo(this.filters,"filters"),this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=(0,x.YZ)(t.kernelConstraint),this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer)}build(s){s=(0,c.U$)(s);const t="channelsFirst"===this.dataFormat?1:s.length-1;if(null==s[t])throw new p.Qp(`The channel dimension of the input should be defined. Found ${s[t]}`);const e=s[t],n=this.kernelSize.concat([e,this.filters]);this.kernel=this.addWeight("kernel",n,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[t]:e}}],this.built=!0}call(s,t){return(0,r.DZQ)(()=>{let e;s=(0,c.un)(s);const n=null==this.bias?null:this.bias.read(),a=C.Cd(this.activation.getClassName());if(null!=a&&2===this.rank)e=ht(s,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate,a);else{if(1===this.rank)e=function W(i,s,t,e=1,n="valid",a,o=1){return(0,r.DZQ)(()=>{if(null==a&&(a=(0,nt.VI)()),(0,M.uM)(a),3!==i.shape.length)throw new p.Qp(`The input of a conv1dWithBias operation should be 3, but is ${i.shape.length} instead.`);if(3!==s.shape.length)throw new p.Qp(`The kernel for a conv1dWithBias operation should be 3, but is ${s.shape.length} instead`);if(null!=t&&1!==t.shape.length)throw new p.Qp(`The bias for a conv1dWithBias operation should be 1, but is ${t.shape.length} instead`);if("channelsFirst"===a&&(i=r.mgz(i,[0,2,1])),"causal"===n)throw new p.EH("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let u=r.kA9(i,s,e,"same"===n?"same":"valid","NWC",o);return null!=t&&(u=S.ni(u,t)),u})}(s,this.kernel.read(),n,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=ht(s,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new p.EH("convolutions greater than 3D are not implemented yet.");e=function Wt(i,s,t,e=[1,1,1],n="valid",a,o){return(0,r.DZQ)(()=>{if(null==a&&(a=(0,nt.VI)()),(0,M.uM)(a),4!==i.rank&&5!==i.rank)throw new p.Qp(`conv3dWithBias expects input to be of rank 4 or 5, but received ${i.rank}.`);if(4!==s.rank&&5!==s.rank)throw new p.Qp(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${i.rank}.`);let u=y(i,a);if("causal"===n)throw new p.EH("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return u=r.IPL(u,s,e,"same"===n?"same":"valid","NDHWC",o),null!=t&&(u=S.ni(u,t)),"channelsFirst"===a&&(u=r.mgz(u,[0,4,1,2,3])),u})}(s,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e})}computeOutputShape(s){s=(0,c.U$)(s);const t=[],e="channelsLast"===this.dataFormat?s.slice(1,s.length-1):s.slice(2);for(let a=0;a<e.length;++a){const o=(0,Y.Ol)(e[a],this.kernelSize[a],this.padding,this.strides[a],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[a]);t.push(o)}let n=[s[0]];return"channelsLast"===this.dataFormat?(n=n.concat(t),n.push(this.filters)):(n.push(this.filters),n=n.concat(t)),n}getConfig(){const s={filters:this.filters,kernelInitializer:(0,z.zo)(this.kernelInitializer),kernelRegularizer:(0,w.R9)(this.kernelRegularizer),kernelConstraint:(0,x.uH)(this.kernelConstraint)},t=super.getConfig();return Object.assign(s,t),s}static verifyArgs(s){if(!("filters"in s)||"number"!=typeof s.filters||s.filters<1)throw new p.Qp(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(s.filters)}`)}}let Bt=(()=>{class i extends At{constructor(t){super(2,t),i.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!C.HP(t.kernelSize,"number",1,2))throw new p.Qp(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}return i.className="Conv2D",i})();r.JFn.registerClass(Bt);let Pt=(()=>{class i extends At{constructor(t){super(3,t),i.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new p.Qp(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}return i.className="Conv3D",i})();r.JFn.registerClass(Pt);let Vt=(()=>{class i extends Bt{constructor(t){if(super(t),this.inputSpec=[new g.eO({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new p.Qp(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=(0,c.U$)(t)).length)throw new p.Qp("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new p.Qp("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],a=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",a,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new g.eO({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,e){return r.DZQ(()=>{let n=(0,c.un)(t);if(4!==n.shape.length)throw new p.Qp(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${n.shape.length}`);const a=n.shape;let u,f;"channelsFirst"===this.dataFormat?(u=2,f=3):(u=1,f=2);const v=a[f],T=this.kernelSize[1],G=this.strides[1],H=[a[0],(0,Y.mW)(a[u],this.strides[0],this.kernelSize[0],this.padding),(0,Y.mW)(v,G,T,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=r.mgz(n,[0,2,3,1]));let st=r.wX9(n,this.kernel.read(),H,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(st=r.mgz(st,[0,3,1,2])),null!=this.bias&&(st=S.ni(st,this.bias.read(),this.dataFormat)),null!=this.activation&&(st=this.activation.apply(st)),st})}computeOutputShape(t){const e=(t=(0,c.U$)(t)).slice();let n,a,o;"channelsFirst"===this.dataFormat?(n=1,a=2,o=3):(n=3,a=1,o=2);const u=this.kernelSize[0],f=this.kernelSize[1],m=this.strides[0],v=this.strides[1];return e[n]=this.filters,e[a]=(0,Y.mW)(e[a],m,u,this.padding),e[o]=(0,Y.mW)(e[o],v,f,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}return i.className="Conv2DTranspose",i})();r.JFn.registerClass(Vt);let Gt=(()=>{class i extends Pt{constructor(t){if(super(t),this.inputSpec=[new g.eO({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new p.Qp(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=(0,c.U$)(t)).length)throw new p.Qp("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new p.Qp("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],a=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",a,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new g.eO({ndim:5,axes:{[e]:n}})],this.built=!0}call(t,e){return r.DZQ(()=>{let n=(0,c.un)(t);if(5!==n.shape.length)throw new p.Qp(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${n.shape.length}`);const a=n.shape;let u,f,m;"channelsFirst"===this.dataFormat?(m=2,u=3,f=4):(m=1,u=2,f=3);const R=a[u],T=a[f],G=this.kernelSize[1],X=this.kernelSize[2],H=this.strides[1],st=this.strides[2],ct=[a[0],(0,Y.mW)(a[m],this.strides[0],this.kernelSize[0],this.padding),(0,Y.mW)(R,H,G,this.padding),(0,Y.mW)(T,st,X,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=r.mgz(n,[0,2,3,4,1]));let pt=r.jIJ(n,this.kernel.read(),ct,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(pt=r.mgz(pt,[0,4,1,2,3])),null!==this.bias&&(pt=S.ni(pt,this.bias.read(),this.dataFormat)),null!==this.activation&&(pt=this.activation.apply(pt)),pt})}computeOutputShape(t){const e=(t=(0,c.U$)(t)).slice();let n,a,o,u;"channelsFirst"===this.dataFormat?(n=1,a=2,o=3,u=4):(n=4,a=1,o=2,u=3);const f=this.kernelSize[0],m=this.kernelSize[1],v=this.kernelSize[2],R=this.strides[0],T=this.strides[1],F=this.strides[2];return e[n]=this.filters,e[a]=(0,Y.mW)(e[a],R,f,this.padding),e[o]=(0,Y.mW)(e[o],T,m,this.padding),e[u]=(0,Y.mW)(e[u],F,v,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}return i.className="Conv3DTranspose",i})();r.JFn.registerClass(Gt);let nn=(()=>{class i extends At{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new p.Qp("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new p.Qp("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new p.Qp(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=(0,z.Fe)(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=(0,w.Bm)(e.depthwiseRegularizer),this.depthwiseConstraint=(0,x.YZ)(e.depthwiseConstraint),this.pointwiseInitializer=(0,z.Fe)(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=(0,w.Bm)(e.pointwiseRegularizer),this.pointwiseConstraint=(0,x.YZ)(e.pointwiseConstraint)}build(t){if((t=(0,c.U$)(t)).length<this.rank+2)throw new p.Qp(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new p.Qp(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const n=t[e],a=this.kernelSize.concat([n,this.depthMultiplier]),o=[];for(let f=0;f<this.rank;++f)o.push(1);o.push(n*this.depthMultiplier,this.filters);const u=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",a,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,u,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",o,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,u,this.pointwiseConstraint),this.bias=this.useBias?this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,u,this.biasConstraint):null,this.inputSpec=[new g.eO({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,e){return(0,r.DZQ)(()=>{let n;if(t=(0,c.un)(t),1===this.rank)throw new p.EH("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=r.mgz(t,[0,2,3,1])),n=r.wdz(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(n=S.ni(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),"channelsFirst"===this.dataFormat&&(n=r.mgz(n,[0,3,1,2])),n})}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=(0,z.zo)(this.depthwiseInitializer),t.pointwiseInitializer=(0,z.zo)(this.pointwiseInitializer),t.depthwiseRegularizer=(0,w.R9)(this.depthwiseRegularizer),t.pointwiseRegularizer=(0,w.R9)(this.pointwiseRegularizer),t.depthwiseConstraint=(0,x.uH)(this.depthwiseConstraint),t.pointwiseConstraint=(0,x.uH)(this.pointwiseConstraint),t}}return i.className="SeparableConv",i})();r.JFn.registerClass((()=>{class i extends nn{constructor(t){super(2,t)}}return i.className="SeparableConv2D",i})()),r.JFn.registerClass((()=>{class i extends At{constructor(t){super(1,t),i.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!C.HP(t.kernelSize,"number",1,1))throw new p.Qp(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}return i.className="Conv1D",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.cropping="number"==typeof t.cropping?[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return(0,r.DZQ)(()=>{if(t=(0,c.un)(t),"channelsLast"===this.dataFormat){const n=S.r0(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return S.r0(n,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const n=S.r0(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return S.r0(n,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}})}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Cropping2D",i})());let _t=(()=>{class i extends g.Wd{constructor(t){super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,M.uM)(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,(0,M.uU)(this.interpolation)}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],null==t[2]?null:this.size[0]*t[2],null==t[3]?null:this.size[1]*t[3]]:[t[0],null==t[1]?null:this.size[0]*t[1],null==t[2]?null:this.size[1]*t[2],t[3]]}call(t,e){return r.DZQ(()=>{let n=(0,c.un)(t);const a=n.shape;if("channelsFirst"===this.dataFormat){n=r.mgz(n,[0,2,3,1]);const o=this.size[0]*a[2],u=this.size[1]*a[3],f="nearest"===this.interpolation?r.Slp.resizeNearestNeighbor(n,[o,u]):r.Slp.resizeBilinear(n,[o,u]);return r.mgz(f,[0,3,1,2])}{const o=this.size[0]*a[1],u=this.size[1]*a[2];return"nearest"===this.interpolation?r.Slp.resizeNearestNeighbor(n,[o,u]):r.Slp.resizeBilinear(n,[o,u])}})}getConfig(){const t={size:this.size,dataFormat:this.dataFormat,interpolation:this.interpolation},e=super.getConfig();return Object.assign(t,e),t}}return i.className="UpSampling2D",i})();r.JFn.registerClass(_t);let te=(()=>{class i extends zt{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=(0,z.Fe)(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=(0,x.YZ)(t.depthwiseConstraint),this.depthwiseRegularizer=(0,w.Bm)(t.depthwiseRegularizer)}build(t){if((t=(0,c.U$)(t)).length<4)throw new p.Qp(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new p.Qp(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e];this.depthwiseKernel=this.addWeight("depthwise_kernel",[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier],null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.bias=this.useBias?this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):null,this.built=!0}call(t,e){return(0,r.DZQ)(()=>{let n=function sn(i,s,t=[1,1],e="valid",n,a){return(0,r.DZQ)(()=>{null==n&&(n=(0,nt.VI)()),(0,M.uM)(n);let o=h(i,n);if(4!==i.rank)throw new p.Qp(`Input for depthwiseConv2d is required to be 4-D, but is instead ${i.rank}-D`);if(4!==s.rank)throw new p.Qp(`depthwiseKernel is required to be 4-D, but is instead ${s.rank}-D`);return o=r.Gl3(o,s,t,"same"===e?"same":"valid","NHWC",a),"channelsFirst"===n&&(o=r.mgz(o,[0,3,1,2])),o})}(t=(0,c.un)(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(n=S.ni(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),n})}computeOutputShape(t){t=(0,c.U$)(t);const n="channelsFirst"===this.dataFormat?t[3]:t[2],a="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,o=(0,Y.Ol)("channelsFirst"===this.dataFormat?t[2]:t[1],this.kernelSize[0],this.padding,this.strides[0]),u=(0,Y.Ol)(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],a,o,u]:[t[0],o,u,a]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=(0,z.zo)(this.depthwiseInitializer),t.depthwiseRegularizer=(0,w.R9)(this.depthwiseRegularizer),t.depthwiseConstraint=(0,x.uH)(this.depthwiseRegularizer),t}}return i.className="DepthwiseConv2D",i})();r.JFn.registerClass(te);var _=l(89119),ee=l(75768),It=l(53340);function ne(i,s,t,e){if(Array.isArray(i)){if(null!=s||null!=t)throw new p.Qp("When inputs is an array, neither initialState or constants should be provided");null!=e&&(t=i.slice(i.length-e,i.length),i=i.slice(0,i.length-e)),i.length>1&&(s=i.slice(1,i.length)),i=i[0]}function n(a){return null==a||Array.isArray(a)?a:[a]}return{inputs:i,initialState:s=n(s),constants:t=n(t)}}function se(i,s,t,e=!1,n,a,o=!1,u=!1){return r.DZQ(()=>{const f=s.shape.length;if(f<3)throw new p.Qp(`Input should be at least 3D, but is ${f}D.`);const m=[1,0].concat(_.y1(2,f));if(s=r.mgz(s,m),null!=a)throw new p.EH("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=n&&((n=r.wgE(r.wgE(n,"bool"),"float32")).rank===f-1&&(n=r.UG6(n,-1)),n=r.mgz(n,m)),e&&(s=r.BEg(s,0),null!=n&&(n=r.BEg(n,0)));const v=[];let R,T=t;const F=s.shape[0],G=r.K$i(s);let X,K;null!=n&&(X=r.K$i(n));for(let H=0;H<F;++H){const st=G[H],lt=r.DZQ(()=>i(st,T));if(null==n)R=lt[0],T=lt[1];else{const ft=r.DZQ(()=>{const ut=X[H],ct=r.jbE(r.P61(ut),ut);return{output:r.WQq(r.lKK(lt[0],ut),r.lKK(T[0],ct)),newStates:T.map((En,Nn)=>r.WQq(r.lKK(lt[1][Nn],ut),r.lKK(En,ct)))}});R=ft.output,T=ft.newStates}u&&v.push(R)}return u&&(K=r.t$z(v,1)),[R,K,T]})}let Dt=(()=>{class i extends g.Wd{constructor(t){let e;if(super(t),null==t.cell)throw new p.Qp("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new Zt({cells:t.cell}):t.cell,null==e.stateSize)throw new p.Qp("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new g.eO({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;return _.y1(0,t).map(e=>null)}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){(0,c.TT)(t)&&(t=t[0]);let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let a;if(a=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const o=[];for(const u of e)o.push([t[0],u]);return[a].concat(o)}return a}computeMask(t,e){return r.DZQ(()=>{Array.isArray(e)&&(e=e[0]);const n=this.returnSequences?e:null;if(this.returnState){const a=this.states.map(o=>null);return[n].concat(a)}return n})}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new p.EH("Constants support is not implemented in RNN yet.");(0,c.TT)(t)&&(t=t[0]);const n=this.stateful?t[0]:null,a=t.slice(2);this.inputSpec[0]=new g.eO({shape:[n,null,...a]});const o=[t[0]].concat(t.slice(2));let u;if(this.cell.build(o),u=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!r.ZSL.arraysEqual(this.stateSpec.map(f=>f.shape[f.shape.length-1]),u))throw new p.Qp(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=u.map(f=>new g.eO({shape:[null,f]}));this.stateful&&this.resetStates()}resetStates(t,e=!1){(0,r.DZQ)(()=>{if(!this.stateful)throw new p.l7("Cannot call resetStates() on an RNN Layer that is not stateful.");const n=this.inputSpec[0].shape[0];if(null==n)throw new p.Qp("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)this.states_=Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(a=>r.Ul9([n,a])):[r.Ul9([n,this.cell.stateSize])];else if(null==t)r.ASo(this.states_),null!=this.keptStates&&(r.ASo(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(a=>r.Ul9([n,a])):this.states_[0]=r.Ul9([n,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new p.Qp(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===e?this.keptStates.push(this.states_.slice()):r.ASo(this.states_);for(let a=0;a<this.states_.length;++a){const o=t[a],u=Array.isArray(this.cell.stateSize)?this.cell.stateSize[a]:this.cell.stateSize,f=[n,u];if(!r.ZSL.arraysEqual(o.shape,f))throw new p.Qp(`State ${a} is incompatible with layer ${this.name}: expected shape=${f}, received shape=${o.shape}`);this.states_[a]=o}}this.states_=this.states_.map(a=>r.aCs(a.clone()))})}apply(t,e){let n=null==e?null:e.initialState,a=null==e?null:e.constants;null==e&&(e={});const o=ne(t,n,a,this.numConstants);t=o.inputs,n=o.initialState,a=o.constants;let u=[],f=[];if(null!=n){e.initialState=n,u=u.concat(n),this.stateSpec=[];for(const v of n)this.stateSpec.push(new g.eO({shape:v.shape}));f=f.concat(this.stateSpec)}if(null!=a&&(e.constants=a,u=u.concat(a),this.numConstants=a.length),u[0]instanceof g.Ar){const v=[t].concat(u),R=this.inputSpec.concat(f),T=this.inputSpec;this.inputSpec=R;const F=super.apply(v,e);return this.inputSpec=T,F}return super.apply(t,e)}call(t,e){return(0,r.DZQ)(()=>{const n=null==e?null:e.mask,a=null==e?null:e.training;let o=null==e?null:e.initialState;t=(0,c.un)(t),null==o&&(o=this.stateful?this.states_:this.getInitialState(t));const u=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(o.length!==u)throw new p.Qp(`RNN Layer has ${u} state(s) but was passed ${o.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const f={training:a},v=se((X,K)=>{const H=this.cell.call([X].concat(K),f);return[H[0],H.slice(1)]},t,o,this.goBackwards,n,null,this.unroll,this.returnSequences),R=v[0],T=v[1],F=v[2];this.stateful&&this.resetStates(F,a);const G=this.returnSequences?T:R;return this.returnState?[G].concat(F):G})}getInitialState(t){return(0,r.DZQ)(()=>{let e=r.Ul9(t.shape);return e=r.czq(e,[1,2]),e=S.UG(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(n=>n>1?S.Vs(e,[1,n]):e):this.cell.stateSize>1?[S.Vs(e,[1,this.cell.stateSize])]:[e]})}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===i.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign(Object.assign(Object.assign({},n),t),e)}static fromConfig(t,e,n={}){const o=(0,It.i)(e.cell,n);return new t(Object.assign(e,{cell:o}))}}return i.className="RNN",i})();r.JFn.registerClass(Dt);class xt extends g.Wd{}let Ut=(()=>{class i extends xt{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,C.oo)(this.units,"units"),this.activation=(0,P.b_)(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,z.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer),this.recurrentRegularizer=(0,w.Bm)(t.recurrentRegularizer),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.kernelConstraint=(0,x.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,x.YZ)(t.recurrentConstraint),this.biasConstraint=(0,x.YZ)(t.biasConstraint),this.dropout=_.jk([1,_.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=_.jk([1,_.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,c.U$)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.bias=this.useBias?this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):null,this.built=!0}call(t,e){return(0,r.DZQ)(()=>{if(2!==t.length)throw new p.Qp(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let n=t[1];t=t[0];const a=null!=e.training&&e.training;let o;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=vt({ones:()=>r.P61(t),rate:this.dropout,training:a,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=vt({ones:()=>r.P61(n),rate:this.recurrentDropout,training:a,dropoutFunc:this.dropoutFunc}));const u=this.dropoutMask,f=this.recurrentDropoutMask;o=S.Om(null!=u?r.lKK(t,u):t,this.kernel.read()),null!=this.bias&&(o=S.ni(o,this.bias.read())),null!=f&&(n=r.lKK(n,f));let m=r.WQq(o,S.Om(n,this.recurrentKernel.read()));return null!=this.activation&&(m=this.activation.apply(m)),[m,m]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,P.Bu)(this.activation),useBias:this.useBias,kernelInitializer:(0,z.zo)(this.kernelInitializer),recurrentInitializer:(0,z.zo)(this.recurrentInitializer),biasInitializer:(0,z.zo)(this.biasInitializer),kernelRegularizer:(0,w.R9)(this.kernelRegularizer),recurrentRegularizer:(0,w.R9)(this.recurrentRegularizer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),kernelConstraint:(0,x.uH)(this.kernelConstraint),recurrentConstraint:(0,x.uH)(this.recurrentConstraint),biasConstraint:(0,x.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign(Object.assign({},t),e)}}return i.className="SimpleRNNCell",i})();r.JFn.registerClass(Ut);let ie=(()=>{class i extends Dt{constructor(t){t.cell=new Ut(t),super(t)}call(t,e){return(0,r.DZQ)(()=>(null!=this.cell.dropoutMask&&(r.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),super.call(t,{mask:null==e?null:e.mask,training:null==e?null:e.training,initialState:null==e?null:e.initialState})))}static fromConfig(t,e){return new t(e)}}return i.className="SimpleRNN",i})();r.JFn.registerClass(ie);let $t=(()=>{class i extends xt{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new p.Qp("GRUCell does not support reset_after parameter set to true.");this.units=t.units,(0,C.oo)(this.units,"units"),this.activation=(0,P.b_)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,P.b_)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,z.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer),this.recurrentRegularizer=(0,w.Bm)(t.recurrentRegularizer),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.kernelConstraint=(0,x.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,x.YZ)(t.recurrentConstraint),this.biasConstraint=(0,x.YZ)(t.biasConstraint),this.dropout=_.jk([1,_.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=_.jk([1,_.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,c.U$)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.bias=this.useBias?this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):null,this.built=!0}call(t,e){return(0,r.DZQ)(()=>{if(2!==t.length)throw new p.Qp(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const n=null!=e.training&&e.training;let a=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=vt({ones:()=>r.P61(t),rate:this.dropout,training:n,count:3,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=vt({ones:()=>r.P61(a),rate:this.recurrentDropout,training:n,count:3,dropoutFunc:this.dropoutFunc}));const u=this.recurrentDropoutMask;let f,m,v;0<this.dropout&&this.dropout<1&&(t=r.lKK(t,this.dropoutMask[0]));let R=S.Om(t,this.kernel.read());this.useBias&&(R=S.ni(R,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(a=r.lKK(a,u[0]));const T=this.recurrentKernel.read(),[F,G]=r.lDo(T,[2*this.units,this.units],T.rank-1),X=S.Om(a,F),[K,H,st]=r.lDo(R,3,R.rank-1),[lt,ft]=r.lDo(X,2,X.rank-1);f=this.recurrentActivation.apply(r.WQq(K,lt)),m=this.recurrentActivation.apply(r.WQq(H,ft));const ut=S.Om(r.lKK(m,a),G);v=this.activation.apply(r.WQq(st,ut));const ct=r.WQq(r.lKK(f,a),r.lKK(r.WQq(1,r.HZy(f)),v));return[ct,ct]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,P.Bu)(this.activation),recurrentActivation:(0,P.Bu)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,z.zo)(this.kernelInitializer),recurrentInitializer:(0,z.zo)(this.recurrentInitializer),biasInitializer:(0,z.zo)(this.biasInitializer),kernelRegularizer:(0,w.R9)(this.kernelRegularizer),recurrentRegularizer:(0,w.R9)(this.recurrentRegularizer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),kernelConstraint:(0,x.uH)(this.kernelConstraint),recurrentConstraint:(0,x.uH)(this.recurrentConstraint),biasConstraint:(0,x.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign(Object.assign({},t),e)}}return i.className="GRUCell",i})();r.JFn.registerClass($t);let re=(()=>{class i extends Dt{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new $t(t),super(t)}call(t,e){return(0,r.DZQ)(()=>(null!=this.cell.dropoutMask&&(r.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),super.call(t,{mask:null==e?null:e.mask,training:null==e?null:e.training,initialState:null==e?null:e.initialState})))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}return i.className="GRU",i})();r.JFn.registerClass(re);let Et=(()=>{class i extends xt{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,C.oo)(this.units,"units"),this.activation=(0,P.b_)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,P.b_)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,z.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer),this.recurrentRegularizer=(0,w.Bm)(t.recurrentRegularizer),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.kernelConstraint=(0,x.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,x.YZ)(t.recurrentConstraint),this.biasConstraint=(0,x.YZ)(t.biasConstraint),this.dropout=_.jk([1,_.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=_.jk([1,_.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;let a;if(t=(0,c.U$)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const o=this.biasInitializer,u=this.units;a=new((e=class extends z.H4{apply(m,v){const R=o.apply([u]),T=(new z.sN).apply([u]),F=o.apply([2*u]);return S.ly(S.ly(R,T),F)}}).className="CustomInit",e)}else a=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,a,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return(0,r.DZQ)(()=>{const n=null!=e.training&&e.training;if(3!==t.length)throw new p.Qp(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let a=t[1];const o=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=vt({ones:()=>r.P61(t),rate:this.dropout,training:n,count:4,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=vt({ones:()=>r.P61(a),rate:this.recurrentDropout,training:n,count:4,dropoutFunc:this.dropoutFunc}));const f=this.recurrentDropoutMask;let m,v,R,T;0<this.dropout&&this.dropout<1&&(t=r.lKK(t,this.dropoutMask[0]));let F=S.Om(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(a=r.lKK(a,f[0])),F=r.WQq(F,S.Om(a,this.recurrentKernel.read())),this.useBias&&(F=S.ni(F,this.bias.read()));const[G,X,K,H]=r.lDo(F,4,F.rank-1);m=this.recurrentActivation.apply(G),v=this.recurrentActivation.apply(X),R=r.WQq(r.lKK(v,o),r.lKK(m,this.activation.apply(K))),T=this.recurrentActivation.apply(H);const st=r.lKK(T,this.activation.apply(R));return[st,st,R]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,P.Bu)(this.activation),recurrentActivation:(0,P.Bu)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,z.zo)(this.kernelInitializer),recurrentInitializer:(0,z.zo)(this.recurrentInitializer),biasInitializer:(0,z.zo)(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:(0,w.R9)(this.kernelRegularizer),recurrentRegularizer:(0,w.R9)(this.recurrentRegularizer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),kernelConstraint:(0,x.uH)(this.kernelConstraint),recurrentConstraint:(0,x.uH)(this.recurrentConstraint),biasConstraint:(0,x.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign(Object.assign({},t),e)}}return i.className="LSTMCell",i})();r.JFn.registerClass(Et);let ae=(()=>{class i extends Dt{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Et(t),super(t)}call(t,e){return(0,r.DZQ)(()=>(null!=this.cell.dropoutMask&&(r.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),super.call(t,{mask:null==e?null:e.mask,training:null==e?null:e.training,initialState:null==e?null:e.initialState})))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}return i.className="LSTM",i})();r.JFn.registerClass(ae);let Zt=(()=>{class i extends xt{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return(0,r.DZQ)(()=>{let n=t.slice(1);const a=[];for(const f of this.cells.slice().reverse())Array.isArray(f.stateSize)?a.push(n.splice(0,f.stateSize.length)):a.push(n.splice(0,1));a.reverse();const o=[];let u;for(let f=0;f<this.cells.length;++f){const m=this.cells[f];n=a[f],u=0===f?[t[0]].concat(n):[u[0]].concat(n),u=m.call(u,e),o.push(u.slice(1))}n=[];for(const f of o.slice().reverse())n.push(...f);return[u[0]].concat(n)})}build(t){let e;(0,c.TT)(t)&&(t=t[0]),this.cells.forEach((n,a)=>{(0,M.IU)(`RNNCell_${a}`,()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]})}),this.built=!0}getConfig(){const t=super.getConfig(),a={cells:this.cells.map(o=>({className:o.getClassName(),config:o.getConfig()}))};return Object.assign(Object.assign({},t),a)}static fromConfig(t,e,n={}){const a=[];for(const o of e.cells)a.push((0,It.i)(o,n));return new t({cells:a})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const n of this.cells)e.push(...n.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return(0,ee.ex)(t)}setWeights(t){const e=[];for(const n of this.cells){const o=t.splice(n.weights.length);for(let u=0;u<n.weights.length;++u)e.push([n.weights[u],o[u]])}(0,ee.UM)(e)}}return i.className="StackedRNNCells",i})();function vt(i){const{ones:s,rate:t,training:e=!1,count:n=1,dropoutFunc:a}=i,o=()=>null!=a?a(s(),t):S.EZ(s(),t),u=()=>S.Ls(o,s,e);return!n||n<=1?r.aCs(u().clone()):Array(n).fill(void 0).map(u).map(m=>r.aCs(m.clone()))}r.JFn.registerClass(Zt);let an=(()=>{class i extends Dt{constructor(t){if(t.unroll)throw new p.EH("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new p.EH("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new g.eO({ndim:5})]}call(t,e){return r.DZQ(()=>{if(null!=this.cell.dropoutMask&&(r.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new p.Qp("ConvRNN2D cell does not support constants");return super.call(t,{mask:null==e?null:e.mask,training:null==e?null:e.training,initialState:null==e?null:e.initialState})})}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return r.DZQ(()=>{const{stateSize:e}=this.cell,a=this.computeSingleOutputShape(t.shape),o=[a[0],...a.slice(2)],u=r.Ul9(o);return Array.isArray(e)?Array(e.length).fill(u):[u]})}resetStates(t,e=!1){r.DZQ(()=>{if(!this.stateful)throw new p.l7("Cannot call resetStates() on an RNN Layer that is not stateful.");const n=this.inputSpec[0].shape,a=this.computeSingleOutputShape(n),o=[a[0],...a.slice(2)];if(null==n[0])throw new p.Qp("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())this.states_=Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(()=>r.Ul9(o)):[r.Ul9(o)];else if(null==t)r.ASo(this.states_),null!=this.keptStates&&(r.ASo(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(()=>r.Ul9(o)):this.states_[0]=r.Ul9(o);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new p.Qp(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);e?this.keptStates.push(this.states_.slice()):r.ASo(this.states_);for(let f=0;f<this.states_.length;++f){const m=t[f],v=o;if(!r.ZSL.arraysEqual(m.shape,v))throw new p.Qp(`State ${f} is incompatible with layer ${this.name}: expected shape=${v}, received shape=${m.shape}`);this.states_[f]=m}}this.states_=this.states_.map(f=>r.aCs(f.clone()))})}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:a,padding:o,strides:u,dilationRate:f}=this.cell,m="channelsFirst"===e,R=t[m?4:3],T=(0,Y.Ol)(t[m?3:2],a[0],o,u[0],f[0]),F=(0,Y.Ol)(R,a[1],o,u[1],f[1]);return[...t.slice(0,2),...m?[n,T,F]:[T,F,n]]}}return i.className="ConvRNN2D",i})(),Kt=(()=>{class i extends Et{constructor(t){const{filters:e,kernelSize:n,strides:a,padding:o,dataFormat:u,dilationRate:f}=t;super(Object.assign(Object.assign({},t),{units:e})),this.filters=e,(0,C.oo)(this.filters,"filters"),this.kernelSize=(0,Y.J)(n,2,"kernelSize"),this.kernelSize.forEach(m=>(0,C.oo)(m,"kernelSize")),this.strides=(0,Y.J)(a||1,2,"strides"),this.strides.forEach(m=>(0,C.oo)(m,"strides")),this.padding=o||"valid",(0,M.tB)(this.padding),this.dataFormat=u||"channelsLast",(0,M.uM)(this.dataFormat),this.dilationRate=(0,Y.J)(f||1,2,"dilationRate"),this.dilationRate.forEach(m=>(0,C.oo)(m,"dilationRate"))}build(t){var e;t=(0,c.U$)(t);const n="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[n])throw new p.Qp(`The channel dimension of the input should be defined. Found ${t[n]}`);const u=this.kernelSize.concat([t[n],4*this.filters]);this.kernel=this.addWeight("kernel",u,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const f=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",f,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let m;if(this.unitForgetBias){const v=this.biasInitializer,R=this.filters;m=new((e=class extends z.H4{apply(F,G){const X=v.apply([R]),K=r.SaS([R]),H=v.apply([2*R]);return S.u1([X,K,H])}}).className="CustomInit",e)}else m=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,m,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return r.DZQ(()=>{if(3!==t.length)throw new p.Qp(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const n=e.training||!1,a=t[0],o=t[1],u=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=vt({ones:()=>r.P61(a),rate:this.dropout,training:n,count:4,dropoutFunc:this.dropoutFunc}));const m=this.dropoutMask,v=(ds,Tn,fs)=>Tn&&Tn[fs]?r.lKK(Tn[fs],ds):ds;let R=v(a,m,0),T=v(a,m,1),F=v(a,m,2),G=v(a,m,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=vt({ones:()=>r.P61(o),rate:this.recurrentDropout,training:n,count:4,dropoutFunc:this.dropoutFunc}));const X=this.recurrentDropoutMask;let K=v(o,X,0),H=v(o,X,1),st=v(o,X,2),lt=v(o,X,3);const[ut,ct,pt,Jt]=r.lDo(this.kernel.read(),4,3),[En,Nn,$s,Zs]=this.useBias?r.lDo(this.bias.read(),4):[null,null,null,null];R=this.inputConv(R,ut,En,this.padding),T=this.inputConv(T,ct,Nn,this.padding),F=this.inputConv(F,pt,$s,this.padding),G=this.inputConv(G,Jt,Zs,this.padding);const[Ks,js,Hs,Js]=r.lDo(this.recurrentKernel.read(),4,3);K=this.recurrentConv(K,Ks),H=this.recurrentConv(H,js),st=this.recurrentConv(st,Hs),lt=this.recurrentConv(lt,Js);const Vs=this.recurrentActivation.apply(r.WQq(R,K)),Gs=this.recurrentActivation.apply(r.WQq(T,H)),hs=r.WQq(r.lKK(Gs,u),r.lKK(Vs,this.activation.apply(r.WQq(F,st)))),cs=r.lKK(this.recurrentActivation.apply(r.WQq(G,lt)),this.activation.apply(hs));return[cs,cs,hs]})}getConfig(){const n=function(i,s){var t={};for(var e in i)Object.prototype.hasOwnProperty.call(i,e)&&s.indexOf(e)<0&&(t[e]=i[e]);if(null!=i&&"function"==typeof Object.getOwnPropertySymbols){var n=0;for(e=Object.getOwnPropertySymbols(i);n<e.length;n++)s.indexOf(e[n])<0&&Object.prototype.propertyIsEnumerable.call(i,e[n])&&(t[e[n]]=i[e[n]])}return t}(super.getConfig(),["units"]),a={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign(Object.assign({},n),a)}inputConv(t,e,n,a){const o=r.Xtf(t,e,this.strides,a||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return n?S.ni(o,n,this.dataFormat):o}recurrentConv(t,e){return r.Xtf(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}return i.className="ConvLSTM2DCell",i})();r.JFn.registerClass(Kt),r.JFn.registerClass((()=>{class i extends an{constructor(t){const e=new Kt(t);super(Object.assign(Object.assign({},t),{cell:e}))}static fromConfig(t,e){return new t(e)}}return i.className="ConvLSTM2D",i})());let jt=(()=>{class i extends g.Wd{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let a=0;a<this.noiseShape.length;++a)n.push(null==this.noiseShape[a]?e[a]:this.noiseShape[a]);return n}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const n=(0,c.un)(t);if(0<this.rate&&this.rate<1){const a=null!=e.training&&e.training,o=this.getNoiseShape(n);return S.Ls(()=>S.EZ(n,this.rate,o,this.seed),()=>n,a)}return t})}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}return i.className="Dropout",i})();r.JFn.registerClass(jt),r.JFn.registerClass((()=>{class i extends jt{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}return i.className="SpatialDropout1D",i})());let ue=(()=>{class i extends g.Wd{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,(0,C.oo)(this.units,"units"),this.activation=(0,P.b_)(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=(0,x.YZ)(t.kernelConstraint),this.biasConstraint=(0,x.YZ)(t.biasConstraint),this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.activityRegularizer=(0,w.Bm)(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=(0,c.U$)(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=(0,c.U$)(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const n=(0,c.un)(t),a=(0,C.Cd)(this.activation.getClassName());let o;return null!=a?o=S.Om(n,this.kernel.read(),a,this.bias?this.bias.read():null):(o=S.Om(n,this.kernel.read()),null!=this.bias&&(o=S.ni(o,this.bias.read())),null!=this.activation&&(o=this.activation.apply(o))),o})}getConfig(){const t={units:this.units,activation:(0,P.Bu)(this.activation),useBias:this.useBias,kernelInitializer:(0,z.zo)(this.kernelInitializer),biasInitializer:(0,z.zo)(this.biasInitializer),kernelRegularizer:(0,w.R9)(this.kernelRegularizer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),kernelConstraint:(0,x.uH)(this.kernelConstraint),biasConstraint:(0,x.uH)(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Dense",i})();r.JFn.registerClass(ue);let he=(()=>{class i extends g.Wd{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=(0,c.U$)(t);for(const e of t.slice(1))if(null==e)throw new p.Qp(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],(0,_.no)(t,1)]}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);let n=(0,c.un)(t);if("channelsFirst"===this.dataFormat&&n.rank>1){const a=[0];for(let o=2;o<n.rank;++o)a.push(o);a.push(1),n=(0,r.mgz)(n,a)}return S.PS(n)})}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}return i.className="Flatten",i})();r.JFn.registerClass(he),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.supportsMasking=!0,this.activation=(0,P.b_)(t.activation)}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const n=(0,c.un)(t);return this.activation.apply(n)})}getConfig(){const t={activation:(0,P.Bu)(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Activation",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return(0,r.DZQ)(()=>(t=(0,c.un)(t),S.ux(t,this.n)))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}return i.className="RepeatVector",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.targetShape=t.targetShape;for(let e=0;e<this.targetShape.length;++e)this.isUnknown(this.targetShape[e])&&(this.targetShape[e]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",a=e.slice();let o=1,u=null;for(let m=0;m<a.length;++m){const v=a[m];if(this.isUnknown(v)){if(null!==u)throw new p.Qp("Can only specifiy one unknown dimension.");u=m}else o*=v}const f=(0,_.no)(t);if(null!==u){if(0===o||f%o!=0)throw new p.Qp(n);a[u]=f/o}else if(f!==o)throw new p.Qp(n);return a}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const n=(0,c.un)(t),a=n.shape,o=a.slice(0,1).concat(this.fixUnknownDimension(a.slice(1),this.targetShape));return(0,r.tQQ)(n,o)})}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Reshape",i})());let pe=(()=>{class i extends g.Wd{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const e=(0,_.y1)(1,t.dims.length+1);if(!r.ZSL.arraysEqual(t.dims.slice().sort(),e))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new g.eO({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=(0,c.U$)(t)).slice();return this.dims.forEach((n,a)=>{e[a+1]=t[n]}),e}call(t,e){return(0,r.mgz)((0,c.un)(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Permute",i})();r.JFn.registerClass(pe),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const n=(0,c.un)(t);return(0,r.bzn)((0,r.Ec)(n,this.maskValue),-1)}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const n=(0,c.un)(t),u=(0,r.bzn)((0,r.Ec)(n,this.maskValue),-1,!0);return(0,r.lKK)(n,(0,r.wgE)(u,n.dtype))})}}return i.className="Masking",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=null==t.inputLength?[e,null]:[e].concat(C.st(t.inputLength))}this.inputDim=t.inputDim,C.oo(this.inputDim,"inputDim"),this.outputDim=t.outputDim,C.oo(this.outputDim,"outputDim"),this.embeddingsInitializer=(0,z.Fe)(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=(0,w.Bm)(t.embeddingsRegularizer),this.activityRegularizer=(0,w.Bm)(t.activityRegularizer),this.embeddingsConstraint=(0,x.YZ)(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return(0,r.DZQ)(()=>this.maskZero?(t=(0,c.un)(t),(0,r.Ec)(t,(0,r.POl)(t))):null)}computeOutputShape(t){if(t=(0,c.U$)(t),null==this.inputLength)return[...t,this.outputDim];const e=C.st(this.inputLength);if(e.length!==t.length-1)throw new p.Qp(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let n=0;for(let a=0;a<e.length;++a){const o=e[a],u=t[a+1];if(null!=o&&null!=u&&o!==u)throw new p.Qp(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==o&&(e[n]=u),n++}}return[t[0],...e,this.outputDim]}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);let n=(0,c.un)(t);"int32"!==n.dtype&&(n=S.wg(n,"int32"));const a=S.kg(this.embeddings.read(),(0,r.tQQ)(n,[n.size]));return(0,r.tQQ)(a,(0,c.U$)(this.computeOutputShape(n.shape)))})}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:(0,z.zo)(this.embeddingsInitializer),embeddingsRegularizer:(0,w.R9)(this.embeddingsRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),embeddingsConstraint:(0,x.uH)(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Embedding",i})());var ye=l(55294);class wt extends g.Wd{constructor(s){super(s||{}),this.supportsMasking=!0}mergeFunction(s){throw new p.EH}computeElementwiseOpOutputShape(s,t){if(null==s||null==t)return null;if(s.length<t.length)return this.computeElementwiseOpOutputShape(t,s);if(0===t.length)return s;const e=s.slice(0,s.length-t.length);for(let n=0;n<t.length;++n){const a=s[s.length-t.length+n],o=t[n];if(null==a||null==o||a<0||o<0)e.push(null);else if(1===a)e.push(o);else if(1===o)e.push(a);else{if(a!==o)throw new p.Qp("Operands could not be broadcast together with shapes "+JSON.stringify(s)+" "+JSON.stringify(t));e.push(a)}}return e}build(s){if(Array.isArray(s)&&!Array.isArray(s[0])&&(s=[(0,c.U$)(s)]),s.length<2)throw new p.Qp(`A merge layer should be called on an Array of at least 2 inputs. Got ${s.length} input(s).`);let t=[];for(const a of s)null!=a&&null!==a[0]&&t.push(a[0]);if(t=C.Am(t),t.length>1)throw new p.Qp(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(s)}.`);let e=null==s[0]?null:s[0].slice(1);for(let a=1;a<s.length;++a){const o=null==s[a]?null:s[a].slice(1);e=this.computeElementwiseOpOutputShape(e,o)}const n=s.map(a=>a.length);this.reshapeRequired=-1!==s.indexOf(null)||1!==C.Am(n).length}call(s,t){return(0,r.DZQ)(()=>{if(this.reshapeRequired){const e=[],n=s.map(a=>a.rank);if(-1===n.indexOf(null)){const a=_.T9(n);for(let o of s){const u=o.rank;for(let f=0;f<a-u;++f)o=S.UG(o,1);e.push(o)}return this.mergeFunction(e)}{let a=!1;for(const f of s){const m=f.rank;if(null==m){const v=f.shape,R=v[0],T=v.slice(1).concat([R]);let F=r.tQQ(f,[R].concat(_.no(v.slice(1))));F=r.mgz(F,[1,0]),F=r.tQQ(F,T),e.push(F),a=!0}else if(m>1){const v=_.y1(1,m).concat([0]);e.push(r.mgz(f,v)),a=!0}else e.push(f)}let o=this.mergeFunction(e);const u=o.rank;if(a)if(null==u){const f=o.shape,v=f[f.length-1],R=[v].concat(f.slice(0,f.length-1));o=r.tQQ(r.mgz(r.tQQ(o,[-1,v]),[1,0]),R)}else if(u>1){const f=[u-1].concat(_.y1(0,u-1));o=r.mgz(o,f)}return o}}return this.mergeFunction(s)})}computeOutputShape(s){let t;t=null==s[0]?null:s[0].slice(1);for(let n=1;n<s.length;++n){const a=null==s[n]?null:s[n].slice(1);t=this.computeElementwiseOpOutputShape(t,a)}let e=[];for(const n of s)null!=n&&null!==n[0]&&e.push(n[0]);return e=C.Am(e),t=1===e.length?e.concat(t):[null].concat(t),t}computeMask(s,t){return r.DZQ(()=>{if(null==t)return null;if(!Array.isArray(t))throw new p.Qp("`mask` should be an Array");if(!Array.isArray(s))throw new p.Qp("`inputs` should be an Array");if(t.length!==s.length)throw new p.Qp(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${s.length} vs ${t.length})`);if(t.every(n=>null==n))return null;let e=(t=t.map(n=>null==n?n:r.UG6(n,0)))[0];for(let n=1;n<t.length-1;++n)e=r.n76(e,t[n]);return e})}}let Nt=(()=>{class i extends wt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=r.WQq(e,t[n]);return e})}}return i.className="Add",i})();r.JFn.registerClass(Nt);let Tt=(()=>{class i extends wt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=r.lKK(e,t[n]);return e})}}return i.className="Multiply",i})();r.JFn.registerClass(Tt);let Ft=(()=>{class i extends wt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=r.WQq(e,t[n]);return r.lKK(1/t.length,e)})}}return i.className="Average",i})();r.JFn.registerClass(Ft);let Lt=(()=>{class i extends wt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0];for(let n=1;n<t.length;++n)e=r.PhQ(e,t[n]);return e})}}return i.className="Maximum",i})();r.JFn.registerClass(Lt);let Mt=(()=>{class i extends wt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0];for(let n=1;n<t.length;++n)e=r.BpO(e,t[n]);return e})}}return i.className="Minimum",i})();r.JFn.registerClass(Mt);let Ot=(()=>{class i extends wt{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new p.Qp("A `Concatenate` layer should be called on a list of at least 2 inputs");let e=!0;for(const a of t)if(null!=a){e=!1;break}if(e)return;const n=[];for(let a=0;a<t.length;++a){const o=t[a].slice();o.splice(this.axis,1);let u=!1;for(const f of n)if(r.ZSL.arraysEqual(f,o)){u=!0;break}u||n.push(o)}if(n.length>1)throw new p.Qp("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return(0,r.DZQ)(()=>S.u1(t,this.axis))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new p.Qp("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),a=this.axis<0?n.length+this.axis:this.axis;for(const o of e.slice(1)){if(null==n[a]||null==o[a]){n[a]=null;break}n[a]+=o[a]}return n}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new p.Qp("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new p.Qp("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new p.Qp(`Mismatch in the length of mask (${e.length}) and the legnth of inputs (${t.length})`);return r.DZQ(()=>{let n=!0;if(e.forEach(u=>{null==u||(n=!1)}),n)return null;const a=[];for(let u=0;u<t.length;++u)a.push(null==e[u]?r.wgE(r.P61(t[u]),"bool"):e[u].rank<t[u].rank?r.UG6(e[u],-1):e[u]);const o=r.xWs(a,this.axis);return r.Q7R(o,-1,!1)})}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Concatenate",i})();function St(i,s){for(;i<0;)i+=s;return i}r.JFn.registerClass(Ot);let ve=(()=>{class i extends wt{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){r.ZSL.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),()=>"A `Dot` layer should be called on a list of exactly 2 inputs.");const e=t[0],n=t[1];if(e.length>3||n.length>3)throw new p.EH("Dot layer does not support tensors of 4D or higher rank yet.");const a=this.interpretAxes(e,n);if(e[a[0]]!==n[a[1]])throw new p.Qp(`Dimension incompatibility: ${e[a[0]]} !== ${n[a[1]]}`)}mergeFunction(t){if(2!==t.length)throw new p.Qp(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let a,e=t[0],n=t[1];return a=Array.isArray(this.axes)?this.axes.map((o,u)=>St(o,t[u].shape.length)):[St(this.axes,e.shape.length),St(this.axes,n.shape.length)],this.normalize&&(e=(0,ye.Yq)(e,a[0]),n=(0,ye.Yq)(n,a[1])),function on(i,s,t){if(i.shape.length>3||s.shape.length>3)throw new p.EH("batchDot is not implemented for tensors of 4D or higher rank yet");if(r.ZSL.assert(i.shape.length>=2,()=>`batchDot requires the rank of x to be >= 2, but got ${i.shape.length}`),r.ZSL.assert(i.shape.length>=2,()=>`batchDot requires the rank of y to be >= 2, but got ${s.shape.length}`),"number"==typeof t&&(t=[t,t]),"complex64"===i.dtype||"complex64"===s.dtype)throw new p.EH("batchDot is not implemented for complex64-type Tensors yet.");const e=i.shape.length,n=s.shape.length;null==t&&(t=[e-1,n-2]);const a=t;return r.DZQ(()=>{let o,u;if(e>n){o=e-n;const f=[];for(let m=0;m<o;++m)f.push(1);s=r.tQQ(s,s.shape.concat(f))}else if(n>e){o=n-e;const f=[];for(let m=0;m<o;++m)f.push(1);i=r.tQQ(i,i.shape.concat(f))}else o=0;if(u=2===i.shape.length&&2===s.shape.length?a[0]===a[1]?r.czq(r.lKK(i,s),a[0]):r.czq(r.lKK(r.mgz(i,[1,0]),s),a[1]):r.NoW(i,s,a[0]!==i.shape.length-1,a[1]===s.shape.length-1),o>0){let f;f=e>n?e+n-3:e-1;const m=[];for(let v=f;v<f+o;++v)m.push(v);u=r.r2V(u,m)}return 1===u.shape.length&&(u=r.UG6(u,1)),u})}(e,n,a)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[St(this.axes,t.length),St(this.axes,e.length)],n}computeOutputShape(t){r.ZSL.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),()=>"A `Dot` layer should be called on a list of exactly 2 inputs.");const e=t[0].slice(),n=t[1].slice();if(e.length>3||n.length>3)throw new p.EH("Dot layer does not support tensors of 4D or higher rank yet.");const a=this.interpretAxes(e,n);e.splice(a[0],1),n.splice(a[1],1),n.splice(0,1);const o=e.concat(n);return 1===o.length&&o.push(1),o}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Dot",i})();function Rt(i,s,t,e,n,a=.001){let o;if(2===i.rank)o=r.BFc(i,s,t,e,n,a);else if(3===i.rank)o=r.kSi(i,s,t,e,n,a);else{if(4!==i.rank)throw new p.EH(`batchNormalization is not implemented for array of rank ${i.rank} yet`);o=r.T5N(i,s,t,e,n,a)}return o}r.JFn.registerClass(ve),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const n=(0,c.un)(t);return S.Ls(()=>(0,r.WQq)(S.FE(n.shape,0,this.stddev),n),()=>n,e.training||!1)})}}return i.className="GaussianNoise",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const n=(0,c.un)(t);if(this.rate>0&&this.rate<1){const a=()=>{const o=Math.sqrt(this.rate/(1-this.rate));return(0,r.lKK)(n,S.FE(n.shape,1,o))};return S.Ls(a,()=>n,e.training||!1)}return n})}}return i.className="GaussianDropout",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||(0,c.un)(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,r.DZQ)(()=>{if(this.rate<1&&this.rate>0){const n=this._getNoiseShape(t),a=()=>{const o=(0,c.un)(t),m=-1.7580993408473766;let v=(0,r.DQN)((0,r.YeY)(n),this.rate);v=S.wg(v,"float32");const R=((1-this.rate)*(1+this.rate*m**2))**-.5,T=-R*m*this.rate,F=(0,r.WQq)((0,r.lKK)(o,v),(0,r.lKK)((0,r.WQq)(v,-1),m));return(0,r.WQq)((0,r.lKK)(F,R),T)};return S.Ls(a,()=>(0,c.un)(t),e.training||!1)}return t})}}return i.className="AlphaDropout",i})());let Ce=(()=>{class i extends g.Wd{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=(0,z.Fe)(t.betaInitializer||"zeros"),this.gammaInitializer=(0,z.Fe)(t.gammaInitializer||"ones"),this.movingMeanInitializer=(0,z.Fe)(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=(0,z.Fe)(t.movingVarianceInitializer||"ones"),this.betaConstraint=(0,x.YZ)(t.betaConstraint),this.gammaConstraint=(0,x.YZ)(t.gammaConstraint),this.betaRegularizer=(0,w.Bm)(t.betaRegularizer),this.gammaRegularizer=(0,w.Bm)(t.gammaRegularizer)}build(t){t=(0,c.U$)(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new p.Qp(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new g.eO({ndim:t.length,axes:{[e]:n}})];const a=[n];this.scale&&(this.gamma=this.addWeight("gamma",a,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",a,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",a,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",a,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,e){return(0,r.DZQ)(()=>{const n=null!=e.training&&e.training,a=(0,c.un)(t),o=a.shape,u=o.length,f=_.y1(0,u),m=this.axis>=0?this.axis:this.axis+u;f.splice(m,1);const v=C.fD(1,u);v[m]=o[m];const R=f.slice();R.sort();const T=!r.ZSL.arraysEqual(R,_.y1(0,u).slice(0,u-1));if(!n)return(()=>{if(T){const lt=(0,r.tQQ)(this.movingMean.read(),v),ft=(0,r.tQQ)(this.movingVariance.read(),v),ut=this.center?(0,r.tQQ)(this.beta.read(),v):null,ct=this.scale?(0,r.tQQ)(this.gamma.read(),v):null;return Rt(a,lt,ft,ut,ct,this.epsilon)}return Rt(a,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[G,X,K]=function hn(i,s,t,e,n=.001){return r.ZSL.arraysEqual(e.slice().sort(),_.y1(0,i.rank-1))?function ln(i,s,t,e,n=.001){return(0,r.DZQ)(()=>{const a=r.Clk(i,e),o=a.mean,u=a.variance;return[Rt(i,o,u,t,s,n),o,u]})}(i,s,t,e,n):function un(i,s,t,e,n=.001){return(0,r.DZQ)(()=>{const a=r.Clk(i,e),o=a.mean,u=a.variance,f=[];for(const G of _.y1(0,i.rank))-1!==e.indexOf(G)?f.push(1):f.push(i.shape[G]);const m=(0,r.tQQ)(o,f),v=(0,r.tQQ)(u,f),R=null==s?null:(0,r.tQQ)(s,f),T=null==t?null:(0,r.tQQ)(t,f);return[Rt(i,m,v,T,R,n),o,u]})}(i,s,t,e,n)}(a,this.gamma.read(),this.beta.read(),f,this.epsilon),H=(lt,ft,ut)=>{r.DZQ(()=>{const ct=1-ut,pt=lt.read(),Jt=r.lKK(r.jbE(pt,ft),ct);lt.write(r.jbE(pt,Jt))})};return(()=>{H(this.movingMean,X,this.momentum),H(this.movingVariance,K,this.momentum)})(),G})}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:(0,z.zo)(this.betaInitializer),gammaInitializer:(0,z.zo)(this.gammaInitializer),movingMeanInitializer:(0,z.zo)(this.movingMeanInitializer),movingVarianceInitializer:(0,z.zo)(this.movingVarianceInitializer),betaRegularizer:(0,w.R9)(this.betaRegularizer),gammaRegularizer:(0,w.R9)(this.gammaRegularizer),betaConstraint:(0,x.uH)(this.betaConstraint),gammaConstraint:(0,x.uH)(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}return i.className="BatchNormalization",i})();r.JFn.registerClass(Ce);let Ae=(()=>{class i extends g.Wd{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const e of this.axis)if(!Number.isInteger(e))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=(0,z.Fe)(t.betaInitializer||"zeros"),this.gammaInitializer=(0,z.Fe)(t.gammaInitializer||"ones"),this.betaRegularizer=(0,w.Bm)(t.betaRegularizer),this.gammaRegularizer=(0,w.Bm)(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=(0,c.U$)(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let o=0;o<this.axis.length;++o)this.axis[o]<0&&(this.axis[o]+=e);for(const o of this.axis)if(o<0||o>=e)throw new Error(`Invalid axis: ${o}`);if(this.axis.length!==C.Am(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const n=this.axis.map(o=>t[o]),a=!0;this.gamma=this.scale?this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,a):null,this.beta=this.center?this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,a):null,this.built=!0}call(t,e){const n=(0,c.un)(t),a=n.shape,o=a.length;return(0,r.DZQ)(()=>{let{mean:f,variance:m}=(0,r.Clk)(n,this.axis,!0);const v=C.fD(1,o);for(const K of this.axis)v[K]=a[K];const R=K=>null!=K&&K.shape.length!==o?r.tQQ(K,v):K;let T=this.scale?R(this.gamma.read()):null,F=this.center?R(this.beta.read()):null;const G=[],X=[];for(let K=0;K<o;++K)-1!==this.axis.indexOf(K)?(G.push(a[K]),X.push(1)):(G.push(1),X.push(a[K]));return f=r.Vsq(f,G),m=r.Vsq(m,G),null!=T&&(T=r.Vsq(T,X)),null!=F&&(F=r.Vsq(F,X)),Rt(n,f,m,F,T,this.epsilon)})}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:(0,z.zo)(this.betaInitializer),gammaInitializer:(0,z.zo)(this.gammaInitializer),betaRegularizer:(0,w.R9)(this.betaRegularizer),gammaRegularizer:(0,w.R9)(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}return i.className="LayerNormalization",i})();r.JFn.registerClass(Ae);let Ie=(()=>{class i extends g.Wd{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?(0,nt.VI)():t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new p.Qp(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new p.Qp(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new p.Qp(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new g.eO({ndim:4})]}computeOutputShape(t){let e,n;return t=(0,c.U$)(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,e){return(0,r.DZQ)(()=>function cn(i,s,t){return(0,r.DZQ)(()=>{if(4!==i.rank)throw new p.Qp(`temporalPadding expects input tensor to be 4-D, but received a ${i.rank}-D tensor.`);if(null==s&&(s=[[1,1],[1,1]]),2!==s.length||2!==s[0].length||2!==s[1].length)throw new p.Qp("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==t&&(t=(0,nt.VI)()),"channelsLast"!==t&&"channelsFirst"!==t)throw new p.Qp(`Unknown data format: ${t}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let e;return e="channelsFirst"===t?[[0,0],[0,0],s[0],s[1]]:[[0,0],s[0],s[1],[0,0]],r.eVF(i,e)})}((0,c.un)(t),this.padding,this.dataFormat))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}return i.className="ZeroPadding2D",i})();function kt(i,s,t,e,n,a){return(0,r.DZQ)(()=>{let o;(0,M.uM)(n),(0,M.Kx)(a),(0,M.tB)(e),null==t&&(t=[1,1]),null==e&&(e="valid"),null==n&&(n=(0,nt.VI)()),null==a&&(a="max"),i=h(i,n);const u="same"===e?"same":"valid";return o="max"===a?r.jgi(i,s,t,u):r.$jT(i,s,t,u),"channelsFirst"===n&&(o=r.mgz(o,[0,3,1,2])),o})}function De(i,s,t,e,n,a){return(0,r.DZQ)(()=>{let o;(0,M.uM)(n),(0,M.Kx)(a),(0,M.tB)(e),null==t&&(t=[1,1,1]),null==e&&(e="valid"),null==n&&(n=(0,nt.VI)()),null==a&&(a="max"),i=y(i,n);const u="same"===e?"same":"valid";return o="max"===a?r.NYV(i,s,t,u):r.sub(i,s,t,u),"channelsFirst"===n&&(o=r.mgz(o,[0,4,1,2,3])),o})}r.JFn.registerClass(Ie);class Se extends g.Wd{constructor(s){if(null==s.poolSize&&(s.poolSize=2),super(s),"number"==typeof s.poolSize)this.poolSize=[s.poolSize];else{if(!Array.isArray(s.poolSize)||1!==s.poolSize.length||"number"!=typeof s.poolSize[0])throw new p.Qp(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(s.poolSize)}`);this.poolSize=s.poolSize}if((0,C.oo)(this.poolSize,"poolSize"),null==s.strides)this.strides=this.poolSize;else if("number"==typeof s.strides)this.strides=[s.strides];else{if(!Array.isArray(s.strides)||1!==s.strides.length||"number"!=typeof s.strides[0])throw new p.Qp(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(s.strides)}`);this.strides=s.strides}(0,C.oo)(this.strides,"strides"),this.padding=null==s.padding?"valid":s.padding,(0,M.tB)(this.padding),this.inputSpec=[new g.eO({ndim:3})]}computeOutputShape(s){s=(0,c.U$)(s);const t=(0,Y.Ol)(s[1],this.poolSize[0],this.padding,this.strides[0]);return[s[0],t,s[2]]}call(s,t){return(0,r.DZQ)(()=>{this.invokeCallHook(s,t),s=S.UG((0,c.un)(s),2);const e=this.poolingFunction((0,c.un)(s),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return r.r2V(e,[2])})}getConfig(){const s={poolSize:this.poolSize,padding:this.padding,strides:this.strides},t=super.getConfig();return Object.assign(s,t),s}}r.JFn.registerClass((()=>{class i extends Se{constructor(t){super(t)}poolingFunction(t,e,n,a,o){return(0,M.uM)(o),(0,M.tB)(a),kt(t,e,n,a,o,"max")}}return i.className="MaxPooling1D",i})()),r.JFn.registerClass((()=>{class i extends Se{constructor(t){super(t)}poolingFunction(t,e,n,a,o){return(0,M.uM)(o),(0,M.tB)(a),kt(t,e,n,a,o,"avg")}}return i.className="AveragePooling1D",i})());class Ee extends g.Wd{constructor(s){if(null==s.poolSize&&(s.poolSize=[2,2]),super(s),this.poolSize=Array.isArray(s.poolSize)?s.poolSize:[s.poolSize,s.poolSize],null==s.strides)this.strides=this.poolSize;else if(Array.isArray(s.strides)){if(2!==s.strides.length)throw new p.Qp(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${s.strides.length}.`);this.strides=s.strides}else this.strides=[s.strides,s.strides];(0,C.oo)(this.poolSize,"poolSize"),(0,C.oo)(this.strides,"strides"),this.padding=null==s.padding?"valid":s.padding,this.dataFormat=null==s.dataFormat?"channelsLast":s.dataFormat,(0,M.uM)(this.dataFormat),(0,M.tB)(this.padding),this.inputSpec=[new g.eO({ndim:4})]}computeOutputShape(s){s=(0,c.U$)(s);let t="channelsFirst"===this.dataFormat?s[2]:s[1],e="channelsFirst"===this.dataFormat?s[3]:s[2];return t=(0,Y.Ol)(t,this.poolSize[0],this.padding,this.strides[0]),e=(0,Y.Ol)(e,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[s[0],s[1],t,e]:[s[0],t,e,s[3]]}call(s,t){return(0,r.DZQ)(()=>(this.invokeCallHook(s,t),this.poolingFunction((0,c.un)(s),this.poolSize,this.strides,this.padding,this.dataFormat)))}getConfig(){const s={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(s,t),s}}r.JFn.registerClass((()=>{class i extends Ee{constructor(t){super(t)}poolingFunction(t,e,n,a,o){return(0,M.uM)(o),(0,M.tB)(a),kt(t,e,n,a,o,"max")}}return i.className="MaxPooling2D",i})()),r.JFn.registerClass((()=>{class i extends Ee{constructor(t){super(t)}poolingFunction(t,e,n,a,o){return(0,M.uM)(o),(0,M.tB)(a),kt(t,e,n,a,o,"avg")}}return i.className="AveragePooling2D",i})());class Fe extends g.Wd{constructor(s){if(null==s.poolSize&&(s.poolSize=[2,2,2]),super(s),this.poolSize=Array.isArray(s.poolSize)?s.poolSize:[s.poolSize,s.poolSize,s.poolSize],null==s.strides)this.strides=this.poolSize;else if(Array.isArray(s.strides)){if(3!==s.strides.length)throw new p.Qp(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${s.strides.length}.`);this.strides=s.strides}else this.strides=[s.strides,s.strides,s.strides];(0,C.oo)(this.poolSize,"poolSize"),(0,C.oo)(this.strides,"strides"),this.padding=null==s.padding?"valid":s.padding,this.dataFormat=null==s.dataFormat?"channelsLast":s.dataFormat,(0,M.uM)(this.dataFormat),(0,M.tB)(this.padding),this.inputSpec=[new g.eO({ndim:5})]}computeOutputShape(s){s=(0,c.U$)(s);let t="channelsFirst"===this.dataFormat?s[2]:s[1],e="channelsFirst"===this.dataFormat?s[3]:s[2],n="channelsFirst"===this.dataFormat?s[4]:s[3];return t=(0,Y.Ol)(t,this.poolSize[0],this.padding,this.strides[0]),e=(0,Y.Ol)(e,this.poolSize[1],this.padding,this.strides[1]),n=(0,Y.Ol)(n,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[s[0],s[1],t,e,n]:[s[0],t,e,n,s[4]]}call(s,t){return(0,r.DZQ)(()=>(this.invokeCallHook(s,t),this.poolingFunction((0,c.un)(s),this.poolSize,this.strides,this.padding,this.dataFormat)))}getConfig(){const s={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(s,t),s}}r.JFn.registerClass((()=>{class i extends Fe{constructor(t){super(t)}poolingFunction(t,e,n,a,o){return(0,M.uM)(o),(0,M.tB)(a),De(t,e,n,a,o,"max")}}return i.className="MaxPooling3D",i})()),r.JFn.registerClass((()=>{class i extends Fe{constructor(t){super(t)}poolingFunction(t,e,n,a,o){return(0,M.uM)(o),(0,M.tB)(a),De(t,e,n,a,o,"avg")}}return i.className="AveragePooling3D",i})());class Oe extends g.Wd{constructor(s){super(s),this.inputSpec=[new g.eO({ndim:3})]}computeOutputShape(s){return[s[0],s[2]]}call(s,t){throw new p.EH}}let ke=(()=>{class i extends Oe{constructor(t){super(t||{})}call(t,e){return(0,r.DZQ)(()=>{const n=(0,c.un)(t);return r.i2o(n,1)})}}return i.className="GlobalAveragePooling1D",i})();r.JFn.registerClass(ke);let Qe=(()=>{class i extends Oe{constructor(t){super(t||{})}call(t,e){return(0,r.DZQ)(()=>{const n=(0,c.un)(t);return r.T9B(n,1)})}}return i.className="GlobalMaxPooling1D",i})();r.JFn.registerClass(Qe);class We extends g.Wd{constructor(s){super(s),this.dataFormat=null==s.dataFormat?"channelsLast":s.dataFormat,(0,M.uM)(this.dataFormat),this.inputSpec=[new g.eO({ndim:4})]}computeOutputShape(s){return"channelsLast"===this.dataFormat?[s[0],s[3]]:[s[0],s[1]]}call(s,t){throw new p.EH}getConfig(){const s={dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(s,t),s}}let Be=(()=>{class i extends We{call(t,e){return(0,r.DZQ)(()=>{const n=(0,c.un)(t);return r.i2o(n,"channelsLast"===this.dataFormat?[1,2]:[2,3])})}}return i.className="GlobalAveragePooling2D",i})();r.JFn.registerClass(Be);let Pe=(()=>{class i extends We{call(t,e){return(0,r.DZQ)(()=>{const n=(0,c.un)(t);return r.T9B(n,"channelsLast"===this.dataFormat?[1,2]:[2,3])})}}return i.className="GlobalMaxPooling2D",i})();r.JFn.registerClass(Pe);var dn=l(89893);class Ue extends g.Wd{constructor(s){super(s),this.layer=s.layer}build(s){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(s){null!=this.layer&&(this.layer.trainable=s)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(s){this.layer.setWeights(s)}getConfig(){const s={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},t=super.getConfig();return Object.assign(s,t),s}setFastWeightInitDuringBuild(s){super.setFastWeightInitDuringBuild(s),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(s)}static fromConfig(s,t,e={}){const a=(0,It.i)(t.layer,e);delete t.layer;const o={layer:a};return Object.assign(o,t),new s(o)}}r.JFn.registerClass((()=>{class i extends Ue{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=(0,c.U$)(t)).length<3)throw new p.Qp(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=(0,c.U$)(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e);return[n[0],t[1]].concat(n.slice(1))}call(t,e){return(0,r.DZQ)(()=>se((u,f)=>[(0,c.un)(this.layer.call(u,e)),[]],t=(0,c.un)(t),[],!1,null,null,!1,!0)[1])}}return i.className="TimeDistributed",i})());let Ze=(()=>{class i extends Ue{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=(0,It.i)(n),e.goBackwards=!0!==e.goBackwards;const a={};if(a.className=t.layer.getClassName(),a.config=e,this.backwardLayer=(0,It.i)(a),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,function fn(i){C.E6(dn.r$,"BidirectionalMergeMode",i)}(this.mergeMode),t.weights)throw new p.EH("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const n=Math.floor(t.length/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let n,a,o,e=this.forwardLayer.computeOutputShape(t);return Array.isArray(e)&&Array.isArray(e[0])||(e=[e]),this.returnState&&(o=e.slice(1)),n=e[0],"concat"===this.mergeMode?(n[n.length-1]*=2,a=[n]):a=null==this.mergeMode?[n,n.slice()]:[n],this.returnState?null==this.mergeMode?a.concat(o).concat(o.slice()):[n].concat(o).concat(o.slice()):C.wL(a)}apply(t,e){let n=null==e?null:e.initialState,a=null==e?null:e.constants;null==e&&(e={});const o=ne(t,n,a,this.numConstants);if(t=o.inputs,n=o.initialState,a=o.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==a)return super.apply(t,e);const u=[],f=[];if(null!=n){const v=n.length;if(v%2>0)throw new p.Qp("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,u.push(...n);const R=n.map(T=>new g.eO({shape:T.shape}));this.forwardLayer.stateSpec=R.slice(0,v/2),this.backwardLayer.stateSpec=R.slice(v/2),f.push(...R)}if(null!=a)throw new p.EH("Support for constants in Bidirectional layers is not implemented yet.");const m=u[0]instanceof g.Ar;for(const v of u)if(v instanceof g.Ar!==m)throw new p.Qp("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(m){const v=[t].concat(u),R=this.inputSpec.concat(f),T=this.inputSpec;this.inputSpec=R;const F=super.apply(v,e);return this.inputSpec=T,F}return super.apply(t,e)}call(t,e){return(0,r.DZQ)(()=>{const n=e.initialState;let a,o,u,f;if(null==n)a=this.forwardLayer.call(t,e),o=this.backwardLayer.call(t,e);else{const m=n.slice(0,n.length/2),v=n.slice(n.length/2);a=this.forwardLayer.call(t,Object.assign(e,{initialState:m})),o=this.backwardLayer.call(t,Object.assign(e,{initialState:v}))}return this.returnState&&(Array.isArray(a)&&(u=a.slice(1).concat(o.slice(1))),a=a[0],o=o[0]),this.returnSequences&&(o=r.BEg(o,1)),"concat"===this.mergeMode?f=S.u1([a,o]):"sum"===this.mergeMode?f=r.WQq(a,o):"ave"===this.mergeMode?f=r.lKK(.5,r.WQq(a,o)):"mul"===this.mergeMode?f=r.lKK(a,o):null==this.mergeMode&&(f=[a,o]),this.returnState?null==this.mergeMode?f.concat(u):[f].concat(u):f})}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){(0,M.IU)(this.forwardLayer.name,()=>{this.forwardLayer.build(t)}),(0,M.IU)(this.backwardLayer.name,()=>{this.backwardLayer.build(t)}),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const o=this.forwardLayer.states.map(u=>null);return Array.isArray(n)?n.concat(o).concat(o):[n].concat(o).concat(o)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=(0,It.i)(e.layer);if(delete e.layer,null!=e.numConstants)throw new p.EH("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const a=e;return a.layer=n,new t(a)}}return i.className="Bidirectional",i})();r.JFn.registerClass(Ze),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.scale=t.scale,this.offset=t.offset?t.offset:0}getConfig(){const t={scale:this.scale,offset:this.offset},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,r.DZQ)(()=>("float32"!==(t=(0,c.un)(t)).dtype&&(t=S.wg(t,"float32")),(0,r.WQq)((0,r.lKK)(t,this.scale),this.offset)))}}return i.className="Rescaling",i})());const{resizeBilinear:mn,cropAndResize:gn}=r.Slp;r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.height=t.height,this.width=t.width}centerCrop(t,e,n,a,o,u,f,m){return(0,r.DZQ)(()=>{let v,R=!1;const K=[e/u,n/f,(a+e)/u,(o+n)/f],H=[];3===t.rank?(R=!0,v=(0,r.t$z)([t])):v=t;for(let ct=0;ct<v.shape[0];ct++)H.push(K);const st=(0,r.OEK)(H,[H.length,4]),lt=(0,r.y17)(0,H.length,1,"int32"),ut=gn(v,st,lt,[a,o],"nearest");return S.wg(R?(0,c.un)((0,r.K$i)(ut)):ut,m)})}upsize(t,e,n,a){return(0,r.DZQ)(()=>{const o=mn(t,[e,n]);return S.wg(o,a)})}call(t,e){return(0,r.DZQ)(()=>{const n=(0,c.un)(t),a=n.dtype,o=n.shape,u=o[o.length-3],f=o[o.length-2];let m=0;u!==this.height&&(m=Math.floor((u-this.height)/2));let v=0;return f!==this.width&&(v=Math.floor((f-this.width)/2),0===v&&(v=1)),m>=0&&v>=0?this.centerCrop(n,m,v,this.height,this.width,u,f,a):this.upsize(t,this.height,this.width,a)})}getConfig(){const t={height:this.height,width:this.width},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){const n=(t=(0,c.U$)(t)).length-2;return t[t.length-3]=this.height,t[n]=this.width,t}}return i.className="CenterCrop",i})());var yn=l(97301);r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.numTokens=t.numTokens,this.outputMode=t.outputMode?t.outputMode:"multiHot"}getConfig(){const t={numTokens:this.numTokens,outputMode:this.outputMode},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){return null==(t=(0,c.U$)(t))?[this.numTokens]:"oneHot"===this.outputMode&&1!==t[t.length-1]?(t.push(this.numTokens),t):(t[t.length-1]=this.numTokens,t)}call(t,e){return(0,r.DZQ)(()=>{let n;if("int32"!==(t=(0,c.un)(t)).dtype&&(t=S.wg(t,"int32")),typeof e.countWeights<"u"){if("count"!==this.outputMode)throw new p.Qp(`countWeights is not used when outputMode !== count.\n              Received countWeights=${e.countWeights}`);n=(0,c.un)(e.countWeights)}const a=(0,r.T9B)(t),o=(0,r.jkA)(t),u=(0,r.rhj)(this.numTokens,a).bufferSync().get(0),f=(0,r.DQN)(o,0).bufferSync().get(0);if(!u||!f)throw new p.Qp(`Input values must be between 0 < values <= numTokens with numTokens=${this.numTokens}`);return yn.w(t,this.outputMode,this.numTokens,n)})}}return i.className="CategoryEncoding",i})());const Je=new Set(["bilinear","nearest"]);let Ve=(()=>{class i extends g.Wd{constructor(t){if(super(t),this.height=t.height,this.width=t.width,t.interpolation){if(!Je.has(t.interpolation))throw new p.Qp(`Invalid interpolation parameter: ${t.interpolation} is not implemented`);this.interpolation=t.interpolation}else this.interpolation="bilinear";this.cropToAspectRatio=!!t.cropToAspectRatio}computeOutputShape(t){return t=(0,c.U$)(t),[this.height,this.width,t[2]]}getConfig(){const t={height:this.height,width:this.width,interpolation:this.interpolation,cropToAspectRatio:this.cropToAspectRatio},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,r.DZQ)(()=>{const n=[this.height,this.width];if("bilinear"===this.interpolation)return r.Slp.resizeBilinear(t,n,!this.cropToAspectRatio);if("nearest"===this.interpolation)return r.Slp.resizeNearestNeighbor(t,n,!this.cropToAspectRatio);throw new Error(`Interpolation is ${this.interpolation} but only ${[...Je]} are supported`)})}}return i.className="Resizing",i})();r.JFn.registerClass(Ve);var bn=l(29947);const Ge=new Set(["bilinear","nearest"]);let Ye=(()=>{class i extends bn.W{constructor(t){super(t);const{factor:e,interpolation:n="bilinear"}=t;if(this.factor=e,Array.isArray(this.factor)&&2===this.factor.length)this.widthLower=this.factor[0],this.widthUpper=this.factor[1];else{if(Array.isArray(this.factor)||!(this.factor>0))throw new p.Qp(`Invalid factor: ${this.factor}. Must be positive number or tuple of 2 numbers`);this.widthLower=-this.factor,this.widthUpper=this.factor}if(this.widthLower<-1||this.widthUpper<-1)throw new p.Qp(`factor must have values larger than -1. Got: ${this.factor}`);if(this.widthUpper<this.widthLower)throw new p.Qp(`factor cannot have upper bound less than lower bound.\n        Got upper bound: ${this.widthUpper}.\n        Got lower bound: ${this.widthLower}\n      `);if(n){if(!Ge.has(n))throw new p.Qp(`Invalid interpolation parameter: ${n} is not implemented`);this.interpolation=n}}getConfig(){const t={factor:this.factor,interpolation:this.interpolation},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){return t=(0,c.U$)(t),[this.imgHeight,-1,t[2]]}call(t,e){return(0,r.DZQ)(()=>{const n=(0,c.un)(t);this.imgHeight=n.shape[n.shape.length-3];const a=n.shape[n.shape.length-2];this.widthFactor=(0,r.YeY)([1],1+this.widthLower,1+this.widthUpper,"float32",this.randomGenerator.next());let o=this.widthFactor.dataSync()[0]*a;o=Math.round(o);const u=[this.imgHeight,o];switch(this.interpolation){case"bilinear":return r.Slp.resizeBilinear(t,u);case"nearest":return r.Slp.resizeNearestNeighbor(t,u);default:throw new Error(`Interpolation is ${this.interpolation}\n          but only ${[...Ge]} are supported`)}})}}return i.className="RandomWidth",i})();function Xe(i){return new ue(i)}r.JFn.registerClass(Ye),l(39416),l(40183),l(73112),l(30593),l(13509),l(30744);var Ds=l(48814),Ct=l(74042),qe=l(27669);let _e=(()=>{class i{constructor(){this.size=null}batch(t,e=!0){const n=this;let a;return r.ZSL.assert(t>0,()=>`batchSize needs to be positive, but it is\n      ${t}`),a=this.size===1/0||null==this.size?this.size:e?Math.ceil(this.size/t):Math.floor(this.size/t),dt((0,b.A)(function*(){return(yield n.iterator()).columnMajorBatch(t,e,Ss)}),a)}concatenate(t){const e=this;let n;return n=this.size===1/0||t.size===1/0?1/0:null!=this.size&&null!=t.size?this.size+t.size:null,dt((0,b.A)(function*(){return(yield e.iterator()).concatenate(yield t.iterator())}),n)}filter(t){const e=this;let n;return n=this.size===1/0?1/0:null,dt((0,b.A)(function*(){return(yield e.iterator()).filter(a=>r.DZQ(()=>t(a)))}),n)}forEachAsync(t){var e=this;return(0,b.A)(function*(){return(yield e.iterator()).forEachAsync(t)})()}map(t){const e=this;return dt((0,b.A)(function*(){return(yield e.iterator()).map(n=>r.DZQ(()=>t(n)))}),this.size)}mapAsync(t){const e=this;return dt((0,b.A)(function*(){return(yield e.iterator()).mapAsync(t)}),this.size)}prefetch(t){if(null==t)throw new RangeError("`Dataset.prefetch()` requires bufferSize to be specified.");const e=this;return dt((0,b.A)(function*(){return(yield e.iterator()).prefetch(t)}),this.size)}repeat(t){const e=this;let n;return n=null!=this.size&&t>0?this.size*t:0===t?0:null!=this.size&&(void 0===t||t<0)?1/0:null,dt((0,b.A)(function*(){const a=(0,Ct.ht)((0,b.A)(function*(){return{value:yield e.iterator(),done:!1}}));return(0,Ct.kP)(a.take(t))}),n)}skip(t){const e=this;let n;return n=null!=this.size&&t>=0&&this.size>=t?this.size-t:null!=this.size&&(this.size<t||void 0===t||t<0)?0:null,dt((0,b.A)(function*(){return(yield e.iterator()).skip(t)}),n)}shuffle(t,e,n=!0){if(null==t||t<0)throw null==this.size?new RangeError("`Dataset.shuffle()` requires bufferSize to be specified."):new RangeError(`\`Dataset.shuffle()\` requires bufferSize to be specified.  If your data fits in main memory (for regular JS objects), and/or GPU memory (for \`tf.Tensor\`s), consider setting bufferSize to the dataset size (${this.size} elements)`);const a=this,o=Ds.alea(e||r.ZSL.now().toString());return dt((0,b.A)(function*(){let u=o.int32();return n&&(u+=o.int32()),(yield a.iterator()).shuffle(t,u.toString())}),this.size)}take(t){const e=this;let n;return n=null!=this.size&&this.size>t?t:null!=this.size&&this.size<=t?this.size:null,dt((0,b.A)(function*(){return(yield e.iterator()).take(t)}),n)}toArray(){var t=this;return(0,b.A)(function*(){if(t.size===1/0)throw new Error("Can not convert infinite data stream to array.");return(yield t.iterator()).toArray()})()}toArrayForTest(){var t=this;return(0,b.A)(function*(){if(t.size===1/0)throw new Error("Can not convert infinite data stream to array.");return(yield t.iterator()).toArrayForTest()})()}}return i.MAX_BUFFER_SIZE=1e4,i})();function dt(i,s=null){return new class extends _e{constructor(){super(...arguments),this.size=s}iterator(){return(0,b.A)(function*(){return i()})()}}}function Ss(i){return null===i?null:(0,qe.mf)(i[0])?{value:Rs(i),recurse:!1}:{value:null,recurse:!0}}function Rs(i){if(0===i.length)throw new Error("Can't make a batch of zero elements.");return i[0]instanceof r.qYS?r.t$z(i):r.OEK(i)}Symbol("out"),Symbol("field"),Symbol("quote"),Symbol("quoteafterquote"),Symbol("quoteinquote"),l(4902),l(11033),l(41014),l(8084),l(98603),l(21750),l(22412),l(77816),r.gYU();function xn(){return(xn=(0,b.A)(function*(){const s=yield(yield fetch("/one-more-battery/assets/data/mnist_images.png")).blob(),t=yield createImageBitmap(s),e=yield fetch("/one-more-battery/assets/data/mnist_labels_uint8"),n=new Uint8Array(yield e.arrayBuffer()),a=n.length,o=new Float32Array(28*a*28),f=new OffscreenCanvas(28,28).getContext("2d"),m=Math.floor(t.width/28);for(let v=0;v<a;v++){const R=28*v*28,T=Math.floor(v/m);f.drawImage(t,v%m*28,28*T,28,28,0,0,28,28);const K=f.getImageData(0,0,28,28);for(let H=0;H<K.data.length/4;H++)o[R+H]=K.data[4*H]/255}return{images:o,labels:n}})).apply(this,arguments)}addEventListener("message",function(){var i=(0,b.A)(function*({}){postMessage({type:"log",message:"Setting backend to cpu..."}),yield r.jh6("cpu"),postMessage({type:"log",message:"Backend set."}),postMessage({type:"log",message:"Loading data..."});const t=performance.now(),{images:e,labels:n}=yield function Bs(){return xn.apply(this,arguments)}(),a=performance.now();postMessage({type:"log",message:`Data loaded in ${(a-t).toFixed(2)} ms.`}),postMessage({type:"log",message:"Creating model..."});const o=function Us(){const i=function tt(i){return new at.Gx(i)}();return i.add(function wn(i){return new he(i)}({inputShape:[28,28]})),i.add(Xe({units:128,activation:"relu",kernelInitializer:"varianceScaling"})),i.add(Xe({units:10,kernelInitializer:"varianceScaling",activation:"softmax"})),i.compile({optimizer:r.BaG.adam(),loss:"categoricalCrossentropy",metrics:["accuracy"]}),i}();postMessage({type:"log",message:"Model created."}),postMessage({type:"log",message:"Starting training..."});const u=function Ms(i){return dt((0,b.A)(function*(){const s=yield i();return(0,Ct.ht)(()=>s.next())}))}(()=>function*Ps(i,s){const t=s.length;for(let e=0;e<t;e++){const n=28*e*28,a=i.slice(n,n+784),o=r.KtR(a,[28,28]).reshape([1,28,28]),u=r.Mw0(r.tGX([s[e]],"int32"),10).squeeze();yield{xs:o,ys:u}}}(e,n)).shuffle(1e3).batch(512);yield o.fitDataset(u,{epochs:20,callbacks:{onEpochEnd:(f,m)=>{f%5==0&&postMessage({type:"log",message:`Epoch ${f+1}: loss = ${m.loss.toFixed(4)}, acc = ${m.acc.toFixed(4)}`})}}}),postMessage({type:"log",message:"Training complete."}),yield o.save("downloads://model"),postMessage({type:"done"})});return function(s){return i.apply(this,arguments)}}())},47054:(I,$,l)=>{"use strict";l.d($,{AS:()=>tt,DZ:()=>D,Hi:()=>Z,Hs:()=>L,aC:()=>k,gJ:()=>E,gY:()=>B,jh:()=>P,jz:()=>p,m1:()=>it});var b=l(611),r=l(25271),O=l(73444),J=l(14548);function B(){(0,r._K)().set("DEBUG",!0)}function Z(){return b.T2}function it(){return b.T2.memory()}function D(Q,q){return b.T2.tidy(Q,q)}function tt(Q){(0,J.getTensorsInContainer)(Q).forEach(ot=>ot.dispose())}function k(Q){return b.T2.keep(Q)}function P(Q){return b.T2.setBackend(Q)}function p(){return b.T2.backendName}function E(Q,q,ot=1){return b.T2.registerBackend(Q,q,ot)}function L(){return b.T2.backend}(0,O.B4)(function j(Q){(0,r._K)().getBool("DEPRECATION_WARNINGS_ENABLED")&&console.warn(Q+" You can disable deprecation warnings with tf.disableDeprecationWarnings().")})},41653:(I,$,l)=>{"use strict";l.d($,{ljI:()=>d.ljI,Vvy:()=>d.Vvy,PH8:()=>d.PH8,OMN:()=>d.OMN,EkD:()=>d.EkD,u8Z:()=>d.u8Z,FSt:()=>d.FSt,Jp_:()=>d.Jp_,p_m:()=>d.p_m,QKF:()=>d.QKF,epO:()=>d.epO,TyE:()=>d.TyE,lxb:()=>d.lxb,zP9:()=>d.zP9,ho8:()=>d.ho8,cS:()=>d.cS,wwC:()=>d.wwC,VCH:()=>d.VCH,jAQ:()=>d.jAQ,Ik2:()=>d.Ik2,N4F:()=>d.N4F,HNs:()=>d.HNs,vj7:()=>d.vj7,KXH:()=>d.KXH,QDP:()=>d.QDP,vaV:()=>d.vaV,pr3:()=>d.pr3,$zE:()=>d.$zE,$dB:()=>d.$dB,p2J:()=>d.p2J,rFm:()=>d.rFm,jfg:()=>d.jfg,A1h:()=>d.A1h,iGz:()=>d.iGz,gC7:()=>d.gC7,Mn0:()=>d.Mn0,MnK:()=>d.MnK,MRQ:()=>d.MRQ,jj_:()=>d.jj_,nY8:()=>d.nY8,GJx:()=>Cn.GJ,wNW:()=>d.wNW,TMz:()=>d.TMz,tGH:()=>d.tGH,X$8:()=>d.X$8,nVu:()=>d.nVu,ORI:()=>d.ORI,jxD:()=>d.jxD,pk0:()=>d.pk0,bP9:()=>d.bP9,XmO:()=>d.XmO,Qgm:()=>d.Qgm,Pah:()=>d.Pah,rsH:()=>d.rsH,BRl:()=>d.BRl,_s9:()=>d._s9,ox3:()=>d.ox3,ybN:()=>d.ybN,ybj:()=>d.ybj,rGP:()=>d.rGP,SQl:()=>d.SQl,BxF:()=>d.BxF,ZgB:()=>d.ZgB,ElG:()=>d.ElG,awo:()=>d.awo,i5R:()=>d.i5R,aAr:()=>d.aAr,T7M:()=>d.T7M,O4G:()=>d.O4G,mxL:()=>d.mxL,XhZ:()=>d.XhZ,lLS:()=>d.lLS,OAQ:()=>d.OAQ,lzr:()=>d.lzr,dv8:()=>d.dv8,gIW:()=>d.gIW,E3$:()=>d.E3$,iPs:()=>d.iPs,uI_:()=>Cn.uI,jM4:()=>d.jM4,ToN:()=>d.ToN,X0$:()=>d.X0$,mIA:()=>d.mIA,CwD:()=>d.CwD,mnI:()=>d.mnI,tG8:()=>d.tG8,Cg$:()=>d.Cg$,RUm:()=>d.RUm,nZd:()=>d.nZd,LXA:()=>d.LXA,VAI:()=>d.VAI,t3d:()=>d.t3d,ySp:()=>d.ySp,cHb:()=>d.cHb,RXX:()=>d.RXX,TL8:()=>d.TL8,LDN:()=>d.LDN,g5A:()=>d.g5A,lNG:()=>d.lNG,LG0:()=>d.LG0,x7F:()=>d.x7F,BLA:()=>d.BLA,WT3:()=>d.WT3,xu7:()=>d.xu7,l0G:()=>d.l0G,SDM:()=>d.SDM,Zl4:()=>d.Zl4,e0f:()=>d.e0f,ylV:()=>d.ylV,urI:()=>d.urI,LWX:()=>d.LWX,ELo:()=>it.E,mM$:()=>d.mM$,ODT:()=>d.ODT,pyJ:()=>d.pyJ,Ncv:()=>d.Ncv,kdj:()=>d.kdj,oJ2:()=>d.oJ2,CQC:()=>d.CQC,mH5:()=>d.mH5,Q6t:()=>d.Q6t,LRy:()=>d.LRy,sDr:()=>d.sDr,huO:()=>d.huO,fUj:()=>d.fUj,P_L:()=>d.P_L,R23:()=>d.R23,hgw:()=>d.hgw,FCQ:()=>d.FCQ,jOE:()=>d.jOE,XQy:()=>d.XQy,D7i:()=>d.D7i,BK4:()=>d.BK4,hVg:()=>d.hVg,TOR:()=>d.TOR,pJc:()=>d.pJc,uWl:()=>d.uWl,l6P:()=>d.l6P,u$b:()=>d.u$b,vI1:()=>d.vI1,YVe:()=>d.YVe,hql:()=>d.hql,J3C:()=>d.J3C,JiE:()=>d.JiE,rFG:()=>d.rFG,Fin:()=>d.Fin,A8B:()=>d.A8B,C8s:()=>d.C8s,BoJ:()=>d.BoJ,L6G:()=>d.L6G,DvZ:()=>d.DvZ,jgd:()=>d.jgd,Blb:()=>d.Blb,dFH:()=>d.dFH,M6A:()=>d.M6A,Ddj:()=>d.Ddj,GZp:()=>d.GZp,pnw:()=>d.pnw,UcO:()=>d.UcO,YAb:()=>d.YAb,iW0:()=>d.iW0,$jE:()=>d.$jE,PbM:()=>d.PbM,WuN:()=>d.WuN,oFs:()=>d.oFs,iuW:()=>d.iuW,qYS:()=>at.qY,ylz:()=>at.yl,X4r:()=>d.X4r,FAs:()=>d.FAs,TBb:()=>d.TBb,dLy:()=>d.dLy,wx0:()=>d.wx0,EwU:()=>d.EwU,dXR:()=>d.dXR,pPe:()=>d.pPe,xJ3:()=>d.xJ3,Dr:()=>d.Dr,tnl:()=>tt.t,WQq:()=>k.W,Q7R:()=>rt.Q,bzn:()=>P.b,FLi:()=>x.F,$jT:()=>p.$,sub:()=>z.s,Hs:()=>bt.Hs,C0T:()=>ts,BFc:()=>w.B,kSi:()=>c.k,T5N:()=>E.T,hOW:()=>L.h,ZEY:()=>J,TaL:()=>N,ra8:()=>V.r,wgE:()=>Q.w,zQh:()=>q.z,o8B:()=>ot.o,xWs:()=>nt.x,I1m:()=>S.I,RPU:()=>M.R,O5O:()=>Y.O,P1l:()=>C.P,kA9:()=>h.k,Xtf:()=>y.X,wX9:()=>W.w,IPL:()=>et.I,jIJ:()=>gt.j,aOp:()=>ht.a,Gl3:()=>Qt.G,eMq:()=>wn,ASo:()=>bt.AS,y4m:()=>Wt.y,EZY:()=>cn.E,Pqc:()=>zt.P,gYU:()=>bt.gY,Hi9:()=>bt.Hi,_K2:()=>Xe._K,LCg:()=>At.L,Y12:()=>Bt.Y,oNF:()=>Pt.o,UG6:()=>Vt.U,y5U:()=>Gt.y,GSj:()=>nn.G,RIf:()=>Yt.R,cZk:()=>Re,kgh:()=>Xt.k,rhj:()=>qt.r,DQN:()=>_t.D,Slp:()=>Gn,io:()=>O,aCs:()=>bt.aC,kpo:()=>es,H8d:()=>sn.H,mPL:()=>Yn,Rm2:()=>te.R,Kko:()=>_.K,HPB:()=>ee.H,VZ:()=>It.V,n76:()=>ne.n,NoW:()=>se.N,T9B:()=>Dt.T,jgi:()=>xt.j,NYV:()=>Ut.e,PhQ:()=>ie.P,i2o:()=>$t.i,m1Z:()=>bt.m1,jkA:()=>re.j,BpO:()=>Et.B,Clk:()=>ae.C,lKK:()=>Zt.l,HZy:()=>vt.H,dA1:()=>_n.d,Ec:()=>rn.E,Mw0:()=>Mn.M,SaS:()=>an.S,P61:()=>Kt.P,eVF:()=>oe.e,n7C:()=>jt.n,NsG:()=>le.N,FE$:()=>ue.F,YeY:()=>he.Y,y17:()=>ce.y,gJX:()=>bt.gJ,tAK:()=>qn.tA,VVh:()=>de.V,tQQ:()=>fe.t,BEg:()=>pe.B,d_2:()=>me.d,WfX:()=>ge.W,wdz:()=>ye.w,JFn:()=>A,jh6:()=>bt.jh,ry7:()=>wt.r,dik:()=>Nt.d,Q$M:()=>On.Q,zAd:()=>Tt.z,wck:()=>kn.w,R0O:()=>Ft.R,Kro:()=>B,Vs9:()=>Qn.V,lw0:()=>Lt.l,lDo:()=>Wn.l,RZD:()=>Mt.R,r2V:()=>Bn.r,t$z:()=>Ot.t,jbE:()=>Pn.j,czq:()=>St.c,chL:()=>D.ch,ymU:()=>on.y,OEK:()=>ve.O,tGX:()=>be.t,KtR:()=>ze.K,d_S:()=>j,DZQ:()=>bt.DZ,Vsq:()=>we.V,BaG:()=>Xn.B,mgz:()=>Un.m,efE:()=>Rt.e,K$i:()=>ln.K,TuY:()=>D.Tu,ZSL:()=>g,bvq:()=>un.b,bgA:()=>Z.r,_M9:()=>hn._,Ul9:()=>Ce.U,POl:()=>Ae.P}),l(80225);var r=l(29609),O=l(3821),J=l(9269),N=l(25905),B=l(1986),A=l(1506),j=l(14548),g=l(21710),Z=l(97762),it=l(56188),at=l(73444),D=l(82891),tt=l(72009),k=l(83034),rt=l(56682),P=l(37547),x=l(71084),p=l(37434),z=l(36806),w=l(44353),c=l(46884),E=l(77807),L=l(3752),V=l(75987),Q=l(92290),q=l(70581),ot=l(61548),nt=l(35213),S=l(35317),M=l(74810),Y=l(36183),C=l(18380),h=l(53054),y=l(12809),W=l(83521),et=l(37740),gt=l(2772),ht=l(26193),Qt=l(73595),Wt=l(4350),zt=l(41325),At=l(68833),Bt=l(78650),Pt=l(68326),Vt=l(42103),Gt=l(45104),nn=l(7684),Yt=l(41467),Xt=l(38837),qt=l(54807),_t=l(94300),sn=l(93580),te=l(8535),_=l(72954),ee=l(89578),It=l(17391),ne=l(21292),se=l(60314),Dt=l(56619),xt=l(40044),Ut=l(42524),ie=l(74659),$t=l(23444),re=l(82825),Et=l(85233),ae=l(11732),Zt=l(59731),vt=l(43267),rn=l(66279),Mn=l(87073),an=l(95032),Kt=l(28692),oe=l(54472),jt=l(53827),le=l(62655),ue=l(41222),he=l(65413),ce=l(16412),de=l(98611),fe=l(94399),pe=l(21067),me=l(10829),ge=l(26966),ye=l(48285),wt=l(53205),Nt=l(94063),On=l(14638),Tt=l(81209),kn=l(29660),Ft=l(24167),Qn=l(52431),Lt=l(14181),Wn=l(91517),Mt=l(51583),Bn=l(41621),Ot=l(92885),Pn=l(59279),St=l(66342),on=l(86580),ve=l(74544),be=l(48449),ze=l(41494),we=l(36409),Rt=l(3609),ln=l(48256),un=l(38121),hn=l(56914),Ce=l(20218),Ae=l(44714),Un=l(77914),cn=l(45733),Re=(l(40638),l(90694),l(15245),l(43),l(63552)),Fe=(l(79208),l(92822),l(32395),l(76221),l(27958)),Le=l(99587),Me=l(60926),Oe=l(70644),ke=l(76333),Qe=l(30561),We=l(50110),Be=l(31393),Pe=l(47774),dn=l(4594),Ue=l(33601),$e=l(26486),fn=l(91719),pn=l(65425),Ze=l(59339),Ke=l(55244),mn=l(52325),gn=l(92974);l(88476),l(24782),l(36170),l(45026),l(29655),l(3733),l(33115),l(79780),l(33030),l(19702),l(56637),l(46764),l(46782),l(37500),l(74305),l(45629),l(41342);const Gn={flipLeftRight:Le.n,grayscaleToRGB:Me.C,resizeNearestNeighbor:fn.b,resizeBilinear:$e.v,rgbToGrayscale:Oe.S,rotateWithOffset:ke.x,cropAndResize:Fe.C,nonMaxSuppression:Qe.L,nonMaxSuppressionAsync:We.z,nonMaxSuppressionWithScore:Be.f,nonMaxSuppressionWithScoreAsync:Pe.l,nonMaxSuppressionPadded:dn.H,nonMaxSuppressionPaddedAsync:Ue.R,threshold:pn.E,transform:Ze.p},Yn={bandPart:Ke.x,gramSchmidt:mn.i,qr:gn.qr};var Xn=l(19057),bt=l(47054),qn=l(56203),Xe=l(25271),_n=l(9173),ts=l(47531),wn=l(6757),es=l(89191),Cn=l(86614),d=l(30162);(0,r.i)()},74042:(I,$,l)=>{"use strict";l.d($,{DJ:()=>k,Fb:()=>tt,ZI:()=>g,dv:()=>nt,ht:()=>it,kP:()=>at,lh:()=>Q});var b=l(10467),r=l(41653),O=l(48814),N=l(58034),B=l(27669),A=l(52531),j=l(92357);function g(C){return new rt(C)}function it(C){return new P(C)}function at(C,h){return new ot(C,h)}function tt(C,h=nt.FAIL){return new S(C,h)}class k{toArray(){var h=this;return(0,b.A)(function*(){const y=[];let W=yield h.next();for(;!W.done;)y.push(W.value),W=yield h.next();return y})()}toArrayForTest(){var h=this;return(0,b.A)(function*(){const y=h.prefetch(100),W=[];let et=yield y.next();for(;!et.done;)W.push(et.value),et=yield y.next();return W})()}resolveFully(){var h=this;return(0,b.A)(function*(){let y=yield h.next();for(;!y.done;)y=yield h.next()})()}resolveWhile(h){var y=this;return(0,b.A)(function*(){let W=yield y.next(),et=h(W.value);for(;!W.done&&et;)W=yield y.next(),et=h(W.value)})()}handleErrors(h){return new L(this,h)}filter(h){return new c(this,h)}map(h){return new E(this,h)}mapAsync(h){return new V(this,h)}serialMapAsync(h){return new V(this,h).serial()}flatmap(h){return new q(this,h)}forEachAsync(h){var y=this;return(0,b.A)(function*(){return y.map(h).resolveFully()})()}serialForEach(h){var y=this;return(0,b.A)(function*(){return y.serialMapAsync(h).resolveWhile(W=>!0===W)})()}rowMajorBatch(h,y=!0){return new w(this,h,y)}columnMajorBatch(h,y=!0,W=B.rN){return this.rowMajorBatch(h,y).map(gt=>(0,B.sy)(gt,W))}concatenate(h,y){return new ot(g([this,h]),y)}take(h){return h<0||null==h?this:new z(this,h)}skip(h){return h<0||null==h?this:new p(this,h)}prefetch(h){return new M(this,h)}shuffle(h,y){return new Y(this,h,y)}serial(){return new x(this)}}class rt extends k{constructor(h){super(),this.items=h,this.trav=0}summary(){return`Array of ${this.items.length} items`}next(){var h=this;return(0,b.A)(function*(){if(h.trav>=h.items.length)return{value:null,done:!0};const y=h.items[h.trav];return h.trav++,{value:(0,N.G)(y),done:!1}})()}}class P extends k{constructor(h){super(),this.nextFn=h}summary(){return"Function call"}next(){var h=this;return(0,b.A)(function*(){try{return h.nextFn()}catch(y){throw y.message=`Error thrown while iterating through a dataset: ${y.message}`,y}})()}}class x extends k{constructor(h){super(),this.upstream=h,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> Serial`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){return h.upstream.next()})()}}class p extends k{constructor(h,y){super(),this.upstream=h,this.maxCount=y,this.count=0,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> Skip`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){for(;h.count++<h.maxCount;){const y=yield h.upstream.next();if(y.done)return y;r.ASo(y.value)}return h.upstream.next()})()}}class z extends k{constructor(h,y){super(),this.upstream=h,this.maxCount=y,this.count=0}summary(){return`${this.upstream.summary()} -> Take`}next(){var h=this;return(0,b.A)(function*(){return h.count++>=h.maxCount?{value:null,done:!0}:h.upstream.next()})()}}class w extends k{constructor(h,y,W=!0){super(),this.upstream=h,this.batchSize=y,this.enableSmallLastBatch=W,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> RowMajorBatch`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){const y=[];for(;y.length<h.batchSize;){const W=yield h.upstream.next();if(W.done)return h.enableSmallLastBatch&&y.length>0?{value:y,done:!1}:{value:null,done:!0};y.push(W.value)}return{value:y,done:!1}})()}}class c extends k{constructor(h,y){super(),this.upstream=h,this.predicate=y,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> Filter`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){for(;;){const y=yield h.upstream.next();if(y.done||h.predicate(y.value))return y;r.ASo(y.value)}})()}}class E extends k{constructor(h,y){super(),this.upstream=h,this.transform=y}summary(){return`${this.upstream.summary()} -> Map`}next(){var h=this;return(0,b.A)(function*(){const y=yield h.upstream.next();if(y.done)return{value:null,done:!0};const W=r.d_S.getTensorsInContainer(y.value),et=h.transform(y.value),gt=r.d_S.getTensorsInContainer(et);for(const ht of W)r.d_S.isTensorInList(ht,gt)||ht.dispose();return{value:et,done:!1}})()}}class L extends k{constructor(h,y){super(),this.upstream=h,this.handler=y,this.count=0,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> handleErrors`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){for(;;)try{return yield h.upstream.next()}catch(y){if(!h.handler(y))return{value:null,done:!0}}})()}}class V extends k{constructor(h,y){super(),this.upstream=h,this.transform=y}summary(){return`${this.upstream.summary()} -> AsyncMap`}next(){var h=this;return(0,b.A)(function*(){const y=yield h.upstream.next();if(y.done)return{value:null,done:!0};const W=r.d_S.getTensorsInContainer(y.value),et=yield h.transform(y.value),gt=r.d_S.getTensorsInContainer(et);for(const ht of W)r.d_S.isTensorInList(ht,gt)||ht.dispose();return{value:et,done:!1}})()}}class Q extends k{constructor(){super(),this.outputQueue=new A.g,this.lastRead=Promise.resolve({value:null,done:!1})}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){for(;0===h.outputQueue.length();)if(!(yield h.pump()))return{value:null,done:!0};return{value:h.outputQueue.shift(),done:!1}})()}}class q extends Q{constructor(h,y){super(),this.upstream=h,this.transform=y}summary(){return`${this.upstream.summary()} -> Flatmap`}pump(){var h=this;return(0,b.A)(function*(){const y=yield h.upstream.next();if(y.done)return!1;const W=r.d_S.getTensorsInContainer(y.value),et=h.transform(y.value),gt=r.d_S.getTensorsInContainer(et);h.outputQueue.pushAll(et);for(const ht of W)r.d_S.isTensorInList(ht,gt)||ht.dispose();return!0})()}}class ot extends k{constructor(h,y){super(),this.baseErrorHandler=y,this.lastRead=null,this.iterator=null,this.moreIterators=h}summary(){return"TODO: fill in upstream of chained summaries -> Chained"}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.readFromChain(h.lastRead),h.lastRead})()}readFromChain(h){var y=this;return(0,b.A)(function*(){if(yield h,null==y.iterator){const et=yield y.moreIterators.next();if(et.done)return{value:null,done:!0};y.iterator=et.value,null!=y.baseErrorHandler&&(y.iterator=y.iterator.handleErrors(y.baseErrorHandler))}const W=yield y.iterator.next();return W.done?(y.iterator=null,y.readFromChain(h)):W})()}}var nt=function(C){return C[C.FAIL=0]="FAIL",C[C.SHORTEST=1]="SHORTEST",C[C.LONGEST=2]="LONGEST",C}(nt||{});class S extends k{constructor(h,y=nt.FAIL){super(),this.iterators=h,this.mismatchMode=y,this.count=0,this.currentPromise=null}summary(){return"{TODO: fill in upstream of zip summaries} -> Zip"}nextState(h){var y=this;return(0,b.A)(function*(){yield h;let W=0,et=0;const ht=yield(0,B.te)(y.iterators,function gt(Qt){return Qt instanceof k?{value:Qt.next().then(zt=>(W++,zt.done&&et++,zt.value)),recurse:!1}:{value:null,recurse:!0}});if(W===et)return{value:null,done:!0};if(et>0)switch(y.mismatchMode){case nt.FAIL:throw new Error(`Zipped streams should have the same length. Mismatched at element ${y.count}.`);case nt.SHORTEST:return{value:null,done:!0}}return y.count++,{value:ht,done:!1}})()}next(){var h=this;return(0,b.A)(function*(){return h.currentPromise=h.nextState(h.currentPromise),h.currentPromise})()}}class M extends k{constructor(h,y){super(),this.upstream=h,this.bufferSize=y,this.buffer=new j.N(y)}summary(){return`${this.upstream.summary()} -> Prefetch`}refill(){for(;!this.buffer.isFull();){const h=this.upstream.next();this.buffer.push(h)}}next(){return this.refill(),this.buffer.shift()}}class Y extends M{constructor(h,y,W){super(h,y),this.upstream=h,this.windowSize=y,this.upstreamExhausted=!1,this.random=O.alea(W||r.ZSL.now().toString()),this.lastRead=Promise.resolve({value:null,done:!1})}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}randomInt(h){return Math.floor(this.random()*h)}chooseIndex(){return this.randomInt(this.buffer.length())}serialNext(){var h=this;return(0,b.A)(function*(){for(h.upstreamExhausted||h.refill();!h.buffer.isEmpty();){const y=h.chooseIndex(),W=yield h.buffer.shuffleExcise(y);if(!W.done)return h.refill(),W;h.upstreamExhausted=!0}return{value:null,done:!0}})()}}},27669:(I,$,l)=>{"use strict";l.d($,{Bl:()=>O,mf:()=>it,rN:()=>A,sy:()=>N,te:()=>j,xZ:()=>Z});var b=l(10467),r=l(41653);function O(D,tt){return J(D,tt)}function J(D,tt,k=new Map,rt=new Set){if(null==D)return null;if("function"==typeof Blob&&D instanceof Blob)return D.slice();if(rt.has(D))throw new Error("Circular references are not supported.");if(k.has(D))return k.get(D);const P=tt(D);if(P.recurse&&null!==P.value)throw new Error("A deep map function may not return both a value and recurse=true.");if(P.recurse){if(Z(D)){const x=Array.isArray(D)?[]:{};rt.add(D);for(const p in D){const w=J(D[p],tt,k,rt);x[p]=w}return rt.delete(D),D.__proto__&&(x.__proto__=D.__proto__),x}throw new Error(`Can't recurse into non-iterable type: ${D}`)}return k.set(D,P.value),P.value}function N(D,tt=A){return B(D,tt)}function B(D,tt,k=new Set){const rt=D[0];if(k.has(rt))throw new Error("Circular references are not supported.");const P=tt(D);if(P.recurse&&null!==P.value)throw new Error("A deep zip function may not return both a value and recurse=true.");if(P.recurse){if(Z(rt)){const x=Array.isArray(rt)?[]:{};k.add(rt);for(const p in rt){const w=B(D.map(c=>c[p]),tt,k);x[p]=w}return k.delete(rt),x}throw new Error(`Can't recurse into non-iterable type: ${rt}`)}return P.value}function A(D){return null===D?null:Z(D[0])?{value:null,recurse:!0}:{value:D,recurse:!1}}function j(D,tt){return g.apply(this,arguments)}function g(){return(g=(0,b.A)(function*(D,tt){const k=new Map;J(D,tt,k);for(const P of Array.from(k.keys())){const x=k.get(P);if(r.ZSL.isPromise(x)){const p=yield x;k.set(P,p)}}return J(D,tt,k)})).apply(this,arguments)}function Z(D){let tt=!1;if(r._K2().get("IS_BROWSER"))tt=D instanceof TextDecoder;else{const{StringDecoder:k}=l(80551);tt=D instanceof k}return null!=D&&!ArrayBuffer.isView(D)&&(Array.isArray(D)||"object"==typeof D&&!(D instanceof r.qYS)&&!(D instanceof Promise)&&!tt)}function it(D){return null==D||function at(D){return null===D||"object"!=typeof D&&"function"!=typeof D}(D)||Array.isArray(D)||"object"==typeof D&&D instanceof r.qYS||r.ZSL.isTypedArray(D)}},91806:(I,$,l)=>{"use strict";l.d($,{m:()=>N,p:()=>B});var b=l(41653),r=l(29887),O=l(42946),J=l(22919);let N=(()=>{class A extends J.Wd{constructor(g){if(super({dtype:g.dtype,name:null!=g.name?g.name:(0,r.v)("input").toString()}),null==g.batchSize&&(g.batchSize=null),null==g.sparse&&(g.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=g.sparse,null!=g.inputShape&&null!=g.batchInputShape)throw new O.Qp("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let Z=g.batchInputShape;if(null==Z){if(null==g.inputShape)throw new O.Qp("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");Z=[g.batchSize].concat(g.inputShape)}else if(null!=g.batchSize)throw new O.Qp("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const it=g.dtype||"float32";this.batchInputShape=Z,this.dtype=it,this.inputSpec=[{shape:Z}];const at=new J.Ar(this.dtype,this.batchInputShape,this,[],{},this.name);at.nodeIndex=0,at.tensorIndex=0,new J.bP({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[at],outputTensors:[at],inputMasks:[null],outputMasks:[null],inputShapes:[Z],outputShapes:[Z]})}apply(g,Z){throw new O.Qp(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}return A.className="InputLayer",A})();function B(A){if(null==A.batchShape&&null==A.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=A.batchShape&&null!=A.shape)throw new O.Qp("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let j=A.batchShape;null!=A.shape&&null==j&&(j=[null].concat(A.shape));let g=A.dtype;return null==g&&(g="float32"),new N({batchInputShape:j,name:A.name,dtype:g,sparse:A.sparse}).inboundNodes[0].outputTensors[0]}b.JFn.registerClass(N)},48631:(I,$,l)=>{"use strict";l.d($,{Gx:()=>p});var b=l(10467),r=l(41653),O=l(29887),J=l(91806),N=l(22919),B=l(31542),A=l(42946),j=l(53340),g=l(24503),it=(l(13425),l(71936));let p=(()=>{class z extends B.Gw{constructor(c){if(super({inputs:[],outputs:[]}),c=c||{},this.trainable=!0,this.built=!1,this.name=null!=c.name?c.name:(0,O.v)("sequential_"),null!=c.layers)for(const E of c.layers)this.add(E)}checkShape(c){if(c.inboundNodes[0].outputTensors[0].shape.some(L=>L<0))throw new A.Qp(`Negative dimension size caused by adding layer ${c.name} with input shape [${c.inboundNodes[0].inputTensors[0].shape}]`)}add(c){const E=c instanceof z||c instanceof B.Gw;let L;if(E){if(L=c,1!==L.outputs.length)throw new A.Qp("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==L.inputs.length)throw new A.Qp("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===c.inboundNodes.length){if(null==c.batchInputShape)throw new A.Qp("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const V=(0,J.p)({batchShape:c.batchInputShape,dtype:c.dtype,name:c.name+"_input"});c.apply(V)}if(E)this.outputs=L.outputs,this.inputs=L.inputs;else{if(1!==c.inboundNodes.length)throw new A.Qp(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${c.name} which has ${c.inboundNodes.length} pre-existing inbound connections.`);if(1!==c.inboundNodes[0].outputTensors.length)throw new A.Qp("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(c),this.outputs=[c.inboundNodes[0].outputTensors[0]],this.inputs=(0,N.X6)(this.outputs[0])}this.inboundNodes=[],new N.bP({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:g.fD(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map(V=>V.shape),outputShapes:this.outputs[0].shape})}else{const V=c.apply(this.outputs[0]);if(Array.isArray(V))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(c),this.outputs=[V],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(c),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const c=this.layers.length-1;this.layers[c].outboundNodes=[],this.outputs=[this.layers[c].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(c,E){return null==this.model&&this.build(),this.model.call(c,E)}build(c){if((0,it.U$)(c),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new B.Gw({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(c,E,L=console.log){this.built||this.build(),super.summary(c,E,L)}setWeights(c){null==this.model&&this.build(),this.model.setWeights(c)}evaluate(c,E,L={}){if(!this.built)throw new A.bu("The model needs to be compiled before being used.");return this.model.evaluate(c,E,L)}evaluateDataset(c,E){var L=this;return(0,b.A)(function*(){if(!L.built)throw new A.bu("The model needs to be compiled before being used.");return L.model.evaluateDataset(c,E)})()}predict(c,E={}){return null==this.model&&this.build(),this.model.predict(c,E)}predictOnBatch(c){return null==this.model&&this.build(),this.model.predictOnBatch(c)}compile(c){this.build(),this.model.compile(c),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(c){this.model.optimizer=c}fit(c,E){var L=this;return(0,b.A)(function*(V,Q,q={}){if(!L.built)throw new A.bu("The model needs to be compiled before being used.");return L.model.fit(V,Q,q)}).apply(this,arguments)}fitDataset(c,E){var L=this;return(0,b.A)(function*(){if(!L.built)throw new A.bu("The model needs to be compiled before being used.");return L.model.fitDataset(c,E)})()}trainOnBatch(c,E){var L=this;return(0,b.A)(function*(){return L.model.trainOnBatch(c,E)})()}static fromConfig(c,E,L={},V=!1){let Q,q={};if(E instanceof Array){if(null==E[0].className||"Merge"===E[0].className)throw new A.Qp("Legacy serialization format not supported yet.");Q=E}else r.ZSL.assert(null!=E.layers,()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field."),Q=E.layers,delete E.layers,q=E;const ot=new c(q);if(!(ot instanceof z))throw new A.EH(`Sequential.fromConfig called on non-Sequential input: ${ot}`);for(const nt of Q){const M=(0,j.i)(nt,void 0,V);V&&M.setFastWeightInitDuringBuild(!0),ot.add(M)}return ot}set stopTraining(c){if(null==this.model)throw new A.Qp("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=c}get stopTraining(){if(null==this.model)throw new A.Qp("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const c=[];for(const E of this.layers){const L={};L.className=E.getClassName(),L.config=E.getConfig(),c.push(L)}return{name:this.name,layers:c}}}return z.className="Sequential",z})();r.JFn.registerClass(p)},13425:(I,$,l)=>{"use strict";l.d($,{M:()=>J,w:()=>O});var b=l(24503);function r(N,B,A){return("inboundNodes"===N||"outputLayers"===N||"inputLayers"===N)&&0===B&&"string"==typeof A}function O(N,B){if(null===N)return null;if("string"==typeof N)return b.Cb(N);if("number"==typeof N||"boolean"==typeof N)return N;if(N instanceof Array){const A=[],j=N.length;for(let g=0;g<j;++g){const Z=N[g];r(B,g,Z)?A.push(Z):A.push(O(Z,B))}return A}{const A={};for(const j of Object.keys(N)){const g=N[j];if("name"===j&&"string"==typeof g)A[j]=g;else{const Z=b.Cb(j);A[Z]=O(g,Z)}}return A}}function J(N,B){if(null==N)return null;if("string"==typeof N)return b.uc(N);if("number"==typeof N||"boolean"==typeof N)return N;if(N instanceof Array){const A=[],j=N.length;for(let g=0;g<j;++g){const Z=N[g];r(B,g,Z)?A.push(Z):A.push(J(Z,B))}return A}{const A={};for(const j of Object.keys(N)){const g=N[j];A[b.uc(j)]="name"!==j&&"className"!==j||"string"!=typeof g?J(g,j):g}return A}}},48814:(I,$,l)=>{var b=l(2495),r=l(57850),O=l(85704),J=l(88114),N=l(79040),B=l(84478),A=l(97454);A.alea=b,A.xor128=r,A.xorwow=O,A.xorshift7=J,A.xor4096=N,A.tychei=B,I.exports=A},10467:(I,$,l)=>{"use strict";function b(O,J,N,B,A,j,g){try{var Z=O[j](g),it=Z.value}catch(at){return void N(at)}Z.done?J(it):Promise.resolve(it).then(B,A)}function r(O){return function(){var J=this,N=arguments;return new Promise(function(B,A){var j=O.apply(J,N);function g(it){b(j,B,A,g,Z,"next",it)}function Z(it){b(j,B,A,g,Z,"throw",it)}g(void 0)})}}l.d($,{A:()=>r})}},Ln={};function U(I){var $=Ln[I];if(void 0!==$)return $.exports;var l=Ln[I]={id:I,loaded:!1,exports:{}};return Fn[I].call(l.exports,l,l.exports,U),l.loaded=!0,l.exports}U.m=Fn,U.x=()=>{var I=U.O(void 0,[6599],()=>U(18633));return U.O(I)},U.amdD=function(){throw new Error("define cannot be used indirect")},U.amdO={},I=[],U.O=($,l,b,r)=>{if(!l){var J=1/0;for(O=0;O<I.length;O++){for(var[l,b,r]=I[O],N=!0,B=0;B<l.length;B++)(!1&r||J>=r)&&Object.keys(U.O).every(at=>U.O[at](l[B]))?l.splice(B--,1):(N=!1,r<J&&(J=r));if(N){I.splice(O--,1);var A=b();void 0!==A&&($=A)}}return $}r=r||0;for(var O=I.length;O>0&&I[O-1][2]>r;O--)I[O]=I[O-1];I[O]=[l,b,r]},U.n=I=>{var $=I&&I.__esModule?()=>I.default:()=>I;return U.d($,{a:$}),$},U.d=(I,$)=>{for(var l in $)U.o($,l)&&!U.o(I,l)&&Object.defineProperty(I,l,{enumerable:!0,get:$[l]})},U.f={},U.e=I=>Promise.all(Object.keys(U.f).reduce(($,l)=>(U.f[l](I,$),$),[])),U.u=I=>I+".d231d2996f23297c.js",U.miniCssF=I=>{},U.o=(I,$)=>Object.prototype.hasOwnProperty.call(I,$),U.r=I=>{typeof Symbol<"u"&&Symbol.toStringTag&&Object.defineProperty(I,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(I,"__esModule",{value:!0})},U.nmd=I=>(I.paths=[],I.children||(I.children=[]),I),(()=>{var I;U.tt=()=>(void 0===I&&(I={createScriptURL:$=>$},typeof trustedTypes<"u"&&trustedTypes.createPolicy&&(I=trustedTypes.createPolicy("angular#bundler",I))),I)})(),U.tu=I=>U.tt().createScriptURL(I),U.p="",(()=>{var I={8633:1};U.f.i=(r,O)=>{I[r]||importScripts(U.tu(U.p+U.u(r)))};var l=self.webpackChunkapp=self.webpackChunkapp||[],b=l.push.bind(l);l.push=r=>{var[O,J,N]=r;for(var B in J)U.o(J,B)&&(U.m[B]=J[B]);for(N&&N(U);O.length;)I[O.pop()]=1;b(r)}})(),(()=>{var I=U.x;U.x=()=>U.e(6599).then(I)})(),U.x()})();