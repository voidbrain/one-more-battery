(()=>{var I,Ns={18633:(I,$,l)=>{"use strict";var b=l(10467),r=l(41653),g=(l(62914),l(10222),l(50531),l(15293),l(91584),l(91806),l(22919)),rt=(l(40083),l(31542),l(48631)),P=l(42804),R=l(67165),p=l(42946),z=l(7257),w=l(59352),c=l(71936);r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=(0,c.un)(t);let s=(0,r.VVh)(t);return null!=this.maxValue&&(s=(0,r.zQh)(s,0,this.maxValue)),s}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}return i.className="ReLU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const s=(0,c.un)(t);return(0,r.H8d)(s,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}return i.className="LeakyReLU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=(0,z.Fe)(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=(0,w.Bm)(t.alphaRegularizer),this.alphaConstraint=(0,R.YZ)(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new p.Qp(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=(0,c.U$)(t)).slice(1);if(null!=this.sharedAxes)for(const a of this.sharedAxes)e[a-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const s={};if(null!=this.sharedAxes)for(let a=1;a<t.length;++a)s[a]=t[a];this.inputSpec=[new g.eO({ndim:t.length,axes:s})],this.built=!0}call(t,e){return t=(0,c.un)(t),(0,r.NsG)(t,this.alpha.read())}getConfig(){const t={alphaInitializer:(0,z.zo)(this.alphaInitializer),alphaRegularizer:(0,w.R9)(this.alphaRegularizer),alphaConstraint:(0,R.uH)(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}return i.className="PReLU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new p.EH(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const s=(0,c.un)(t);return(0,r.Pqc)(s)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}return i.className="ELU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const s=(0,c.un)(t);return(0,r.lKK)(s,(0,r.wgE)((0,r.rhj)(s,this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}return i.className="ThresholdedReLU",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new P.rF).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){return(0,r.DZQ)(()=>{let s=(0,c.un)(t);const a=e.mask;if(null!=a){const o=(0,r.lKK)((0,r.jbE)((0,r.SaS)(s.shape),(0,r.wgE)(a,s.dtype)),(0,r.d_2)(-1e9));s=(0,r.WQq)(s,o)}return this.axis instanceof Array?this.axis.length>1?(0,r.oNF)((0,r.jbE)(s,(0,r.VZ)(s,this.axis,!0))):this.softmax(s,this.axis[0]):this.softmax(s,this.axis)})}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Softmax",i})());var et=l(17513),D=l(99198),L=l(9980),G=l(51367),C=l(24503);function h(i,n){return(0,r.DZQ)(()=>((0,L.uM)(n),"channelsFirst"===n?r.mgz(i,[0,2,3,1]):i))}function y(i,n){return(0,r.DZQ)(()=>((0,L.uM)(n),"channelsFirst"===n?r.mgz(i,[0,2,3,4,1]):i))}function ut(i,n,t,e=[1,1],s="valid",a,o,u=null){return(0,r.DZQ)(()=>{if(null==a&&(a=(0,et.VI)()),(0,L.uM)(a),3!==i.rank&&4!==i.rank)throw new p.Qp(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${i.rank}.`);if(3!==n.rank&&4!==n.rank)throw new p.Qp(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${i.rank}.`);let f=h(i,a);if("causal"===s)throw new p.EH("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return f=r.cZk.conv2d({x:f,filter:n,strides:e,pad:"same"===s?"same":"valid",dilations:o,dataFormat:"NHWC",bias:t,activation:u}),"channelsFirst"===a&&(f=r.mgz(f,[0,3,1,2])),f})}class bt extends g.Wd{constructor(n,t){if(super(t),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",bt.verifyArgs(t),this.rank=n,C.oo(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new p.EH(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=(0,G.J)(t.kernelSize,n,"kernelSize"),this.strides=(0,G.J)(null==t.strides?1:t.strides,n,"strides"),this.padding=null==t.padding?"valid":t.padding,(0,L.tB)(this.padding),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,L.uM)(this.dataFormat),this.activation=(0,P.b_)(t.activation),this.useBias=null==t.useBias||t.useBias,this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=(0,R.YZ)(t.biasConstraint),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.activityRegularizer=(0,w.Bm)(t.activityRegularizer),this.dilationRate=(0,G.J)(null==t.dilationRate?1:t.dilationRate,n,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new p.Qp(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new p.Qp(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new p.Qp(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(n){if(C.vA("kernelSize"in n,"required key 'kernelSize' not in config"),"number"!=typeof n.kernelSize&&!C.HP(n.kernelSize,"number",1,3))throw new p.Qp(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(n.kernelSize)}.`)}getConfig(){const n={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:(0,P.Bu)(this.activation),useBias:this.useBias,biasInitializer:(0,z.zo)(this.biasInitializer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),biasConstraint:(0,R.uH)(this.biasConstraint)},t=super.getConfig();return Object.assign(n,t),n}}class At extends bt{constructor(n,t){super(n,t),this.kernel=null,At.verifyArgs(t),this.filters=t.filters,C.oo(this.filters,"filters"),this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=(0,R.YZ)(t.kernelConstraint),this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer)}build(n){n=(0,c.U$)(n);const t="channelsFirst"===this.dataFormat?1:n.length-1;if(null==n[t])throw new p.Qp(`The channel dimension of the input should be defined. Found ${n[t]}`);const e=n[t],s=this.kernelSize.concat([e,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[t]:e}}],this.built=!0}call(n,t){return(0,r.DZQ)(()=>{let e;n=(0,c.un)(n);const s=null==this.bias?null:this.bias.read(),a=C.Cd(this.activation.getClassName());if(null!=a&&2===this.rank)e=ut(n,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate,a);else{if(1===this.rank)e=function O(i,n,t,e=1,s="valid",a,o=1){return(0,r.DZQ)(()=>{if(null==a&&(a=(0,et.VI)()),(0,L.uM)(a),3!==i.shape.length)throw new p.Qp(`The input of a conv1dWithBias operation should be 3, but is ${i.shape.length} instead.`);if(3!==n.shape.length)throw new p.Qp(`The kernel for a conv1dWithBias operation should be 3, but is ${n.shape.length} instead`);if(null!=t&&1!==t.shape.length)throw new p.Qp(`The bias for a conv1dWithBias operation should be 1, but is ${t.shape.length} instead`);if("channelsFirst"===a&&(i=r.mgz(i,[0,2,1])),"causal"===s)throw new p.EH("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let u=r.kA9(i,n,e,"same"===s?"same":"valid","NWC",o);return null!=t&&(u=D.ni(u,t)),u})}(n,this.kernel.read(),s,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=ut(n,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new p.EH("convolutions greater than 3D are not implemented yet.");e=function Wt(i,n,t,e=[1,1,1],s="valid",a,o){return(0,r.DZQ)(()=>{if(null==a&&(a=(0,et.VI)()),(0,L.uM)(a),4!==i.rank&&5!==i.rank)throw new p.Qp(`conv3dWithBias expects input to be of rank 4 or 5, but received ${i.rank}.`);if(4!==n.rank&&5!==n.rank)throw new p.Qp(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${i.rank}.`);let u=y(i,a);if("causal"===s)throw new p.EH("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return u=r.IPL(u,n,e,"same"===s?"same":"valid","NDHWC",o),null!=t&&(u=D.ni(u,t)),"channelsFirst"===a&&(u=r.mgz(u,[0,4,1,2,3])),u})}(n,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e})}computeOutputShape(n){n=(0,c.U$)(n);const t=[],e="channelsLast"===this.dataFormat?n.slice(1,n.length-1):n.slice(2);for(let a=0;a<e.length;++a){const o=(0,G.Ol)(e[a],this.kernelSize[a],this.padding,this.strides[a],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[a]);t.push(o)}let s=[n[0]];return"channelsLast"===this.dataFormat?(s=s.concat(t),s.push(this.filters)):(s.push(this.filters),s=s.concat(t)),s}getConfig(){const n={filters:this.filters,kernelInitializer:(0,z.zo)(this.kernelInitializer),kernelRegularizer:(0,w.R9)(this.kernelRegularizer),kernelConstraint:(0,R.uH)(this.kernelConstraint)},t=super.getConfig();return Object.assign(n,t),n}static verifyArgs(n){if(!("filters"in n)||"number"!=typeof n.filters||n.filters<1)throw new p.Qp(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(n.filters)}`)}}let Bt=(()=>{class i extends At{constructor(t){super(2,t),i.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!C.HP(t.kernelSize,"number",1,2))throw new p.Qp(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}return i.className="Conv2D",i})();r.JFn.registerClass(Bt);let Pt=(()=>{class i extends At{constructor(t){super(3,t),i.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new p.Qp(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}return i.className="Conv3D",i})();r.JFn.registerClass(Pt);let Vt=(()=>{class i extends Bt{constructor(t){if(super(t),this.inputSpec=[new g.eO({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new p.Qp(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=(0,c.U$)(t)).length)throw new p.Qp("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new p.Qp("The channel dimension of the inputs should be defined. Found `None`.");const s=t[e],a=this.kernelSize.concat([this.filters,s]);this.kernel=this.addWeight("kernel",a,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new g.eO({ndim:4,axes:{[e]:s}})],this.built=!0}call(t,e){return r.DZQ(()=>{let s=(0,c.un)(t);if(4!==s.shape.length)throw new p.Qp(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${s.shape.length}`);const a=s.shape;let u,f;"channelsFirst"===this.dataFormat?(u=2,f=3):(u=1,f=2);const v=a[f],N=this.kernelSize[1],J=this.strides[1],j=[a[0],(0,G.mW)(a[u],this.strides[0],this.kernelSize[0],this.padding),(0,G.mW)(v,J,N,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(s=r.mgz(s,[0,2,3,1]));let st=r.wX9(s,this.kernel.read(),j,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(st=r.mgz(st,[0,3,1,2])),null!=this.bias&&(st=D.ni(st,this.bias.read(),this.dataFormat)),null!=this.activation&&(st=this.activation.apply(st)),st})}computeOutputShape(t){const e=(t=(0,c.U$)(t)).slice();let s,a,o;"channelsFirst"===this.dataFormat?(s=1,a=2,o=3):(s=3,a=1,o=2);const u=this.kernelSize[0],f=this.kernelSize[1],m=this.strides[0],v=this.strides[1];return e[s]=this.filters,e[a]=(0,G.mW)(e[a],m,u,this.padding),e[o]=(0,G.mW)(e[o],v,f,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}return i.className="Conv2DTranspose",i})();r.JFn.registerClass(Vt);let Gt=(()=>{class i extends Pt{constructor(t){if(super(t),this.inputSpec=[new g.eO({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new p.Qp(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=(0,c.U$)(t)).length)throw new p.Qp("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new p.Qp("The channel dimension of the inputs should be defined. Found `None`.");const s=t[e],a=this.kernelSize.concat([this.filters,s]);this.kernel=this.addWeight("kernel",a,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new g.eO({ndim:5,axes:{[e]:s}})],this.built=!0}call(t,e){return r.DZQ(()=>{let s=(0,c.un)(t);if(5!==s.shape.length)throw new p.Qp(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${s.shape.length}`);const a=s.shape;let u,f,m;"channelsFirst"===this.dataFormat?(m=2,u=3,f=4):(m=1,u=2,f=3);const x=a[u],N=a[f],J=this.kernelSize[1],Y=this.kernelSize[2],j=this.strides[1],st=this.strides[2],ht=[a[0],(0,G.mW)(a[m],this.strides[0],this.kernelSize[0],this.padding),(0,G.mW)(x,j,J,this.padding),(0,G.mW)(N,st,Y,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(s=r.mgz(s,[0,2,3,4,1]));let pt=r.jIJ(s,this.kernel.read(),ht,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(pt=r.mgz(pt,[0,4,1,2,3])),null!==this.bias&&(pt=D.ni(pt,this.bias.read(),this.dataFormat)),null!==this.activation&&(pt=this.activation.apply(pt)),pt})}computeOutputShape(t){const e=(t=(0,c.U$)(t)).slice();let s,a,o,u;"channelsFirst"===this.dataFormat?(s=1,a=2,o=3,u=4):(s=4,a=1,o=2,u=3);const f=this.kernelSize[0],m=this.kernelSize[1],v=this.kernelSize[2],x=this.strides[0],N=this.strides[1],F=this.strides[2];return e[s]=this.filters,e[a]=(0,G.mW)(e[a],x,f,this.padding),e[o]=(0,G.mW)(e[o],N,m,this.padding),e[u]=(0,G.mW)(e[u],F,v,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}return i.className="Conv3DTranspose",i})();r.JFn.registerClass(Gt);let ss=(()=>{class i extends At{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new p.Qp("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new p.Qp("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new p.Qp(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=(0,z.Fe)(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=(0,w.Bm)(e.depthwiseRegularizer),this.depthwiseConstraint=(0,R.YZ)(e.depthwiseConstraint),this.pointwiseInitializer=(0,z.Fe)(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=(0,w.Bm)(e.pointwiseRegularizer),this.pointwiseConstraint=(0,R.YZ)(e.pointwiseConstraint)}build(t){if((t=(0,c.U$)(t)).length<this.rank+2)throw new p.Qp(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new p.Qp(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const s=t[e],a=this.kernelSize.concat([s,this.depthMultiplier]),o=[];for(let f=0;f<this.rank;++f)o.push(1);o.push(s*this.depthMultiplier,this.filters);const u=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",a,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,u,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",o,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,u,this.pointwiseConstraint),this.bias=this.useBias?this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,u,this.biasConstraint):null,this.inputSpec=[new g.eO({ndim:this.rank+2,axes:{[e]:s}})],this.built=!0}call(t,e){return(0,r.DZQ)(()=>{let s;if(t=(0,c.un)(t),1===this.rank)throw new p.EH("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=r.mgz(t,[0,2,3,1])),s=r.wdz(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(s=D.ni(s,this.bias.read(),this.dataFormat)),null!=this.activation&&(s=this.activation.apply(s)),"channelsFirst"===this.dataFormat&&(s=r.mgz(s,[0,3,1,2])),s})}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=(0,z.zo)(this.depthwiseInitializer),t.pointwiseInitializer=(0,z.zo)(this.pointwiseInitializer),t.depthwiseRegularizer=(0,w.R9)(this.depthwiseRegularizer),t.pointwiseRegularizer=(0,w.R9)(this.pointwiseRegularizer),t.depthwiseConstraint=(0,R.uH)(this.depthwiseConstraint),t.pointwiseConstraint=(0,R.uH)(this.pointwiseConstraint),t}}return i.className="SeparableConv",i})();r.JFn.registerClass((()=>{class i extends ss{constructor(t){super(2,t)}}return i.className="SeparableConv2D",i})()),r.JFn.registerClass((()=>{class i extends At{constructor(t){super(1,t),i.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!C.HP(t.kernelSize,"number",1,1))throw new p.Qp(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}return i.className="Conv1D",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.cropping="number"==typeof t.cropping?[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return(0,r.DZQ)(()=>{if(t=(0,c.un)(t),"channelsLast"===this.dataFormat){const s=D.r0(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return D.r0(s,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const s=D.r0(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return D.r0(s,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}})}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Cropping2D",i})());let _t=(()=>{class i extends g.Wd{constructor(t){super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,L.uM)(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,(0,L.uU)(this.interpolation)}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],null==t[2]?null:this.size[0]*t[2],null==t[3]?null:this.size[1]*t[3]]:[t[0],null==t[1]?null:this.size[0]*t[1],null==t[2]?null:this.size[1]*t[2],t[3]]}call(t,e){return r.DZQ(()=>{let s=(0,c.un)(t);const a=s.shape;if("channelsFirst"===this.dataFormat){s=r.mgz(s,[0,2,3,1]);const o=this.size[0]*a[2],u=this.size[1]*a[3],f="nearest"===this.interpolation?r.Slp.resizeNearestNeighbor(s,[o,u]):r.Slp.resizeBilinear(s,[o,u]);return r.mgz(f,[0,3,1,2])}{const o=this.size[0]*a[1],u=this.size[1]*a[2];return"nearest"===this.interpolation?r.Slp.resizeNearestNeighbor(s,[o,u]):r.Slp.resizeBilinear(s,[o,u])}})}getConfig(){const t={size:this.size,dataFormat:this.dataFormat,interpolation:this.interpolation},e=super.getConfig();return Object.assign(t,e),t}}return i.className="UpSampling2D",i})();r.JFn.registerClass(_t);let te=(()=>{class i extends bt{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=(0,z.Fe)(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=(0,R.YZ)(t.depthwiseConstraint),this.depthwiseRegularizer=(0,w.Bm)(t.depthwiseRegularizer)}build(t){if((t=(0,c.U$)(t)).length<4)throw new p.Qp(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new p.Qp(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const s=t[e];this.depthwiseKernel=this.addWeight("depthwise_kernel",[this.kernelSize[0],this.kernelSize[1],s,this.depthMultiplier],null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.bias=this.useBias?this.addWeight("bias",[s*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):null,this.built=!0}call(t,e){return(0,r.DZQ)(()=>{let s=function ns(i,n,t=[1,1],e="valid",s,a){return(0,r.DZQ)(()=>{null==s&&(s=(0,et.VI)()),(0,L.uM)(s);let o=h(i,s);if(4!==i.rank)throw new p.Qp(`Input for depthwiseConv2d is required to be 4-D, but is instead ${i.rank}-D`);if(4!==n.rank)throw new p.Qp(`depthwiseKernel is required to be 4-D, but is instead ${n.rank}-D`);return o=r.Gl3(o,n,t,"same"===e?"same":"valid","NHWC",a),"channelsFirst"===s&&(o=r.mgz(o,[0,3,1,2])),o})}(t=(0,c.un)(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(s=D.ni(s,this.bias.read(),this.dataFormat)),null!=this.activation&&(s=this.activation.apply(s)),s})}computeOutputShape(t){t=(0,c.U$)(t);const s="channelsFirst"===this.dataFormat?t[3]:t[2],a="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,o=(0,G.Ol)("channelsFirst"===this.dataFormat?t[2]:t[1],this.kernelSize[0],this.padding,this.strides[0]),u=(0,G.Ol)(s,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],a,o,u]:[t[0],o,u,a]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=(0,z.zo)(this.depthwiseInitializer),t.depthwiseRegularizer=(0,w.R9)(this.depthwiseRegularizer),t.depthwiseConstraint=(0,R.uH)(this.depthwiseRegularizer),t}}return i.className="DepthwiseConv2D",i})();r.JFn.registerClass(te);var X=l(89119),ee=l(75768),It=l(53340);function se(i,n,t,e){if(Array.isArray(i)){if(null!=n||null!=t)throw new p.Qp("When inputs is an array, neither initialState or constants should be provided");null!=e&&(t=i.slice(i.length-e,i.length),i=i.slice(0,i.length-e)),i.length>1&&(n=i.slice(1,i.length)),i=i[0]}function s(a){return null==a||Array.isArray(a)?a:[a]}return{inputs:i,initialState:n=s(n),constants:t=s(t)}}function ne(i,n,t,e=!1,s,a,o=!1,u=!1){return r.DZQ(()=>{const f=n.shape.length;if(f<3)throw new p.Qp(`Input should be at least 3D, but is ${f}D.`);const m=[1,0].concat(X.y1(2,f));if(n=r.mgz(n,m),null!=a)throw new p.EH("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=s&&((s=r.wgE(r.wgE(s,"bool"),"float32")).rank===f-1&&(s=r.UG6(s,-1)),s=r.mgz(s,m)),e&&(n=r.BEg(n,0),null!=s&&(s=r.BEg(s,0)));const v=[];let x,N=t;const F=n.shape[0],J=r.K$i(n);let Y,U;null!=s&&(Y=r.K$i(s));for(let j=0;j<F;++j){const st=J[j],at=r.DZQ(()=>i(st,N));if(null==s)x=at[0],N=at[1];else{const ft=r.DZQ(()=>{const lt=Y[j],ht=r.jbE(r.P61(lt),lt);return{output:r.WQq(r.lKK(at[0],lt),r.lKK(N[0],ht)),newStates:N.map((Rs,Es)=>r.WQq(r.lKK(at[1][Es],lt),r.lKK(Rs,ht)))}});x=ft.output,N=ft.newStates}u&&v.push(x)}return u&&(U=r.t$z(v,1)),[x,U,N]})}let Dt=(()=>{class i extends g.Wd{constructor(t){let e;if(super(t),null==t.cell)throw new p.Qp("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new Zt({cells:t.cell}):t.cell,null==e.stateSize)throw new p.Qp("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new g.eO({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;return X.y1(0,t).map(e=>null)}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){(0,c.TT)(t)&&(t=t[0]);let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const s=e[0];let a;if(a=this.returnSequences?[t[0],t[1],s]:[t[0],s],this.returnState){const o=[];for(const u of e)o.push([t[0],u]);return[a].concat(o)}return a}computeMask(t,e){return r.DZQ(()=>{Array.isArray(e)&&(e=e[0]);const s=this.returnSequences?e:null;if(this.returnState){const a=this.states.map(o=>null);return[s].concat(a)}return s})}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let s=0;s<t;++s)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new p.EH("Constants support is not implemented in RNN yet.");(0,c.TT)(t)&&(t=t[0]);const s=this.stateful?t[0]:null,a=t.slice(2);this.inputSpec[0]=new g.eO({shape:[s,null,...a]});const o=[t[0]].concat(t.slice(2));let u;if(this.cell.build(o),u=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!r.ZSL.arraysEqual(this.stateSpec.map(f=>f.shape[f.shape.length-1]),u))throw new p.Qp(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=u.map(f=>new g.eO({shape:[null,f]}));this.stateful&&this.resetStates()}resetStates(t,e=!1){(0,r.DZQ)(()=>{if(!this.stateful)throw new p.l7("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new p.Qp("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)this.states_=Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(a=>r.Ul9([s,a])):[r.Ul9([s,this.cell.stateSize])];else if(null==t)r.ASo(this.states_),null!=this.keptStates&&(r.ASo(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(a=>r.Ul9([s,a])):this.states_[0]=r.Ul9([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new p.Qp(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===e?this.keptStates.push(this.states_.slice()):r.ASo(this.states_);for(let a=0;a<this.states_.length;++a){const o=t[a],u=Array.isArray(this.cell.stateSize)?this.cell.stateSize[a]:this.cell.stateSize,f=[s,u];if(!r.ZSL.arraysEqual(o.shape,f))throw new p.Qp(`State ${a} is incompatible with layer ${this.name}: expected shape=${f}, received shape=${o.shape}`);this.states_[a]=o}}this.states_=this.states_.map(a=>r.aCs(a.clone()))})}apply(t,e){let s=null==e?null:e.initialState,a=null==e?null:e.constants;null==e&&(e={});const o=se(t,s,a,this.numConstants);t=o.inputs,s=o.initialState,a=o.constants;let u=[],f=[];if(null!=s){e.initialState=s,u=u.concat(s),this.stateSpec=[];for(const v of s)this.stateSpec.push(new g.eO({shape:v.shape}));f=f.concat(this.stateSpec)}if(null!=a&&(e.constants=a,u=u.concat(a),this.numConstants=a.length),u[0]instanceof g.Ar){const v=[t].concat(u),x=this.inputSpec.concat(f),N=this.inputSpec;this.inputSpec=x;const F=super.apply(v,e);return this.inputSpec=N,F}return super.apply(t,e)}call(t,e){return(0,r.DZQ)(()=>{const s=null==e?null:e.mask,a=null==e?null:e.training;let o=null==e?null:e.initialState;t=(0,c.un)(t),null==o&&(o=this.stateful?this.states_:this.getInitialState(t));const u=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(o.length!==u)throw new p.Qp(`RNN Layer has ${u} state(s) but was passed ${o.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const f={training:a},v=ne((Y,U)=>{const j=this.cell.call([Y].concat(U),f);return[j[0],j.slice(1)]},t,o,this.goBackwards,s,null,this.unroll,this.returnSequences),x=v[0],N=v[1],F=v[2];this.stateful&&this.resetStates(F,a);const J=this.returnSequences?N:x;return this.returnState?[J].concat(F):J})}getInitialState(t){return(0,r.DZQ)(()=>{let e=r.Ul9(t.shape);return e=r.czq(e,[1,2]),e=D.UG(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(s=>s>1?D.Vs(e,[1,s]):e):this.cell.stateSize>1?[D.Vs(e,[1,this.cell.stateSize])]:[e]})}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const s=this.cell.getConfig();return this.getClassName()===i.className&&(e.cell={className:this.cell.getClassName(),config:s}),Object.assign(Object.assign(Object.assign({},s),t),e)}static fromConfig(t,e,s={}){const o=(0,It.i)(e.cell,s);return new t(Object.assign(e,{cell:o}))}}return i.className="RNN",i})();r.JFn.registerClass(Dt);class Rt extends g.Wd{}let Ut=(()=>{class i extends Rt{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,C.oo)(this.units,"units"),this.activation=(0,P.b_)(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,z.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer),this.recurrentRegularizer=(0,w.Bm)(t.recurrentRegularizer),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.kernelConstraint=(0,R.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,R.YZ)(t.recurrentConstraint),this.biasConstraint=(0,R.YZ)(t.biasConstraint),this.dropout=X.jk([1,X.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=X.jk([1,X.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,c.U$)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.bias=this.useBias?this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):null,this.built=!0}call(t,e){return(0,r.DZQ)(()=>{if(2!==t.length)throw new p.Qp(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let s=t[1];t=t[0];const a=null!=e.training&&e.training;let o;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=vt({ones:()=>r.P61(t),rate:this.dropout,training:a,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=vt({ones:()=>r.P61(s),rate:this.recurrentDropout,training:a,dropoutFunc:this.dropoutFunc}));const u=this.dropoutMask,f=this.recurrentDropoutMask;o=D.Om(null!=u?r.lKK(t,u):t,this.kernel.read()),null!=this.bias&&(o=D.ni(o,this.bias.read())),null!=f&&(s=r.lKK(s,f));let m=r.WQq(o,D.Om(s,this.recurrentKernel.read()));return null!=this.activation&&(m=this.activation.apply(m)),[m,m]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,P.Bu)(this.activation),useBias:this.useBias,kernelInitializer:(0,z.zo)(this.kernelInitializer),recurrentInitializer:(0,z.zo)(this.recurrentInitializer),biasInitializer:(0,z.zo)(this.biasInitializer),kernelRegularizer:(0,w.R9)(this.kernelRegularizer),recurrentRegularizer:(0,w.R9)(this.recurrentRegularizer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),kernelConstraint:(0,R.uH)(this.kernelConstraint),recurrentConstraint:(0,R.uH)(this.recurrentConstraint),biasConstraint:(0,R.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign(Object.assign({},t),e)}}return i.className="SimpleRNNCell",i})();r.JFn.registerClass(Ut);let ie=(()=>{class i extends Dt{constructor(t){t.cell=new Ut(t),super(t)}call(t,e){return(0,r.DZQ)(()=>(null!=this.cell.dropoutMask&&(r.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),super.call(t,{mask:null==e?null:e.mask,training:null==e?null:e.training,initialState:null==e?null:e.initialState})))}static fromConfig(t,e){return new t(e)}}return i.className="SimpleRNN",i})();r.JFn.registerClass(ie);let $t=(()=>{class i extends Rt{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new p.Qp("GRUCell does not support reset_after parameter set to true.");this.units=t.units,(0,C.oo)(this.units,"units"),this.activation=(0,P.b_)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,P.b_)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,z.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer),this.recurrentRegularizer=(0,w.Bm)(t.recurrentRegularizer),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.kernelConstraint=(0,R.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,R.YZ)(t.recurrentConstraint),this.biasConstraint=(0,R.YZ)(t.biasConstraint),this.dropout=X.jk([1,X.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=X.jk([1,X.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,c.U$)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.bias=this.useBias?this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):null,this.built=!0}call(t,e){return(0,r.DZQ)(()=>{if(2!==t.length)throw new p.Qp(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const s=null!=e.training&&e.training;let a=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=vt({ones:()=>r.P61(t),rate:this.dropout,training:s,count:3,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=vt({ones:()=>r.P61(a),rate:this.recurrentDropout,training:s,count:3,dropoutFunc:this.dropoutFunc}));const u=this.recurrentDropoutMask;let f,m,v;0<this.dropout&&this.dropout<1&&(t=r.lKK(t,this.dropoutMask[0]));let x=D.Om(t,this.kernel.read());this.useBias&&(x=D.ni(x,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(a=r.lKK(a,u[0]));const N=this.recurrentKernel.read(),[F,J]=r.lDo(N,[2*this.units,this.units],N.rank-1),Y=D.Om(a,F),[U,j,st]=r.lDo(x,3,x.rank-1),[at,ft]=r.lDo(Y,2,Y.rank-1);f=this.recurrentActivation.apply(r.WQq(U,at)),m=this.recurrentActivation.apply(r.WQq(j,ft));const lt=D.Om(r.lKK(m,a),J);v=this.activation.apply(r.WQq(st,lt));const ht=r.WQq(r.lKK(f,a),r.lKK(r.WQq(1,r.HZy(f)),v));return[ht,ht]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,P.Bu)(this.activation),recurrentActivation:(0,P.Bu)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,z.zo)(this.kernelInitializer),recurrentInitializer:(0,z.zo)(this.recurrentInitializer),biasInitializer:(0,z.zo)(this.biasInitializer),kernelRegularizer:(0,w.R9)(this.kernelRegularizer),recurrentRegularizer:(0,w.R9)(this.recurrentRegularizer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),kernelConstraint:(0,R.uH)(this.kernelConstraint),recurrentConstraint:(0,R.uH)(this.recurrentConstraint),biasConstraint:(0,R.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign(Object.assign({},t),e)}}return i.className="GRUCell",i})();r.JFn.registerClass($t);let re=(()=>{class i extends Dt{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new $t(t),super(t)}call(t,e){return(0,r.DZQ)(()=>(null!=this.cell.dropoutMask&&(r.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),super.call(t,{mask:null==e?null:e.mask,training:null==e?null:e.training,initialState:null==e?null:e.initialState})))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}return i.className="GRU",i})();r.JFn.registerClass(re);let Et=(()=>{class i extends Rt{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,C.oo)(this.units,"units"),this.activation=(0,P.b_)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,P.b_)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,z.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer),this.recurrentRegularizer=(0,w.Bm)(t.recurrentRegularizer),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.kernelConstraint=(0,R.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,R.YZ)(t.recurrentConstraint),this.biasConstraint=(0,R.YZ)(t.biasConstraint),this.dropout=X.jk([1,X.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=X.jk([1,X.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;let a;if(t=(0,c.U$)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const o=this.biasInitializer,u=this.units;a=new((e=class extends z.H4{apply(m,v){const x=o.apply([u]),N=(new z.sN).apply([u]),F=o.apply([2*u]);return D.ly(D.ly(x,N),F)}}).className="CustomInit",e)}else a=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,a,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return(0,r.DZQ)(()=>{const s=null!=e.training&&e.training;if(3!==t.length)throw new p.Qp(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let a=t[1];const o=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=vt({ones:()=>r.P61(t),rate:this.dropout,training:s,count:4,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=vt({ones:()=>r.P61(a),rate:this.recurrentDropout,training:s,count:4,dropoutFunc:this.dropoutFunc}));const f=this.recurrentDropoutMask;let m,v,x,N;0<this.dropout&&this.dropout<1&&(t=r.lKK(t,this.dropoutMask[0]));let F=D.Om(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(a=r.lKK(a,f[0])),F=r.WQq(F,D.Om(a,this.recurrentKernel.read())),this.useBias&&(F=D.ni(F,this.bias.read()));const[J,Y,U,j]=r.lDo(F,4,F.rank-1);m=this.recurrentActivation.apply(J),v=this.recurrentActivation.apply(Y),x=r.WQq(r.lKK(v,o),r.lKK(m,this.activation.apply(U))),N=this.recurrentActivation.apply(j);const st=r.lKK(N,this.activation.apply(x));return[st,st,x]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,P.Bu)(this.activation),recurrentActivation:(0,P.Bu)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,z.zo)(this.kernelInitializer),recurrentInitializer:(0,z.zo)(this.recurrentInitializer),biasInitializer:(0,z.zo)(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:(0,w.R9)(this.kernelRegularizer),recurrentRegularizer:(0,w.R9)(this.recurrentRegularizer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),kernelConstraint:(0,R.uH)(this.kernelConstraint),recurrentConstraint:(0,R.uH)(this.recurrentConstraint),biasConstraint:(0,R.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign(Object.assign({},t),e)}}return i.className="LSTMCell",i})();r.JFn.registerClass(Et);let ae=(()=>{class i extends Dt{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Et(t),super(t)}call(t,e){return(0,r.DZQ)(()=>(null!=this.cell.dropoutMask&&(r.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),super.call(t,{mask:null==e?null:e.mask,training:null==e?null:e.training,initialState:null==e?null:e.initialState})))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}return i.className="LSTM",i})();r.JFn.registerClass(ae);let Zt=(()=>{class i extends Rt{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return(0,r.DZQ)(()=>{let s=t.slice(1);const a=[];for(const f of this.cells.slice().reverse())Array.isArray(f.stateSize)?a.push(s.splice(0,f.stateSize.length)):a.push(s.splice(0,1));a.reverse();const o=[];let u;for(let f=0;f<this.cells.length;++f){const m=this.cells[f];s=a[f],u=0===f?[t[0]].concat(s):[u[0]].concat(s),u=m.call(u,e),o.push(u.slice(1))}s=[];for(const f of o.slice().reverse())s.push(...f);return[u[0]].concat(s)})}build(t){let e;(0,c.TT)(t)&&(t=t[0]),this.cells.forEach((s,a)=>{(0,L.IU)(`RNNCell_${a}`,()=>{s.build(t),e=Array.isArray(s.stateSize)?s.stateSize[0]:s.stateSize,t=[t[0],e]})}),this.built=!0}getConfig(){const t=super.getConfig(),a={cells:this.cells.map(o=>({className:o.getClassName(),config:o.getConfig()}))};return Object.assign(Object.assign({},t),a)}static fromConfig(t,e,s={}){const a=[];for(const o of e.cells)a.push((0,It.i)(o,s));return new t({cells:a})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const s of this.cells)e.push(...s.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return(0,ee.ex)(t)}setWeights(t){const e=[];for(const s of this.cells){const o=t.splice(s.weights.length);for(let u=0;u<s.weights.length;++u)e.push([s.weights[u],o[u]])}(0,ee.UM)(e)}}return i.className="StackedRNNCells",i})();function vt(i){const{ones:n,rate:t,training:e=!1,count:s=1,dropoutFunc:a}=i,o=()=>null!=a?a(n(),t):D.EZ(n(),t),u=()=>D.Ls(o,n,e);return!s||s<=1?r.aCs(u().clone()):Array(s).fill(void 0).map(u).map(m=>r.aCs(m.clone()))}r.JFn.registerClass(Zt);let rs=(()=>{class i extends Dt{constructor(t){if(t.unroll)throw new p.EH("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new p.EH("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new g.eO({ndim:5})]}call(t,e){return r.DZQ(()=>{if(null!=this.cell.dropoutMask&&(r.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new p.Qp("ConvRNN2D cell does not support constants");return super.call(t,{mask:null==e?null:e.mask,training:null==e?null:e.training,initialState:null==e?null:e.initialState})})}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return r.DZQ(()=>{const{stateSize:e}=this.cell,a=this.computeSingleOutputShape(t.shape),o=[a[0],...a.slice(2)],u=r.Ul9(o);return Array.isArray(e)?Array(e.length).fill(u):[u]})}resetStates(t,e=!1){r.DZQ(()=>{if(!this.stateful)throw new p.l7("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,a=this.computeSingleOutputShape(s),o=[a[0],...a.slice(2)];if(null==s[0])throw new p.Qp("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())this.states_=Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(()=>r.Ul9(o)):[r.Ul9(o)];else if(null==t)r.ASo(this.states_),null!=this.keptStates&&(r.ASo(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(()=>r.Ul9(o)):this.states_[0]=r.Ul9(o);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new p.Qp(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);e?this.keptStates.push(this.states_.slice()):r.ASo(this.states_);for(let f=0;f<this.states_.length;++f){const m=t[f],v=o;if(!r.ZSL.arraysEqual(m.shape,v))throw new p.Qp(`State ${f} is incompatible with layer ${this.name}: expected shape=${v}, received shape=${m.shape}`);this.states_[f]=m}}this.states_=this.states_.map(f=>r.aCs(f.clone()))})}computeSingleOutputShape(t){const{dataFormat:e,filters:s,kernelSize:a,padding:o,strides:u,dilationRate:f}=this.cell,m="channelsFirst"===e,x=t[m?4:3],N=(0,G.Ol)(t[m?3:2],a[0],o,u[0],f[0]),F=(0,G.Ol)(x,a[1],o,u[1],f[1]);return[...t.slice(0,2),...m?[s,N,F]:[N,F,s]]}}return i.className="ConvRNN2D",i})(),Kt=(()=>{class i extends Et{constructor(t){const{filters:e,kernelSize:s,strides:a,padding:o,dataFormat:u,dilationRate:f}=t;super(Object.assign(Object.assign({},t),{units:e})),this.filters=e,(0,C.oo)(this.filters,"filters"),this.kernelSize=(0,G.J)(s,2,"kernelSize"),this.kernelSize.forEach(m=>(0,C.oo)(m,"kernelSize")),this.strides=(0,G.J)(a||1,2,"strides"),this.strides.forEach(m=>(0,C.oo)(m,"strides")),this.padding=o||"valid",(0,L.tB)(this.padding),this.dataFormat=u||"channelsLast",(0,L.uM)(this.dataFormat),this.dilationRate=(0,G.J)(f||1,2,"dilationRate"),this.dilationRate.forEach(m=>(0,C.oo)(m,"dilationRate"))}build(t){var e;t=(0,c.U$)(t);const s="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[s])throw new p.Qp(`The channel dimension of the input should be defined. Found ${t[s]}`);const u=this.kernelSize.concat([t[s],4*this.filters]);this.kernel=this.addWeight("kernel",u,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const f=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",f,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let m;if(this.unitForgetBias){const v=this.biasInitializer,x=this.filters;m=new((e=class extends z.H4{apply(F,J){const Y=v.apply([x]),U=r.SaS([x]),j=v.apply([2*x]);return D.u1([Y,U,j])}}).className="CustomInit",e)}else m=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,m,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return r.DZQ(()=>{if(3!==t.length)throw new p.Qp(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const s=e.training||!1,a=t[0],o=t[1],u=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=vt({ones:()=>r.P61(a),rate:this.dropout,training:s,count:4,dropoutFunc:this.dropoutFunc}));const m=this.dropoutMask,v=(dn,Ts,fn)=>Ts&&Ts[fn]?r.lKK(Ts[fn],dn):dn;let x=v(a,m,0),N=v(a,m,1),F=v(a,m,2),J=v(a,m,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=vt({ones:()=>r.P61(o),rate:this.recurrentDropout,training:s,count:4,dropoutFunc:this.dropoutFunc}));const Y=this.recurrentDropoutMask;let U=v(o,Y,0),j=v(o,Y,1),st=v(o,Y,2),at=v(o,Y,3);const[lt,ht,pt,Jt]=r.lDo(this.kernel.read(),4,3),[Rs,Es,$n,Zn]=this.useBias?r.lDo(this.bias.read(),4):[null,null,null,null];x=this.inputConv(x,lt,Rs,this.padding),N=this.inputConv(N,ht,Es,this.padding),F=this.inputConv(F,pt,$n,this.padding),J=this.inputConv(J,Jt,Zn,this.padding);const[Kn,jn,Hn,Jn]=r.lDo(this.recurrentKernel.read(),4,3);U=this.recurrentConv(U,Kn),j=this.recurrentConv(j,jn),st=this.recurrentConv(st,Hn),at=this.recurrentConv(at,Jn);const Vn=this.recurrentActivation.apply(r.WQq(x,U)),Gn=this.recurrentActivation.apply(r.WQq(N,j)),hn=r.WQq(r.lKK(Gn,u),r.lKK(Vn,this.activation.apply(r.WQq(F,st)))),cn=r.lKK(this.recurrentActivation.apply(r.WQq(J,at)),this.activation.apply(hn));return[cn,cn,hn]})}getConfig(){const s=function(i,n){var t={};for(var e in i)Object.prototype.hasOwnProperty.call(i,e)&&n.indexOf(e)<0&&(t[e]=i[e]);if(null!=i&&"function"==typeof Object.getOwnPropertySymbols){var s=0;for(e=Object.getOwnPropertySymbols(i);s<e.length;s++)n.indexOf(e[s])<0&&Object.prototype.propertyIsEnumerable.call(i,e[s])&&(t[e[s]]=i[e[s]])}return t}(super.getConfig(),["units"]),a={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign(Object.assign({},s),a)}inputConv(t,e,s,a){const o=r.Xtf(t,e,this.strides,a||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return s?D.ni(o,s,this.dataFormat):o}recurrentConv(t,e){return r.Xtf(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}return i.className="ConvLSTM2DCell",i})();r.JFn.registerClass(Kt),r.JFn.registerClass((()=>{class i extends rs{constructor(t){const e=new Kt(t);super(Object.assign(Object.assign({},t),{cell:e}))}static fromConfig(t,e){return new t(e)}}return i.className="ConvLSTM2D",i})());let jt=(()=>{class i extends g.Wd{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,s=[];for(let a=0;a<this.noiseShape.length;++a)s.push(null==this.noiseShape[a]?e[a]:this.noiseShape[a]);return s}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const s=(0,c.un)(t);if(0<this.rate&&this.rate<1){const a=null!=e.training&&e.training,o=this.getNoiseShape(s);return D.Ls(()=>D.EZ(s,this.rate,o,this.seed),()=>s,a)}return t})}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}return i.className="Dropout",i})();r.JFn.registerClass(jt),r.JFn.registerClass((()=>{class i extends jt{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}return i.className="SpatialDropout1D",i})());let ue=(()=>{class i extends g.Wd{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,(0,C.oo)(this.units,"units"),this.activation=(0,P.b_)(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=(0,z.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=(0,z.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=(0,R.YZ)(t.kernelConstraint),this.biasConstraint=(0,R.YZ)(t.biasConstraint),this.kernelRegularizer=(0,w.Bm)(t.kernelRegularizer),this.biasRegularizer=(0,w.Bm)(t.biasRegularizer),this.activityRegularizer=(0,w.Bm)(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=(0,c.U$)(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=(0,c.U$)(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const s=(0,c.un)(t),a=(0,C.Cd)(this.activation.getClassName());let o;return null!=a?o=D.Om(s,this.kernel.read(),a,this.bias?this.bias.read():null):(o=D.Om(s,this.kernel.read()),null!=this.bias&&(o=D.ni(o,this.bias.read())),null!=this.activation&&(o=this.activation.apply(o))),o})}getConfig(){const t={units:this.units,activation:(0,P.Bu)(this.activation),useBias:this.useBias,kernelInitializer:(0,z.zo)(this.kernelInitializer),biasInitializer:(0,z.zo)(this.biasInitializer),kernelRegularizer:(0,w.R9)(this.kernelRegularizer),biasRegularizer:(0,w.R9)(this.biasRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),kernelConstraint:(0,R.uH)(this.kernelConstraint),biasConstraint:(0,R.uH)(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Dense",i})();r.JFn.registerClass(ue);let he=(()=>{class i extends g.Wd{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=(0,c.U$)(t);for(const e of t.slice(1))if(null==e)throw new p.Qp(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],(0,X.no)(t,1)]}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);let s=(0,c.un)(t);if("channelsFirst"===this.dataFormat&&s.rank>1){const a=[0];for(let o=2;o<s.rank;++o)a.push(o);a.push(1),s=(0,r.mgz)(s,a)}return D.PS(s)})}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}return i.className="Flatten",i})();r.JFn.registerClass(he),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.supportsMasking=!0,this.activation=(0,P.b_)(t.activation)}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const s=(0,c.un)(t);return this.activation.apply(s)})}getConfig(){const t={activation:(0,P.Bu)(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Activation",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return(0,r.DZQ)(()=>(t=(0,c.un)(t),D.ux(t,this.n)))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}return i.className="RepeatVector",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.targetShape=t.targetShape;for(let e=0;e<this.targetShape.length;++e)this.isUnknown(this.targetShape[e])&&(this.targetShape[e]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const s="Total size of new array must be unchanged.",a=e.slice();let o=1,u=null;for(let m=0;m<a.length;++m){const v=a[m];if(this.isUnknown(v)){if(null!==u)throw new p.Qp("Can only specifiy one unknown dimension.");u=m}else o*=v}const f=(0,X.no)(t);if(null!==u){if(0===o||f%o!=0)throw new p.Qp(s);a[u]=f/o}else if(f!==o)throw new p.Qp(s);return a}computeOutputShape(t){let e=!1;for(let s=0;s<t.length;++s)if(this.isUnknown(t[s])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const s=(0,c.un)(t),a=s.shape,o=a.slice(0,1).concat(this.fixUnknownDimension(a.slice(1),this.targetShape));return(0,r.tQQ)(s,o)})}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Reshape",i})());let pe=(()=>{class i extends g.Wd{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const e=(0,X.y1)(1,t.dims.length+1);if(!r.ZSL.arraysEqual(t.dims.slice().sort(),e))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new g.eO({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=(0,c.U$)(t)).slice();return this.dims.forEach((s,a)=>{e[a+1]=t[s]}),e}call(t,e){return(0,r.mgz)((0,c.un)(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Permute",i})();r.JFn.registerClass(pe),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const s=(0,c.un)(t);return(0,r.bzn)((0,r.Ec)(s,this.maskValue),-1)}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const s=(0,c.un)(t),u=(0,r.bzn)((0,r.Ec)(s,this.maskValue),-1,!0);return(0,r.lKK)(s,(0,r.wgE)(u,s.dtype))})}}return i.className="Masking",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=null==t.inputLength?[e,null]:[e].concat(C.st(t.inputLength))}this.inputDim=t.inputDim,C.oo(this.inputDim,"inputDim"),this.outputDim=t.outputDim,C.oo(this.outputDim,"outputDim"),this.embeddingsInitializer=(0,z.Fe)(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=(0,w.Bm)(t.embeddingsRegularizer),this.activityRegularizer=(0,w.Bm)(t.activityRegularizer),this.embeddingsConstraint=(0,R.YZ)(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return(0,r.DZQ)(()=>this.maskZero?(t=(0,c.un)(t),(0,r.Ec)(t,(0,r.POl)(t))):null)}computeOutputShape(t){if(t=(0,c.U$)(t),null==this.inputLength)return[...t,this.outputDim];const e=C.st(this.inputLength);if(e.length!==t.length-1)throw new p.Qp(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let s=0;for(let a=0;a<e.length;++a){const o=e[a],u=t[a+1];if(null!=o&&null!=u&&o!==u)throw new p.Qp(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==o&&(e[s]=u),s++}}return[t[0],...e,this.outputDim]}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);let s=(0,c.un)(t);"int32"!==s.dtype&&(s=D.wg(s,"int32"));const a=D.kg(this.embeddings.read(),(0,r.tQQ)(s,[s.size]));return(0,r.tQQ)(a,(0,c.U$)(this.computeOutputShape(s.shape)))})}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:(0,z.zo)(this.embeddingsInitializer),embeddingsRegularizer:(0,w.R9)(this.embeddingsRegularizer),activityRegularizer:(0,w.R9)(this.activityRegularizer),embeddingsConstraint:(0,R.uH)(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Embedding",i})());var ye=l(55294);class zt extends g.Wd{constructor(n){super(n||{}),this.supportsMasking=!0}mergeFunction(n){throw new p.EH}computeElementwiseOpOutputShape(n,t){if(null==n||null==t)return null;if(n.length<t.length)return this.computeElementwiseOpOutputShape(t,n);if(0===t.length)return n;const e=n.slice(0,n.length-t.length);for(let s=0;s<t.length;++s){const a=n[n.length-t.length+s],o=t[s];if(null==a||null==o||a<0||o<0)e.push(null);else if(1===a)e.push(o);else if(1===o)e.push(a);else{if(a!==o)throw new p.Qp("Operands could not be broadcast together with shapes "+JSON.stringify(n)+" "+JSON.stringify(t));e.push(a)}}return e}build(n){if(Array.isArray(n)&&!Array.isArray(n[0])&&(n=[(0,c.U$)(n)]),n.length<2)throw new p.Qp(`A merge layer should be called on an Array of at least 2 inputs. Got ${n.length} input(s).`);let t=[];for(const a of n)null!=a&&null!==a[0]&&t.push(a[0]);if(t=C.Am(t),t.length>1)throw new p.Qp(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(n)}.`);let e=null==n[0]?null:n[0].slice(1);for(let a=1;a<n.length;++a){const o=null==n[a]?null:n[a].slice(1);e=this.computeElementwiseOpOutputShape(e,o)}const s=n.map(a=>a.length);this.reshapeRequired=-1!==n.indexOf(null)||1!==C.Am(s).length}call(n,t){return(0,r.DZQ)(()=>{if(this.reshapeRequired){const e=[],s=n.map(a=>a.rank);if(-1===s.indexOf(null)){const a=X.T9(s);for(let o of n){const u=o.rank;for(let f=0;f<a-u;++f)o=D.UG(o,1);e.push(o)}return this.mergeFunction(e)}{let a=!1;for(const f of n){const m=f.rank;if(null==m){const v=f.shape,x=v[0],N=v.slice(1).concat([x]);let F=r.tQQ(f,[x].concat(X.no(v.slice(1))));F=r.mgz(F,[1,0]),F=r.tQQ(F,N),e.push(F),a=!0}else if(m>1){const v=X.y1(1,m).concat([0]);e.push(r.mgz(f,v)),a=!0}else e.push(f)}let o=this.mergeFunction(e);const u=o.rank;if(a)if(null==u){const f=o.shape,v=f[f.length-1],x=[v].concat(f.slice(0,f.length-1));o=r.tQQ(r.mgz(r.tQQ(o,[-1,v]),[1,0]),x)}else if(u>1){const f=[u-1].concat(X.y1(0,u-1));o=r.mgz(o,f)}return o}}return this.mergeFunction(n)})}computeOutputShape(n){let t;t=null==n[0]?null:n[0].slice(1);for(let s=1;s<n.length;++s){const a=null==n[s]?null:n[s].slice(1);t=this.computeElementwiseOpOutputShape(t,a)}let e=[];for(const s of n)null!=s&&null!==s[0]&&e.push(s[0]);return e=C.Am(e),t=1===e.length?e.concat(t):[null].concat(t),t}computeMask(n,t){return r.DZQ(()=>{if(null==t)return null;if(!Array.isArray(t))throw new p.Qp("`mask` should be an Array");if(!Array.isArray(n))throw new p.Qp("`inputs` should be an Array");if(t.length!==n.length)throw new p.Qp(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${n.length} vs ${t.length})`);if(t.every(s=>null==s))return null;let e=(t=t.map(s=>null==s?s:r.UG6(s,0)))[0];for(let s=1;s<t.length-1;++s)e=r.n76(e,t[s]);return e})}}let Tt=(()=>{class i extends zt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0].clone();for(let s=1;s<t.length;++s)e=r.WQq(e,t[s]);return e})}}return i.className="Add",i})();r.JFn.registerClass(Tt);let Nt=(()=>{class i extends zt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0].clone();for(let s=1;s<t.length;++s)e=r.lKK(e,t[s]);return e})}}return i.className="Multiply",i})();r.JFn.registerClass(Nt);let Ft=(()=>{class i extends zt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0].clone();for(let s=1;s<t.length;++s)e=r.WQq(e,t[s]);return r.lKK(1/t.length,e)})}}return i.className="Average",i})();r.JFn.registerClass(Ft);let Lt=(()=>{class i extends zt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0];for(let s=1;s<t.length;++s)e=r.PhQ(e,t[s]);return e})}}return i.className="Maximum",i})();r.JFn.registerClass(Lt);let Mt=(()=>{class i extends zt{constructor(t){super(t)}mergeFunction(t){return(0,r.DZQ)(()=>{let e=t[0];for(let s=1;s<t.length;++s)e=r.BpO(e,t[s]);return e})}}return i.className="Minimum",i})();r.JFn.registerClass(Mt);let Ot=(()=>{class i extends zt{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new p.Qp("A `Concatenate` layer should be called on a list of at least 2 inputs");let e=!0;for(const a of t)if(null!=a){e=!1;break}if(e)return;const s=[];for(let a=0;a<t.length;++a){const o=t[a].slice();o.splice(this.axis,1);let u=!1;for(const f of s)if(r.ZSL.arraysEqual(f,o)){u=!0;break}u||s.push(o)}if(s.length>1)throw new p.Qp("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return(0,r.DZQ)(()=>D.u1(t,this.axis))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new p.Qp("A `Concatenate` layer should be called on a list of inputs.");const e=t,s=e[0].slice(),a=this.axis<0?s.length+this.axis:this.axis;for(const o of e.slice(1)){if(null==s[a]||null==o[a]){s[a]=null;break}s[a]+=o[a]}return s}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new p.Qp("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new p.Qp("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new p.Qp(`Mismatch in the length of mask (${e.length}) and the legnth of inputs (${t.length})`);return r.DZQ(()=>{let s=!0;if(e.forEach(u=>{null==u||(s=!1)}),s)return null;const a=[];for(let u=0;u<t.length;++u)a.push(null==e[u]?r.wgE(r.P61(t[u]),"bool"):e[u].rank<t[u].rank?r.UG6(e[u],-1):e[u]);const o=r.xWs(a,this.axis);return r.Q7R(o,-1,!1)})}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Concatenate",i})();function St(i,n){for(;i<0;)i+=n;return i}r.JFn.registerClass(Ot);let ve=(()=>{class i extends zt{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){r.ZSL.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),()=>"A `Dot` layer should be called on a list of exactly 2 inputs.");const e=t[0],s=t[1];if(e.length>3||s.length>3)throw new p.EH("Dot layer does not support tensors of 4D or higher rank yet.");const a=this.interpretAxes(e,s);if(e[a[0]]!==s[a[1]])throw new p.Qp(`Dimension incompatibility: ${e[a[0]]} !== ${s[a[1]]}`)}mergeFunction(t){if(2!==t.length)throw new p.Qp(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let a,e=t[0],s=t[1];return a=Array.isArray(this.axes)?this.axes.map((o,u)=>St(o,t[u].shape.length)):[St(this.axes,e.shape.length),St(this.axes,s.shape.length)],this.normalize&&(e=(0,ye.Yq)(e,a[0]),s=(0,ye.Yq)(s,a[1])),function as(i,n,t){if(i.shape.length>3||n.shape.length>3)throw new p.EH("batchDot is not implemented for tensors of 4D or higher rank yet");if(r.ZSL.assert(i.shape.length>=2,()=>`batchDot requires the rank of x to be >= 2, but got ${i.shape.length}`),r.ZSL.assert(i.shape.length>=2,()=>`batchDot requires the rank of y to be >= 2, but got ${n.shape.length}`),"number"==typeof t&&(t=[t,t]),"complex64"===i.dtype||"complex64"===n.dtype)throw new p.EH("batchDot is not implemented for complex64-type Tensors yet.");const e=i.shape.length,s=n.shape.length;null==t&&(t=[e-1,s-2]);const a=t;return r.DZQ(()=>{let o,u;if(e>s){o=e-s;const f=[];for(let m=0;m<o;++m)f.push(1);n=r.tQQ(n,n.shape.concat(f))}else if(s>e){o=s-e;const f=[];for(let m=0;m<o;++m)f.push(1);i=r.tQQ(i,i.shape.concat(f))}else o=0;if(u=2===i.shape.length&&2===n.shape.length?a[0]===a[1]?r.czq(r.lKK(i,n),a[0]):r.czq(r.lKK(r.mgz(i,[1,0]),n),a[1]):r.NoW(i,n,a[0]!==i.shape.length-1,a[1]===n.shape.length-1),o>0){let f;f=e>s?e+s-3:e-1;const m=[];for(let v=f;v<f+o;++v)m.push(v);u=r.r2V(u,m)}return 1===u.shape.length&&(u=r.UG6(u,1)),u})}(e,s,a)}interpretAxes(t,e){let s;return s=Array.isArray(this.axes)?this.axes:[St(this.axes,t.length),St(this.axes,e.length)],s}computeOutputShape(t){r.ZSL.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),()=>"A `Dot` layer should be called on a list of exactly 2 inputs.");const e=t[0].slice(),s=t[1].slice();if(e.length>3||s.length>3)throw new p.EH("Dot layer does not support tensors of 4D or higher rank yet.");const a=this.interpretAxes(e,s);e.splice(a[0],1),s.splice(a[1],1),s.splice(0,1);const o=e.concat(s);return 1===o.length&&o.push(1),o}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}return i.className="Dot",i})();function xt(i,n,t,e,s,a=.001){let o;if(2===i.rank)o=r.BFc(i,n,t,e,s,a);else if(3===i.rank)o=r.kSi(i,n,t,e,s,a);else{if(4!==i.rank)throw new p.EH(`batchNormalization is not implemented for array of rank ${i.rank} yet`);o=r.T5N(i,n,t,e,s,a)}return o}r.JFn.registerClass(ve),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const s=(0,c.un)(t);return D.Ls(()=>(0,r.WQq)(D.FE(s.shape,0,this.stddev),s),()=>s,e.training||!1)})}}return i.className="GaussianNoise",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,r.DZQ)(()=>{this.invokeCallHook(t,e);const s=(0,c.un)(t);if(this.rate>0&&this.rate<1){const a=()=>{const o=Math.sqrt(this.rate/(1-this.rate));return(0,r.lKK)(s,D.FE(s.shape,1,o))};return D.Ls(a,()=>s,e.training||!1)}return s})}}return i.className="GaussianDropout",i})()),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||(0,c.un)(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,r.DZQ)(()=>{if(this.rate<1&&this.rate>0){const s=this._getNoiseShape(t),a=()=>{const o=(0,c.un)(t),m=-1.7580993408473766;let v=(0,r.DQN)((0,r.YeY)(s),this.rate);v=D.wg(v,"float32");const x=((1-this.rate)*(1+this.rate*m**2))**-.5,N=-x*m*this.rate,F=(0,r.WQq)((0,r.lKK)(o,v),(0,r.lKK)((0,r.WQq)(v,-1),m));return(0,r.WQq)((0,r.lKK)(F,x),N)};return D.Ls(a,()=>(0,c.un)(t),e.training||!1)}return t})}}return i.className="AlphaDropout",i})());let Ce=(()=>{class i extends g.Wd{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=(0,z.Fe)(t.betaInitializer||"zeros"),this.gammaInitializer=(0,z.Fe)(t.gammaInitializer||"ones"),this.movingMeanInitializer=(0,z.Fe)(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=(0,z.Fe)(t.movingVarianceInitializer||"ones"),this.betaConstraint=(0,R.YZ)(t.betaConstraint),this.gammaConstraint=(0,R.YZ)(t.gammaConstraint),this.betaRegularizer=(0,w.Bm)(t.betaRegularizer),this.gammaRegularizer=(0,w.Bm)(t.gammaRegularizer)}build(t){t=(0,c.U$)(t);const e=this.axis>=0?this.axis:this.axis+t.length,s=t[e];if(null==s)throw new p.Qp(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new g.eO({ndim:t.length,axes:{[e]:s}})];const a=[s];this.scale&&(this.gamma=this.addWeight("gamma",a,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",a,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",a,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",a,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,e){return(0,r.DZQ)(()=>{const s=null!=e.training&&e.training,a=(0,c.un)(t),o=a.shape,u=o.length,f=X.y1(0,u),m=this.axis>=0?this.axis:this.axis+u;f.splice(m,1);const v=C.fD(1,u);v[m]=o[m];const x=f.slice();x.sort();const N=!r.ZSL.arraysEqual(x,X.y1(0,u).slice(0,u-1));if(!s)return(()=>{if(N){const at=(0,r.tQQ)(this.movingMean.read(),v),ft=(0,r.tQQ)(this.movingVariance.read(),v),lt=this.center?(0,r.tQQ)(this.beta.read(),v):null,ht=this.scale?(0,r.tQQ)(this.gamma.read(),v):null;return xt(a,at,ft,lt,ht,this.epsilon)}return xt(a,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[J,Y,U]=function us(i,n,t,e,s=.001){return r.ZSL.arraysEqual(e.slice().sort(),X.y1(0,i.rank-1))?function os(i,n,t,e,s=.001){return(0,r.DZQ)(()=>{const a=r.Clk(i,e),o=a.mean,u=a.variance;return[xt(i,o,u,t,n,s),o,u]})}(i,n,t,e,s):function ls(i,n,t,e,s=.001){return(0,r.DZQ)(()=>{const a=r.Clk(i,e),o=a.mean,u=a.variance,f=[];for(const J of X.y1(0,i.rank))-1!==e.indexOf(J)?f.push(1):f.push(i.shape[J]);const m=(0,r.tQQ)(o,f),v=(0,r.tQQ)(u,f),x=null==n?null:(0,r.tQQ)(n,f),N=null==t?null:(0,r.tQQ)(t,f);return[xt(i,m,v,N,x,s),o,u]})}(i,n,t,e,s)}(a,this.gamma.read(),this.beta.read(),f,this.epsilon),j=(at,ft,lt)=>{r.DZQ(()=>{const ht=1-lt,pt=at.read(),Jt=r.lKK(r.jbE(pt,ft),ht);at.write(r.jbE(pt,Jt))})};return(()=>{j(this.movingMean,Y,this.momentum),j(this.movingVariance,U,this.momentum)})(),J})}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:(0,z.zo)(this.betaInitializer),gammaInitializer:(0,z.zo)(this.gammaInitializer),movingMeanInitializer:(0,z.zo)(this.movingMeanInitializer),movingVarianceInitializer:(0,z.zo)(this.movingVarianceInitializer),betaRegularizer:(0,w.R9)(this.betaRegularizer),gammaRegularizer:(0,w.R9)(this.gammaRegularizer),betaConstraint:(0,R.uH)(this.betaConstraint),gammaConstraint:(0,R.uH)(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}return i.className="BatchNormalization",i})();r.JFn.registerClass(Ce);let Ae=(()=>{class i extends g.Wd{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const e of this.axis)if(!Number.isInteger(e))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=(0,z.Fe)(t.betaInitializer||"zeros"),this.gammaInitializer=(0,z.Fe)(t.gammaInitializer||"ones"),this.betaRegularizer=(0,w.Bm)(t.betaRegularizer),this.gammaRegularizer=(0,w.Bm)(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=(0,c.U$)(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let o=0;o<this.axis.length;++o)this.axis[o]<0&&(this.axis[o]+=e);for(const o of this.axis)if(o<0||o>=e)throw new Error(`Invalid axis: ${o}`);if(this.axis.length!==C.Am(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const s=this.axis.map(o=>t[o]),a=!0;this.gamma=this.scale?this.addWeight("gamma",s,"float32",this.gammaInitializer,this.gammaRegularizer,a):null,this.beta=this.center?this.addWeight("beta",s,"float32",this.betaInitializer,this.betaRegularizer,a):null,this.built=!0}call(t,e){const s=(0,c.un)(t),a=s.shape,o=a.length;return(0,r.DZQ)(()=>{let{mean:f,variance:m}=(0,r.Clk)(s,this.axis,!0);const v=C.fD(1,o);for(const U of this.axis)v[U]=a[U];const x=U=>null!=U&&U.shape.length!==o?r.tQQ(U,v):U;let N=this.scale?x(this.gamma.read()):null,F=this.center?x(this.beta.read()):null;const J=[],Y=[];for(let U=0;U<o;++U)-1!==this.axis.indexOf(U)?(J.push(a[U]),Y.push(1)):(J.push(1),Y.push(a[U]));return f=r.Vsq(f,J),m=r.Vsq(m,J),null!=N&&(N=r.Vsq(N,Y)),null!=F&&(F=r.Vsq(F,Y)),xt(s,f,m,F,N,this.epsilon)})}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:(0,z.zo)(this.betaInitializer),gammaInitializer:(0,z.zo)(this.gammaInitializer),betaRegularizer:(0,w.R9)(this.betaRegularizer),gammaRegularizer:(0,w.R9)(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}return i.className="LayerNormalization",i})();r.JFn.registerClass(Ae);let Ie=(()=>{class i extends g.Wd{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?(0,et.VI)():t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new p.Qp(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,s;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],s=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new p.Qp(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new p.Qp(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);s=t.padding[1]}this.padding=[e,s]}this.inputSpec=[new g.eO({ndim:4})]}computeOutputShape(t){let e,s;return t=(0,c.U$)(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,s=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,s]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,s=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,s,t[3]])}call(t,e){return(0,r.DZQ)(()=>function hs(i,n,t){return(0,r.DZQ)(()=>{if(4!==i.rank)throw new p.Qp(`temporalPadding expects input tensor to be 4-D, but received a ${i.rank}-D tensor.`);if(null==n&&(n=[[1,1],[1,1]]),2!==n.length||2!==n[0].length||2!==n[1].length)throw new p.Qp("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==t&&(t=(0,et.VI)()),"channelsLast"!==t&&"channelsFirst"!==t)throw new p.Qp(`Unknown data format: ${t}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let e;return e="channelsFirst"===t?[[0,0],[0,0],n[0],n[1]]:[[0,0],n[0],n[1],[0,0]],r.eVF(i,e)})}((0,c.un)(t),this.padding,this.dataFormat))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}return i.className="ZeroPadding2D",i})();function kt(i,n,t,e,s,a){return(0,r.DZQ)(()=>{let o;(0,L.uM)(s),(0,L.Kx)(a),(0,L.tB)(e),null==t&&(t=[1,1]),null==e&&(e="valid"),null==s&&(s=(0,et.VI)()),null==a&&(a="max"),i=h(i,s);const u="same"===e?"same":"valid";return o="max"===a?r.jgi(i,n,t,u):r.$jT(i,n,t,u),"channelsFirst"===s&&(o=r.mgz(o,[0,3,1,2])),o})}function De(i,n,t,e,s,a){return(0,r.DZQ)(()=>{let o;(0,L.uM)(s),(0,L.Kx)(a),(0,L.tB)(e),null==t&&(t=[1,1,1]),null==e&&(e="valid"),null==s&&(s=(0,et.VI)()),null==a&&(a="max"),i=y(i,s);const u="same"===e?"same":"valid";return o="max"===a?r.NYV(i,n,t,u):r.sub(i,n,t,u),"channelsFirst"===s&&(o=r.mgz(o,[0,4,1,2,3])),o})}r.JFn.registerClass(Ie);class Se extends g.Wd{constructor(n){if(null==n.poolSize&&(n.poolSize=2),super(n),"number"==typeof n.poolSize)this.poolSize=[n.poolSize];else{if(!Array.isArray(n.poolSize)||1!==n.poolSize.length||"number"!=typeof n.poolSize[0])throw new p.Qp(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(n.poolSize)}`);this.poolSize=n.poolSize}if((0,C.oo)(this.poolSize,"poolSize"),null==n.strides)this.strides=this.poolSize;else if("number"==typeof n.strides)this.strides=[n.strides];else{if(!Array.isArray(n.strides)||1!==n.strides.length||"number"!=typeof n.strides[0])throw new p.Qp(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(n.strides)}`);this.strides=n.strides}(0,C.oo)(this.strides,"strides"),this.padding=null==n.padding?"valid":n.padding,(0,L.tB)(this.padding),this.inputSpec=[new g.eO({ndim:3})]}computeOutputShape(n){n=(0,c.U$)(n);const t=(0,G.Ol)(n[1],this.poolSize[0],this.padding,this.strides[0]);return[n[0],t,n[2]]}call(n,t){return(0,r.DZQ)(()=>{this.invokeCallHook(n,t),n=D.UG((0,c.un)(n),2);const e=this.poolingFunction((0,c.un)(n),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return r.r2V(e,[2])})}getConfig(){const n={poolSize:this.poolSize,padding:this.padding,strides:this.strides},t=super.getConfig();return Object.assign(n,t),n}}r.JFn.registerClass((()=>{class i extends Se{constructor(t){super(t)}poolingFunction(t,e,s,a,o){return(0,L.uM)(o),(0,L.tB)(a),kt(t,e,s,a,o,"max")}}return i.className="MaxPooling1D",i})()),r.JFn.registerClass((()=>{class i extends Se{constructor(t){super(t)}poolingFunction(t,e,s,a,o){return(0,L.uM)(o),(0,L.tB)(a),kt(t,e,s,a,o,"avg")}}return i.className="AveragePooling1D",i})());class Ee extends g.Wd{constructor(n){if(null==n.poolSize&&(n.poolSize=[2,2]),super(n),this.poolSize=Array.isArray(n.poolSize)?n.poolSize:[n.poolSize,n.poolSize],null==n.strides)this.strides=this.poolSize;else if(Array.isArray(n.strides)){if(2!==n.strides.length)throw new p.Qp(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${n.strides.length}.`);this.strides=n.strides}else this.strides=[n.strides,n.strides];(0,C.oo)(this.poolSize,"poolSize"),(0,C.oo)(this.strides,"strides"),this.padding=null==n.padding?"valid":n.padding,this.dataFormat=null==n.dataFormat?"channelsLast":n.dataFormat,(0,L.uM)(this.dataFormat),(0,L.tB)(this.padding),this.inputSpec=[new g.eO({ndim:4})]}computeOutputShape(n){n=(0,c.U$)(n);let t="channelsFirst"===this.dataFormat?n[2]:n[1],e="channelsFirst"===this.dataFormat?n[3]:n[2];return t=(0,G.Ol)(t,this.poolSize[0],this.padding,this.strides[0]),e=(0,G.Ol)(e,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[n[0],n[1],t,e]:[n[0],t,e,n[3]]}call(n,t){return(0,r.DZQ)(()=>(this.invokeCallHook(n,t),this.poolingFunction((0,c.un)(n),this.poolSize,this.strides,this.padding,this.dataFormat)))}getConfig(){const n={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(n,t),n}}r.JFn.registerClass((()=>{class i extends Ee{constructor(t){super(t)}poolingFunction(t,e,s,a,o){return(0,L.uM)(o),(0,L.tB)(a),kt(t,e,s,a,o,"max")}}return i.className="MaxPooling2D",i})()),r.JFn.registerClass((()=>{class i extends Ee{constructor(t){super(t)}poolingFunction(t,e,s,a,o){return(0,L.uM)(o),(0,L.tB)(a),kt(t,e,s,a,o,"avg")}}return i.className="AveragePooling2D",i})());class Fe extends g.Wd{constructor(n){if(null==n.poolSize&&(n.poolSize=[2,2,2]),super(n),this.poolSize=Array.isArray(n.poolSize)?n.poolSize:[n.poolSize,n.poolSize,n.poolSize],null==n.strides)this.strides=this.poolSize;else if(Array.isArray(n.strides)){if(3!==n.strides.length)throw new p.Qp(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${n.strides.length}.`);this.strides=n.strides}else this.strides=[n.strides,n.strides,n.strides];(0,C.oo)(this.poolSize,"poolSize"),(0,C.oo)(this.strides,"strides"),this.padding=null==n.padding?"valid":n.padding,this.dataFormat=null==n.dataFormat?"channelsLast":n.dataFormat,(0,L.uM)(this.dataFormat),(0,L.tB)(this.padding),this.inputSpec=[new g.eO({ndim:5})]}computeOutputShape(n){n=(0,c.U$)(n);let t="channelsFirst"===this.dataFormat?n[2]:n[1],e="channelsFirst"===this.dataFormat?n[3]:n[2],s="channelsFirst"===this.dataFormat?n[4]:n[3];return t=(0,G.Ol)(t,this.poolSize[0],this.padding,this.strides[0]),e=(0,G.Ol)(e,this.poolSize[1],this.padding,this.strides[1]),s=(0,G.Ol)(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[n[0],n[1],t,e,s]:[n[0],t,e,s,n[4]]}call(n,t){return(0,r.DZQ)(()=>(this.invokeCallHook(n,t),this.poolingFunction((0,c.un)(n),this.poolSize,this.strides,this.padding,this.dataFormat)))}getConfig(){const n={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(n,t),n}}r.JFn.registerClass((()=>{class i extends Fe{constructor(t){super(t)}poolingFunction(t,e,s,a,o){return(0,L.uM)(o),(0,L.tB)(a),De(t,e,s,a,o,"max")}}return i.className="MaxPooling3D",i})()),r.JFn.registerClass((()=>{class i extends Fe{constructor(t){super(t)}poolingFunction(t,e,s,a,o){return(0,L.uM)(o),(0,L.tB)(a),De(t,e,s,a,o,"avg")}}return i.className="AveragePooling3D",i})());class Oe extends g.Wd{constructor(n){super(n),this.inputSpec=[new g.eO({ndim:3})]}computeOutputShape(n){return[n[0],n[2]]}call(n,t){throw new p.EH}}let ke=(()=>{class i extends Oe{constructor(t){super(t||{})}call(t,e){return(0,r.DZQ)(()=>{const s=(0,c.un)(t);return r.i2o(s,1)})}}return i.className="GlobalAveragePooling1D",i})();r.JFn.registerClass(ke);let Qe=(()=>{class i extends Oe{constructor(t){super(t||{})}call(t,e){return(0,r.DZQ)(()=>{const s=(0,c.un)(t);return r.T9B(s,1)})}}return i.className="GlobalMaxPooling1D",i})();r.JFn.registerClass(Qe);class We extends g.Wd{constructor(n){super(n),this.dataFormat=null==n.dataFormat?"channelsLast":n.dataFormat,(0,L.uM)(this.dataFormat),this.inputSpec=[new g.eO({ndim:4})]}computeOutputShape(n){return"channelsLast"===this.dataFormat?[n[0],n[3]]:[n[0],n[1]]}call(n,t){throw new p.EH}getConfig(){const n={dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(n,t),n}}let Be=(()=>{class i extends We{call(t,e){return(0,r.DZQ)(()=>{const s=(0,c.un)(t);return r.i2o(s,"channelsLast"===this.dataFormat?[1,2]:[2,3])})}}return i.className="GlobalAveragePooling2D",i})();r.JFn.registerClass(Be);let Pe=(()=>{class i extends We{call(t,e){return(0,r.DZQ)(()=>{const s=(0,c.un)(t);return r.T9B(s,"channelsLast"===this.dataFormat?[1,2]:[2,3])})}}return i.className="GlobalMaxPooling2D",i})();r.JFn.registerClass(Pe);var cs=l(89893);class Ue extends g.Wd{constructor(n){super(n),this.layer=n.layer}build(n){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(n){null!=this.layer&&(this.layer.trainable=n)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(n){this.layer.setWeights(n)}getConfig(){const n={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},t=super.getConfig();return Object.assign(n,t),n}setFastWeightInitDuringBuild(n){super.setFastWeightInitDuringBuild(n),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(n)}static fromConfig(n,t,e={}){const a=(0,It.i)(t.layer,e);delete t.layer;const o={layer:a};return Object.assign(o,t),new n(o)}}r.JFn.registerClass((()=>{class i extends Ue{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=(0,c.U$)(t)).length<3)throw new p.Qp(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=(0,c.U$)(t))[0]].concat(t.slice(2)),s=this.layer.computeOutputShape(e);return[s[0],t[1]].concat(s.slice(1))}call(t,e){return(0,r.DZQ)(()=>ne((u,f)=>[(0,c.un)(this.layer.call(u,e)),[]],t=(0,c.un)(t),[],!1,null,null,!1,!0)[1])}}return i.className="TimeDistributed",i})());let Ze=(()=>{class i extends Ue{constructor(t){super(t);const e=t.layer.getConfig(),s={};s.className=t.layer.getClassName(),s.config=e,this.forwardLayer=(0,It.i)(s),e.goBackwards=!0!==e.goBackwards;const a={};if(a.className=t.layer.getClassName(),a.config=e,this.backwardLayer=(0,It.i)(a),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,function ds(i){C.E6(cs.r$,"BidirectionalMergeMode",i)}(this.mergeMode),t.weights)throw new p.EH("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const s=Math.floor(t.length/2);this.forwardLayer.setWeights(t.slice(0,s)),this.backwardLayer.setWeights(t.slice(s))}computeOutputShape(t){let s,a,o,e=this.forwardLayer.computeOutputShape(t);return Array.isArray(e)&&Array.isArray(e[0])||(e=[e]),this.returnState&&(o=e.slice(1)),s=e[0],"concat"===this.mergeMode?(s[s.length-1]*=2,a=[s]):a=null==this.mergeMode?[s,s.slice()]:[s],this.returnState?null==this.mergeMode?a.concat(o).concat(o.slice()):[s].concat(o).concat(o.slice()):C.wL(a)}apply(t,e){let s=null==e?null:e.initialState,a=null==e?null:e.constants;null==e&&(e={});const o=se(t,s,a,this.numConstants);if(t=o.inputs,s=o.initialState,a=o.constants,Array.isArray(t)&&(s=t.slice(1),t=t[0]),(null==s||0===s.length)&&null==a)return super.apply(t,e);const u=[],f=[];if(null!=s){const v=s.length;if(v%2>0)throw new p.Qp("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=s,u.push(...s);const x=s.map(N=>new g.eO({shape:N.shape}));this.forwardLayer.stateSpec=x.slice(0,v/2),this.backwardLayer.stateSpec=x.slice(v/2),f.push(...x)}if(null!=a)throw new p.EH("Support for constants in Bidirectional layers is not implemented yet.");const m=u[0]instanceof g.Ar;for(const v of u)if(v instanceof g.Ar!==m)throw new p.Qp("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(m){const v=[t].concat(u),x=this.inputSpec.concat(f),N=this.inputSpec;this.inputSpec=x;const F=super.apply(v,e);return this.inputSpec=N,F}return super.apply(t,e)}call(t,e){return(0,r.DZQ)(()=>{const s=e.initialState;let a,o,u,f;if(null==s)a=this.forwardLayer.call(t,e),o=this.backwardLayer.call(t,e);else{const m=s.slice(0,s.length/2),v=s.slice(s.length/2);a=this.forwardLayer.call(t,Object.assign(e,{initialState:m})),o=this.backwardLayer.call(t,Object.assign(e,{initialState:v}))}return this.returnState&&(Array.isArray(a)&&(u=a.slice(1).concat(o.slice(1))),a=a[0],o=o[0]),this.returnSequences&&(o=r.BEg(o,1)),"concat"===this.mergeMode?f=D.u1([a,o]):"sum"===this.mergeMode?f=r.WQq(a,o):"ave"===this.mergeMode?f=r.lKK(.5,r.WQq(a,o)):"mul"===this.mergeMode?f=r.lKK(a,o):null==this.mergeMode&&(f=[a,o]),this.returnState?null==this.mergeMode?f.concat(u):[f].concat(u):f})}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){(0,L.IU)(this.forwardLayer.name,()=>{this.forwardLayer.build(t)}),(0,L.IU)(this.backwardLayer.name,()=>{this.backwardLayer.build(t)}),this.built=!0}computeMask(t,e){let s;if(Array.isArray(e)&&(e=e[0]),s=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const o=this.forwardLayer.states.map(u=>null);return Array.isArray(s)?s.concat(o).concat(o):[s].concat(o).concat(o)}return s}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const s=(0,It.i)(e.layer);if(delete e.layer,null!=e.numConstants)throw new p.EH("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const a=e;return a.layer=s,new t(a)}}return i.className="Bidirectional",i})();r.JFn.registerClass(Ze),r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.scale=t.scale,this.offset=t.offset?t.offset:0}getConfig(){const t={scale:this.scale,offset:this.offset},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,r.DZQ)(()=>("float32"!==(t=(0,c.un)(t)).dtype&&(t=D.wg(t,"float32")),(0,r.WQq)((0,r.lKK)(t,this.scale),this.offset)))}}return i.className="Rescaling",i})());const{resizeBilinear:ps,cropAndResize:ms}=r.Slp;r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.height=t.height,this.width=t.width}centerCrop(t,e,s,a,o,u,f,m){return(0,r.DZQ)(()=>{let v,x=!1;const U=[e/u,s/f,(a+e)/u,(o+s)/f],j=[];3===t.rank?(x=!0,v=(0,r.t$z)([t])):v=t;for(let ht=0;ht<v.shape[0];ht++)j.push(U);const st=(0,r.OEK)(j,[j.length,4]),at=(0,r.y17)(0,j.length,1,"int32"),lt=ms(v,st,at,[a,o],"nearest");return D.wg(x?(0,c.un)((0,r.K$i)(lt)):lt,m)})}upsize(t,e,s,a){return(0,r.DZQ)(()=>{const o=ps(t,[e,s]);return D.wg(o,a)})}call(t,e){return(0,r.DZQ)(()=>{const s=(0,c.un)(t),a=s.dtype,o=s.shape,u=o[o.length-3],f=o[o.length-2];let m=0;u!==this.height&&(m=Math.floor((u-this.height)/2));let v=0;return f!==this.width&&(v=Math.floor((f-this.width)/2),0===v&&(v=1)),m>=0&&v>=0?this.centerCrop(s,m,v,this.height,this.width,u,f,a):this.upsize(t,this.height,this.width,a)})}getConfig(){const t={height:this.height,width:this.width},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){const s=(t=(0,c.U$)(t)).length-2;return t[t.length-3]=this.height,t[s]=this.width,t}}return i.className="CenterCrop",i})());var gs=l(97301);r.JFn.registerClass((()=>{class i extends g.Wd{constructor(t){super(t),this.numTokens=t.numTokens,this.outputMode=t.outputMode?t.outputMode:"multiHot"}getConfig(){const t={numTokens:this.numTokens,outputMode:this.outputMode},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){return null==(t=(0,c.U$)(t))?[this.numTokens]:"oneHot"===this.outputMode&&1!==t[t.length-1]?(t.push(this.numTokens),t):(t[t.length-1]=this.numTokens,t)}call(t,e){return(0,r.DZQ)(()=>{let s;if("int32"!==(t=(0,c.un)(t)).dtype&&(t=D.wg(t,"int32")),typeof e.countWeights<"u"){if("count"!==this.outputMode)throw new p.Qp(`countWeights is not used when outputMode !== count.\n              Received countWeights=${e.countWeights}`);s=(0,c.un)(e.countWeights)}const a=(0,r.T9B)(t),o=(0,r.jkA)(t),u=(0,r.rhj)(this.numTokens,a).bufferSync().get(0),f=(0,r.DQN)(o,0).bufferSync().get(0);if(!u||!f)throw new p.Qp(`Input values must be between 0 < values <= numTokens with numTokens=${this.numTokens}`);return gs.w(t,this.outputMode,this.numTokens,s)})}}return i.className="CategoryEncoding",i})());const Je=new Set(["bilinear","nearest"]);let Ve=(()=>{class i extends g.Wd{constructor(t){if(super(t),this.height=t.height,this.width=t.width,t.interpolation){if(!Je.has(t.interpolation))throw new p.Qp(`Invalid interpolation parameter: ${t.interpolation} is not implemented`);this.interpolation=t.interpolation}else this.interpolation="bilinear";this.cropToAspectRatio=!!t.cropToAspectRatio}computeOutputShape(t){return t=(0,c.U$)(t),[this.height,this.width,t[2]]}getConfig(){const t={height:this.height,width:this.width,interpolation:this.interpolation,cropToAspectRatio:this.cropToAspectRatio},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,r.DZQ)(()=>{const s=[this.height,this.width];if("bilinear"===this.interpolation)return r.Slp.resizeBilinear(t,s,!this.cropToAspectRatio);if("nearest"===this.interpolation)return r.Slp.resizeNearestNeighbor(t,s,!this.cropToAspectRatio);throw new Error(`Interpolation is ${this.interpolation} but only ${[...Je]} are supported`)})}}return i.className="Resizing",i})();r.JFn.registerClass(Ve);var vs=l(29947);const Ge=new Set(["bilinear","nearest"]);let Ye=(()=>{class i extends vs.W{constructor(t){super(t);const{factor:e,interpolation:s="bilinear"}=t;if(this.factor=e,Array.isArray(this.factor)&&2===this.factor.length)this.widthLower=this.factor[0],this.widthUpper=this.factor[1];else{if(Array.isArray(this.factor)||!(this.factor>0))throw new p.Qp(`Invalid factor: ${this.factor}. Must be positive number or tuple of 2 numbers`);this.widthLower=-this.factor,this.widthUpper=this.factor}if(this.widthLower<-1||this.widthUpper<-1)throw new p.Qp(`factor must have values larger than -1. Got: ${this.factor}`);if(this.widthUpper<this.widthLower)throw new p.Qp(`factor cannot have upper bound less than lower bound.\n        Got upper bound: ${this.widthUpper}.\n        Got lower bound: ${this.widthLower}\n      `);if(s){if(!Ge.has(s))throw new p.Qp(`Invalid interpolation parameter: ${s} is not implemented`);this.interpolation=s}}getConfig(){const t={factor:this.factor,interpolation:this.interpolation},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){return t=(0,c.U$)(t),[this.imgHeight,-1,t[2]]}call(t,e){return(0,r.DZQ)(()=>{const s=(0,c.un)(t);this.imgHeight=s.shape[s.shape.length-3];const a=s.shape[s.shape.length-2];this.widthFactor=(0,r.YeY)([1],1+this.widthLower,1+this.widthUpper,"float32",this.randomGenerator.next());let o=this.widthFactor.dataSync()[0]*a;o=Math.round(o);const u=[this.imgHeight,o];switch(this.interpolation){case"bilinear":return r.Slp.resizeBilinear(t,u);case"nearest":return r.Slp.resizeNearestNeighbor(t,u);default:throw new Error(`Interpolation is ${this.interpolation}\n          but only ${[...Ge]} are supported`)}})}}return i.className="RandomWidth",i})();function Xe(i){return new ue(i)}r.JFn.registerClass(Ye),l(39416),l(40183),l(73112),l(30593),l(13509),l(30744);var Dn=l(48814),Ct=l(74042),qe=l(27669);let _e=(()=>{class i{constructor(){this.size=null}batch(t,e=!0){const s=this;let a;return r.ZSL.assert(t>0,()=>`batchSize needs to be positive, but it is\n      ${t}`),a=this.size===1/0||null==this.size?this.size:e?Math.ceil(this.size/t):Math.floor(this.size/t),dt((0,b.A)(function*(){return(yield s.iterator()).columnMajorBatch(t,e,Sn)}),a)}concatenate(t){const e=this;let s;return s=this.size===1/0||t.size===1/0?1/0:null!=this.size&&null!=t.size?this.size+t.size:null,dt((0,b.A)(function*(){return(yield e.iterator()).concatenate(yield t.iterator())}),s)}filter(t){const e=this;let s;return s=this.size===1/0?1/0:null,dt((0,b.A)(function*(){return(yield e.iterator()).filter(a=>r.DZQ(()=>t(a)))}),s)}forEachAsync(t){var e=this;return(0,b.A)(function*(){return(yield e.iterator()).forEachAsync(t)})()}map(t){const e=this;return dt((0,b.A)(function*(){return(yield e.iterator()).map(s=>r.DZQ(()=>t(s)))}),this.size)}mapAsync(t){const e=this;return dt((0,b.A)(function*(){return(yield e.iterator()).mapAsync(t)}),this.size)}prefetch(t){if(null==t)throw new RangeError("`Dataset.prefetch()` requires bufferSize to be specified.");const e=this;return dt((0,b.A)(function*(){return(yield e.iterator()).prefetch(t)}),this.size)}repeat(t){const e=this;let s;return s=null!=this.size&&t>0?this.size*t:0===t?0:null!=this.size&&(void 0===t||t<0)?1/0:null,dt((0,b.A)(function*(){const a=(0,Ct.ht)((0,b.A)(function*(){return{value:yield e.iterator(),done:!1}}));return(0,Ct.kP)(a.take(t))}),s)}skip(t){const e=this;let s;return s=null!=this.size&&t>=0&&this.size>=t?this.size-t:null!=this.size&&(this.size<t||void 0===t||t<0)?0:null,dt((0,b.A)(function*(){return(yield e.iterator()).skip(t)}),s)}shuffle(t,e,s=!0){if(null==t||t<0)throw null==this.size?new RangeError("`Dataset.shuffle()` requires bufferSize to be specified."):new RangeError(`\`Dataset.shuffle()\` requires bufferSize to be specified.  If your data fits in main memory (for regular JS objects), and/or GPU memory (for \`tf.Tensor\`s), consider setting bufferSize to the dataset size (${this.size} elements)`);const a=this,o=Dn.alea(e||r.ZSL.now().toString());return dt((0,b.A)(function*(){let u=o.int32();return s&&(u+=o.int32()),(yield a.iterator()).shuffle(t,u.toString())}),this.size)}take(t){const e=this;let s;return s=null!=this.size&&this.size>t?t:null!=this.size&&this.size<=t?this.size:null,dt((0,b.A)(function*(){return(yield e.iterator()).take(t)}),s)}toArray(){var t=this;return(0,b.A)(function*(){if(t.size===1/0)throw new Error("Can not convert infinite data stream to array.");return(yield t.iterator()).toArray()})()}toArrayForTest(){var t=this;return(0,b.A)(function*(){if(t.size===1/0)throw new Error("Can not convert infinite data stream to array.");return(yield t.iterator()).toArrayForTest()})()}}return i.MAX_BUFFER_SIZE=1e4,i})();function dt(i,n=null){return new class extends _e{constructor(){super(...arguments),this.size=n}iterator(){return(0,b.A)(function*(){return i()})()}}}function Sn(i){return null===i?null:(0,qe.mf)(i[0])?{value:xn(i),recurse:!1}:{value:null,recurse:!0}}function xn(i){if(0===i.length)throw new Error("Can't make a batch of zero elements.");return i[0]instanceof r.qYS?r.t$z(i):r.OEK(i)}Symbol("out"),Symbol("field"),Symbol("quote"),Symbol("quoteafterquote"),Symbol("quoteinquote"),l(4902),l(11033),l(41014),l(8084),l(98603),l(21750),l(22412),l(77816);function xs(){return(xs=(0,b.A)(function*(){const n=yield(yield fetch("/one-more-battery/assets/data/mnist_images.png")).blob(),t=yield createImageBitmap(n),e=yield fetch("/one-more-battery/assets/data/mnist_labels_uint8"),s=new Uint8Array(yield e.arrayBuffer()),a=s.length,o=new Float32Array(28*a*28),f=new OffscreenCanvas(28,28).getContext("2d"),m=Math.floor(t.width/28);for(let v=0;v<a;v++){const x=28*v*28,N=Math.floor(v/m);f.drawImage(t,v%m*28,28*N,28,28,0,0,28,28);const U=f.getImageData(0,0,28,28);for(let j=0;j<U.data.length/4;j++)o[x+j]=U.data[4*j]/255}return{images:o,labels:s}})).apply(this,arguments)}addEventListener("message",function(){var i=(0,b.A)(function*({}){postMessage({type:"log",message:"Setting backend to cpu..."}),yield r.jh6("cpu"),postMessage({type:"log",message:"Backend set."}),postMessage({type:"log",message:"Loading data..."});const t=performance.now(),{images:e,labels:s}=yield function Bn(){return xs.apply(this,arguments)}(),a=performance.now();postMessage({type:"log",message:`Data loaded in ${(a-t).toFixed(2)} ms.`}),postMessage({type:"log",message:"Creating model..."});const o=function Un(){const i=function _(i){return new rt.Gx(i)}();return i.add(function zs(i){return new he(i)}({inputShape:[28,28]})),i.add(Xe({units:128,activation:"relu",kernelInitializer:"varianceScaling"})),i.add(Xe({units:10,kernelInitializer:"varianceScaling",activation:"softmax"})),i.compile({optimizer:r.BaG.adam(),loss:"categoricalCrossentropy",metrics:["accuracy"]}),i}();postMessage({type:"log",message:"Model created."}),postMessage({type:"log",message:"Starting training..."});const u=function Mn(i){return dt((0,b.A)(function*(){const n=yield i();return(0,Ct.ht)(()=>n.next())}))}(()=>function*Pn(i,n){const t=n.length;for(let e=0;e<t;e++){const s=28*e*28,a=i.slice(s,s+784),o=r.KtR(a,[28,28]).reshape([1,28,28]),u=r.Mw0(r.tGX([n[e]],"int32"),10).squeeze();yield{xs:o,ys:u}}}(e,s)).shuffle(1e3).batch(512);yield o.fitDataset(u,{epochs:20,callbacks:{onEpochEnd:(f,m)=>{f%5==0&&postMessage({type:"log",message:`Epoch ${f+1}: loss = ${m.loss.toFixed(4)}, acc = ${m.acc.toFixed(4)}`})}}}),postMessage({type:"log",message:"Training complete."}),yield o.save("downloads://model"),postMessage({type:"done"})});return function(n){return i.apply(this,arguments)}}())},41653:(I,$,l)=>{"use strict";l.d($,{ljI:()=>d.ljI,Vvy:()=>d.Vvy,PH8:()=>d.PH8,OMN:()=>d.OMN,EkD:()=>d.EkD,u8Z:()=>d.u8Z,FSt:()=>d.FSt,Jp_:()=>d.Jp_,p_m:()=>d.p_m,QKF:()=>d.QKF,epO:()=>d.epO,TyE:()=>d.TyE,lxb:()=>d.lxb,zP9:()=>d.zP9,ho8:()=>d.ho8,cS:()=>d.cS,wwC:()=>d.wwC,VCH:()=>d.VCH,jAQ:()=>d.jAQ,Ik2:()=>d.Ik2,N4F:()=>d.N4F,HNs:()=>d.HNs,vj7:()=>d.vj7,KXH:()=>d.KXH,QDP:()=>d.QDP,vaV:()=>d.vaV,pr3:()=>d.pr3,$zE:()=>d.$zE,$dB:()=>d.$dB,p2J:()=>d.p2J,rFm:()=>d.rFm,jfg:()=>d.jfg,A1h:()=>d.A1h,iGz:()=>d.iGz,gC7:()=>d.gC7,Mn0:()=>d.Mn0,MnK:()=>d.MnK,MRQ:()=>d.MRQ,jj_:()=>d.jj_,nY8:()=>d.nY8,GJx:()=>ws.GJ,wNW:()=>d.wNW,TMz:()=>d.TMz,tGH:()=>d.tGH,X$8:()=>d.X$8,nVu:()=>d.nVu,ORI:()=>d.ORI,jxD:()=>d.jxD,pk0:()=>d.pk0,bP9:()=>d.bP9,XmO:()=>d.XmO,Qgm:()=>d.Qgm,Pah:()=>d.Pah,rsH:()=>d.rsH,BRl:()=>d.BRl,_s9:()=>d._s9,ox3:()=>d.ox3,ybN:()=>d.ybN,ybj:()=>d.ybj,rGP:()=>d.rGP,SQl:()=>d.SQl,BxF:()=>d.BxF,ZgB:()=>d.ZgB,ElG:()=>d.ElG,awo:()=>d.awo,i5R:()=>d.i5R,aAr:()=>d.aAr,T7M:()=>d.T7M,O4G:()=>d.O4G,mxL:()=>d.mxL,XhZ:()=>d.XhZ,lLS:()=>d.lLS,OAQ:()=>d.OAQ,lzr:()=>d.lzr,dv8:()=>d.dv8,gIW:()=>d.gIW,E3$:()=>d.E3$,iPs:()=>d.iPs,uI_:()=>ws.uI,jM4:()=>d.jM4,ToN:()=>d.ToN,X0$:()=>d.X0$,mIA:()=>d.mIA,CwD:()=>d.CwD,mnI:()=>d.mnI,tG8:()=>d.tG8,Cg$:()=>d.Cg$,RUm:()=>d.RUm,nZd:()=>d.nZd,LXA:()=>d.LXA,VAI:()=>d.VAI,t3d:()=>d.t3d,ySp:()=>d.ySp,cHb:()=>d.cHb,RXX:()=>d.RXX,TL8:()=>d.TL8,LDN:()=>d.LDN,g5A:()=>d.g5A,lNG:()=>d.lNG,LG0:()=>d.LG0,x7F:()=>d.x7F,BLA:()=>d.BLA,WT3:()=>d.WT3,xu7:()=>d.xu7,l0G:()=>d.l0G,SDM:()=>d.SDM,Zl4:()=>d.Zl4,e0f:()=>d.e0f,ylV:()=>d.ylV,urI:()=>d.urI,LWX:()=>d.LWX,ELo:()=>nt.E,mM$:()=>d.mM$,ODT:()=>d.ODT,pyJ:()=>d.pyJ,Ncv:()=>d.Ncv,kdj:()=>d.kdj,oJ2:()=>d.oJ2,CQC:()=>d.CQC,mH5:()=>d.mH5,Q6t:()=>d.Q6t,LRy:()=>d.LRy,sDr:()=>d.sDr,huO:()=>d.huO,fUj:()=>d.fUj,P_L:()=>d.P_L,R23:()=>d.R23,hgw:()=>d.hgw,FCQ:()=>d.FCQ,jOE:()=>d.jOE,XQy:()=>d.XQy,D7i:()=>d.D7i,BK4:()=>d.BK4,hVg:()=>d.hVg,TOR:()=>d.TOR,pJc:()=>d.pJc,uWl:()=>d.uWl,l6P:()=>d.l6P,u$b:()=>d.u$b,vI1:()=>d.vI1,YVe:()=>d.YVe,hql:()=>d.hql,J3C:()=>d.J3C,JiE:()=>d.JiE,rFG:()=>d.rFG,Fin:()=>d.Fin,A8B:()=>d.A8B,C8s:()=>d.C8s,BoJ:()=>d.BoJ,L6G:()=>d.L6G,DvZ:()=>d.DvZ,jgd:()=>d.jgd,Blb:()=>d.Blb,dFH:()=>d.dFH,M6A:()=>d.M6A,Ddj:()=>d.Ddj,GZp:()=>d.GZp,pnw:()=>d.pnw,UcO:()=>d.UcO,YAb:()=>d.YAb,iW0:()=>d.iW0,$jE:()=>d.$jE,PbM:()=>d.PbM,WuN:()=>d.WuN,oFs:()=>d.oFs,iuW:()=>d.iuW,qYS:()=>rt.qY,ylz:()=>rt.yl,X4r:()=>d.X4r,FAs:()=>d.FAs,TBb:()=>d.TBb,dLy:()=>d.dLy,wx0:()=>d.wx0,EwU:()=>d.EwU,dXR:()=>d.dXR,pPe:()=>d.pPe,xJ3:()=>d.xJ3,Dr:()=>d.Dr,tnl:()=>_.t,WQq:()=>Q.W,Q7R:()=>it.Q,bzn:()=>P.b,FLi:()=>R.F,$jT:()=>p.$,sub:()=>z.s,Hs:()=>wt.Hs,C0T:()=>_s,BFc:()=>w.B,kSi:()=>c.k,T5N:()=>E.T,hOW:()=>M.h,ZEY:()=>H,TaL:()=>T,ra8:()=>V.r,wgE:()=>ot.w,zQh:()=>tt.z,o8B:()=>ct.o,xWs:()=>et.x,I1m:()=>D.I,RPU:()=>L.R,O5O:()=>G.O,P1l:()=>C.P,kA9:()=>h.k,Xtf:()=>y.X,wX9:()=>O.w,IPL:()=>q.I,jIJ:()=>gt.j,aOp:()=>ut.a,Gl3:()=>Qt.G,eMq:()=>zs,ASo:()=>wt.AS,y4m:()=>Wt.y,EZY:()=>hs.E,Pqc:()=>bt.P,Hi9:()=>wt.Hi,_K2:()=>Xe._K,LCg:()=>At.L,Y12:()=>Bt.Y,oNF:()=>Pt.o,UG6:()=>Vt.U,y5U:()=>Gt.y,GSj:()=>ss.G,RIf:()=>Yt.R,cZk:()=>xe,kgh:()=>Xt.k,rhj:()=>qt.r,DQN:()=>_t.D,Slp:()=>Vs,io:()=>k,aCs:()=>wt.aC,kpo:()=>tn,H8d:()=>ns.H,mPL:()=>Gs,Rm2:()=>te.R,Kko:()=>X.K,HPB:()=>ee.H,VZ:()=>It.V,n76:()=>se.n,NoW:()=>ne.N,T9B:()=>Dt.T,jgi:()=>Rt.j,NYV:()=>Ut.e,PhQ:()=>ie.P,i2o:()=>$t.i,m1Z:()=>wt.m1,jkA:()=>re.j,BpO:()=>Et.B,Clk:()=>ae.C,lKK:()=>Zt.l,HZy:()=>vt.H,dA1:()=>qs.d,Ec:()=>is.E,Mw0:()=>Ls.M,SaS:()=>rs.S,P61:()=>Kt.P,eVF:()=>oe.e,n7C:()=>jt.n,NsG:()=>le.N,FE$:()=>ue.F,YeY:()=>he.Y,y17:()=>ce.y,gJX:()=>wt.gJ,tAK:()=>Xs.tA,VVh:()=>de.V,tQQ:()=>fe.t,BEg:()=>pe.B,d_2:()=>me.d,WfX:()=>ge.W,wdz:()=>ye.w,JFn:()=>A,jh6:()=>wt.jh,ry7:()=>zt.r,dik:()=>Tt.d,Q$M:()=>Ms.Q,zAd:()=>Nt.z,wck:()=>Os.w,R0O:()=>Ft.R,Kro:()=>B,Vs9:()=>ks.V,lw0:()=>Lt.l,lDo:()=>Qs.l,RZD:()=>Mt.R,r2V:()=>Ws.r,t$z:()=>Ot.t,jbE:()=>Bs.j,czq:()=>St.c,chL:()=>S.ch,ymU:()=>as.y,OEK:()=>ve.O,tGX:()=>be.t,KtR:()=>ze.K,d_S:()=>K,DZQ:()=>wt.DZ,Vsq:()=>we.V,BaG:()=>Ys.B,mgz:()=>Ps.m,efE:()=>xt.e,K$i:()=>os.K,TuY:()=>S.Tu,ZSL:()=>g,bvq:()=>ls.b,bgA:()=>Z.r,_M9:()=>us._,Ul9:()=>Ce.U,POl:()=>Ae.P}),l(80225);var r=l(29609),k=l(3821),H=l(9269),T=l(25905),B=l(1986),A=l(1506),K=l(14548),g=l(21710),Z=l(97762),nt=l(56188),rt=l(73444),S=l(82891),_=l(72009),Q=l(83034),it=l(56682),P=l(37547),R=l(71084),p=l(37434),z=l(36806),w=l(44353),c=l(46884),E=l(77807),M=l(3752),V=l(75987),ot=l(92290),tt=l(70581),ct=l(61548),et=l(35213),D=l(35317),L=l(74810),G=l(36183),C=l(18380),h=l(53054),y=l(12809),O=l(83521),q=l(37740),gt=l(2772),ut=l(26193),Qt=l(73595),Wt=l(4350),bt=l(41325),At=l(68833),Bt=l(78650),Pt=l(68326),Vt=l(42103),Gt=l(45104),ss=l(7684),Yt=l(41467),Xt=l(38837),qt=l(54807),_t=l(94300),ns=l(93580),te=l(8535),X=l(72954),ee=l(89578),It=l(17391),se=l(21292),ne=l(60314),Dt=l(56619),Rt=l(40044),Ut=l(42524),ie=l(74659),$t=l(23444),re=l(82825),Et=l(85233),ae=l(11732),Zt=l(59731),vt=l(43267),is=l(66279),Ls=l(87073),rs=l(95032),Kt=l(28692),oe=l(54472),jt=l(53827),le=l(62655),ue=l(41222),he=l(65413),ce=l(16412),de=l(98611),fe=l(94399),pe=l(21067),me=l(10829),ge=l(26966),ye=l(48285),zt=l(53205),Tt=l(94063),Ms=l(14638),Nt=l(81209),Os=l(29660),Ft=l(24167),ks=l(52431),Lt=l(14181),Qs=l(91517),Mt=l(51583),Ws=l(41621),Ot=l(92885),Bs=l(59279),St=l(66342),as=l(86580),ve=l(74544),be=l(48449),ze=l(41494),we=l(36409),xt=l(3609),os=l(48256),ls=l(38121),us=l(56914),Ce=l(20218),Ae=l(44714),Ps=l(77914),hs=l(45733),xe=(l(40638),l(90694),l(15245),l(43),l(63552)),Fe=(l(79208),l(92822),l(32395),l(76221),l(27958)),Le=l(99587),Me=l(60926),Oe=l(70644),ke=l(76333),Qe=l(30561),We=l(50110),Be=l(31393),Pe=l(47774),cs=l(4594),Ue=l(33601),$e=l(26486),ds=l(91719),fs=l(65425),Ze=l(59339),Ke=l(55244),ps=l(52325),ms=l(92974);l(88476),l(24782),l(36170),l(45026),l(29655),l(3733),l(33115),l(79780),l(33030),l(19702),l(56637),l(46764),l(46782),l(37500),l(74305),l(45629),l(41342);const Vs={flipLeftRight:Le.n,grayscaleToRGB:Me.C,resizeNearestNeighbor:ds.b,resizeBilinear:$e.v,rgbToGrayscale:Oe.S,rotateWithOffset:ke.x,cropAndResize:Fe.C,nonMaxSuppression:Qe.L,nonMaxSuppressionAsync:We.z,nonMaxSuppressionWithScore:Be.f,nonMaxSuppressionWithScoreAsync:Pe.l,nonMaxSuppressionPadded:cs.H,nonMaxSuppressionPaddedAsync:Ue.R,threshold:fs.E,transform:Ze.p},Gs={bandPart:Ke.x,gramSchmidt:ps.i,qr:ms.qr};var Ys=l(19057),wt=l(47054),Xs=l(56203),Xe=l(25271),qs=l(9173),_s=l(47531),zs=l(6757),tn=l(89191),ws=l(86614),d=l(30162);(0,r.i)()},74042:(I,$,l)=>{"use strict";l.d($,{DJ:()=>Q,Fb:()=>_,ZI:()=>g,dv:()=>et,ht:()=>nt,kP:()=>rt,lh:()=>ot});var b=l(10467),r=l(41653),k=l(48814),T=l(58034),B=l(27669),A=l(52531),K=l(92357);function g(C){return new it(C)}function nt(C){return new P(C)}function rt(C,h){return new ct(C,h)}function _(C,h=et.FAIL){return new D(C,h)}class Q{toArray(){var h=this;return(0,b.A)(function*(){const y=[];let O=yield h.next();for(;!O.done;)y.push(O.value),O=yield h.next();return y})()}toArrayForTest(){var h=this;return(0,b.A)(function*(){const y=h.prefetch(100),O=[];let q=yield y.next();for(;!q.done;)O.push(q.value),q=yield y.next();return O})()}resolveFully(){var h=this;return(0,b.A)(function*(){let y=yield h.next();for(;!y.done;)y=yield h.next()})()}resolveWhile(h){var y=this;return(0,b.A)(function*(){let O=yield y.next(),q=h(O.value);for(;!O.done&&q;)O=yield y.next(),q=h(O.value)})()}handleErrors(h){return new M(this,h)}filter(h){return new c(this,h)}map(h){return new E(this,h)}mapAsync(h){return new V(this,h)}serialMapAsync(h){return new V(this,h).serial()}flatmap(h){return new tt(this,h)}forEachAsync(h){var y=this;return(0,b.A)(function*(){return y.map(h).resolveFully()})()}serialForEach(h){var y=this;return(0,b.A)(function*(){return y.serialMapAsync(h).resolveWhile(O=>!0===O)})()}rowMajorBatch(h,y=!0){return new w(this,h,y)}columnMajorBatch(h,y=!0,O=B.rN){return this.rowMajorBatch(h,y).map(gt=>(0,B.sy)(gt,O))}concatenate(h,y){return new ct(g([this,h]),y)}take(h){return h<0||null==h?this:new z(this,h)}skip(h){return h<0||null==h?this:new p(this,h)}prefetch(h){return new L(this,h)}shuffle(h,y){return new G(this,h,y)}serial(){return new R(this)}}class it extends Q{constructor(h){super(),this.items=h,this.trav=0}summary(){return`Array of ${this.items.length} items`}next(){var h=this;return(0,b.A)(function*(){if(h.trav>=h.items.length)return{value:null,done:!0};const y=h.items[h.trav];return h.trav++,{value:(0,T.G)(y),done:!1}})()}}class P extends Q{constructor(h){super(),this.nextFn=h}summary(){return"Function call"}next(){var h=this;return(0,b.A)(function*(){try{return h.nextFn()}catch(y){throw y.message=`Error thrown while iterating through a dataset: ${y.message}`,y}})()}}class R extends Q{constructor(h){super(),this.upstream=h,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> Serial`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){return h.upstream.next()})()}}class p extends Q{constructor(h,y){super(),this.upstream=h,this.maxCount=y,this.count=0,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> Skip`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){for(;h.count++<h.maxCount;){const y=yield h.upstream.next();if(y.done)return y;r.ASo(y.value)}return h.upstream.next()})()}}class z extends Q{constructor(h,y){super(),this.upstream=h,this.maxCount=y,this.count=0}summary(){return`${this.upstream.summary()} -> Take`}next(){var h=this;return(0,b.A)(function*(){return h.count++>=h.maxCount?{value:null,done:!0}:h.upstream.next()})()}}class w extends Q{constructor(h,y,O=!0){super(),this.upstream=h,this.batchSize=y,this.enableSmallLastBatch=O,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> RowMajorBatch`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){const y=[];for(;y.length<h.batchSize;){const O=yield h.upstream.next();if(O.done)return h.enableSmallLastBatch&&y.length>0?{value:y,done:!1}:{value:null,done:!0};y.push(O.value)}return{value:y,done:!1}})()}}class c extends Q{constructor(h,y){super(),this.upstream=h,this.predicate=y,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> Filter`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){for(;;){const y=yield h.upstream.next();if(y.done||h.predicate(y.value))return y;r.ASo(y.value)}})()}}class E extends Q{constructor(h,y){super(),this.upstream=h,this.transform=y}summary(){return`${this.upstream.summary()} -> Map`}next(){var h=this;return(0,b.A)(function*(){const y=yield h.upstream.next();if(y.done)return{value:null,done:!0};const O=r.d_S.getTensorsInContainer(y.value),q=h.transform(y.value),gt=r.d_S.getTensorsInContainer(q);for(const ut of O)r.d_S.isTensorInList(ut,gt)||ut.dispose();return{value:q,done:!1}})()}}class M extends Q{constructor(h,y){super(),this.upstream=h,this.handler=y,this.count=0,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return`${this.upstream.summary()} -> handleErrors`}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){for(;;)try{return yield h.upstream.next()}catch(y){if(!h.handler(y))return{value:null,done:!0}}})()}}class V extends Q{constructor(h,y){super(),this.upstream=h,this.transform=y}summary(){return`${this.upstream.summary()} -> AsyncMap`}next(){var h=this;return(0,b.A)(function*(){const y=yield h.upstream.next();if(y.done)return{value:null,done:!0};const O=r.d_S.getTensorsInContainer(y.value),q=yield h.transform(y.value),gt=r.d_S.getTensorsInContainer(q);for(const ut of O)r.d_S.isTensorInList(ut,gt)||ut.dispose();return{value:q,done:!1}})()}}class ot extends Q{constructor(){super(),this.outputQueue=new A.g,this.lastRead=Promise.resolve({value:null,done:!1})}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}serialNext(){var h=this;return(0,b.A)(function*(){for(;0===h.outputQueue.length();)if(!(yield h.pump()))return{value:null,done:!0};return{value:h.outputQueue.shift(),done:!1}})()}}class tt extends ot{constructor(h,y){super(),this.upstream=h,this.transform=y}summary(){return`${this.upstream.summary()} -> Flatmap`}pump(){var h=this;return(0,b.A)(function*(){const y=yield h.upstream.next();if(y.done)return!1;const O=r.d_S.getTensorsInContainer(y.value),q=h.transform(y.value),gt=r.d_S.getTensorsInContainer(q);h.outputQueue.pushAll(q);for(const ut of O)r.d_S.isTensorInList(ut,gt)||ut.dispose();return!0})()}}class ct extends Q{constructor(h,y){super(),this.baseErrorHandler=y,this.lastRead=null,this.iterator=null,this.moreIterators=h}summary(){return"TODO: fill in upstream of chained summaries -> Chained"}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.readFromChain(h.lastRead),h.lastRead})()}readFromChain(h){var y=this;return(0,b.A)(function*(){if(yield h,null==y.iterator){const q=yield y.moreIterators.next();if(q.done)return{value:null,done:!0};y.iterator=q.value,null!=y.baseErrorHandler&&(y.iterator=y.iterator.handleErrors(y.baseErrorHandler))}const O=yield y.iterator.next();return O.done?(y.iterator=null,y.readFromChain(h)):O})()}}var et=function(C){return C[C.FAIL=0]="FAIL",C[C.SHORTEST=1]="SHORTEST",C[C.LONGEST=2]="LONGEST",C}(et||{});class D extends Q{constructor(h,y=et.FAIL){super(),this.iterators=h,this.mismatchMode=y,this.count=0,this.currentPromise=null}summary(){return"{TODO: fill in upstream of zip summaries} -> Zip"}nextState(h){var y=this;return(0,b.A)(function*(){yield h;let O=0,q=0;const ut=yield(0,B.te)(y.iterators,function gt(Qt){return Qt instanceof Q?{value:Qt.next().then(bt=>(O++,bt.done&&q++,bt.value)),recurse:!1}:{value:null,recurse:!0}});if(O===q)return{value:null,done:!0};if(q>0)switch(y.mismatchMode){case et.FAIL:throw new Error(`Zipped streams should have the same length. Mismatched at element ${y.count}.`);case et.SHORTEST:return{value:null,done:!0}}return y.count++,{value:ut,done:!1}})()}next(){var h=this;return(0,b.A)(function*(){return h.currentPromise=h.nextState(h.currentPromise),h.currentPromise})()}}class L extends Q{constructor(h,y){super(),this.upstream=h,this.bufferSize=y,this.buffer=new K.N(y)}summary(){return`${this.upstream.summary()} -> Prefetch`}refill(){for(;!this.buffer.isFull();){const h=this.upstream.next();this.buffer.push(h)}}next(){return this.refill(),this.buffer.shift()}}class G extends L{constructor(h,y,O){super(h,y),this.upstream=h,this.windowSize=y,this.upstreamExhausted=!1,this.random=k.alea(O||r.ZSL.now().toString()),this.lastRead=Promise.resolve({value:null,done:!1})}next(){var h=this;return(0,b.A)(function*(){return h.lastRead=h.lastRead.then(()=>h.serialNext()),h.lastRead})()}randomInt(h){return Math.floor(this.random()*h)}chooseIndex(){return this.randomInt(this.buffer.length())}serialNext(){var h=this;return(0,b.A)(function*(){for(h.upstreamExhausted||h.refill();!h.buffer.isEmpty();){const y=h.chooseIndex(),O=yield h.buffer.shuffleExcise(y);if(!O.done)return h.refill(),O;h.upstreamExhausted=!0}return{value:null,done:!0}})()}}},27669:(I,$,l)=>{"use strict";l.d($,{Bl:()=>k,mf:()=>nt,rN:()=>A,sy:()=>T,te:()=>K,xZ:()=>Z});var b=l(10467),r=l(41653);function k(S,_){return H(S,_)}function H(S,_,Q=new Map,it=new Set){if(null==S)return null;if("function"==typeof Blob&&S instanceof Blob)return S.slice();if(it.has(S))throw new Error("Circular references are not supported.");if(Q.has(S))return Q.get(S);const P=_(S);if(P.recurse&&null!==P.value)throw new Error("A deep map function may not return both a value and recurse=true.");if(P.recurse){if(Z(S)){const R=Array.isArray(S)?[]:{};it.add(S);for(const p in S){const w=H(S[p],_,Q,it);R[p]=w}return it.delete(S),S.__proto__&&(R.__proto__=S.__proto__),R}throw new Error(`Can't recurse into non-iterable type: ${S}`)}return Q.set(S,P.value),P.value}function T(S,_=A){return B(S,_)}function B(S,_,Q=new Set){const it=S[0];if(Q.has(it))throw new Error("Circular references are not supported.");const P=_(S);if(P.recurse&&null!==P.value)throw new Error("A deep zip function may not return both a value and recurse=true.");if(P.recurse){if(Z(it)){const R=Array.isArray(it)?[]:{};Q.add(it);for(const p in it){const w=B(S.map(c=>c[p]),_,Q);R[p]=w}return Q.delete(it),R}throw new Error(`Can't recurse into non-iterable type: ${it}`)}return P.value}function A(S){return null===S?null:Z(S[0])?{value:null,recurse:!0}:{value:S,recurse:!1}}function K(S,_){return g.apply(this,arguments)}function g(){return(g=(0,b.A)(function*(S,_){const Q=new Map;H(S,_,Q);for(const P of Array.from(Q.keys())){const R=Q.get(P);if(r.ZSL.isPromise(R)){const p=yield R;Q.set(P,p)}}return H(S,_,Q)})).apply(this,arguments)}function Z(S){let _=!1;if(r._K2().get("IS_BROWSER"))_=S instanceof TextDecoder;else{const{StringDecoder:Q}=l(80551);_=S instanceof Q}return null!=S&&!ArrayBuffer.isView(S)&&(Array.isArray(S)||"object"==typeof S&&!(S instanceof r.qYS)&&!(S instanceof Promise)&&!_)}function nt(S){return null==S||function rt(S){return null===S||"object"!=typeof S&&"function"!=typeof S}(S)||Array.isArray(S)||"object"==typeof S&&S instanceof r.qYS||r.ZSL.isTypedArray(S)}},91806:(I,$,l)=>{"use strict";l.d($,{m:()=>T,p:()=>B});var b=l(41653),r=l(29887),k=l(42946),H=l(22919);let T=(()=>{class A extends H.Wd{constructor(g){if(super({dtype:g.dtype,name:null!=g.name?g.name:(0,r.v)("input").toString()}),null==g.batchSize&&(g.batchSize=null),null==g.sparse&&(g.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=g.sparse,null!=g.inputShape&&null!=g.batchInputShape)throw new k.Qp("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let Z=g.batchInputShape;if(null==Z){if(null==g.inputShape)throw new k.Qp("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");Z=[g.batchSize].concat(g.inputShape)}else if(null!=g.batchSize)throw new k.Qp("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const nt=g.dtype||"float32";this.batchInputShape=Z,this.dtype=nt,this.inputSpec=[{shape:Z}];const rt=new H.Ar(this.dtype,this.batchInputShape,this,[],{},this.name);rt.nodeIndex=0,rt.tensorIndex=0,new H.bP({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[rt],outputTensors:[rt],inputMasks:[null],outputMasks:[null],inputShapes:[Z],outputShapes:[Z]})}apply(g,Z){throw new k.Qp(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}return A.className="InputLayer",A})();function B(A){if(null==A.batchShape&&null==A.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=A.batchShape&&null!=A.shape)throw new k.Qp("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let K=A.batchShape;null!=A.shape&&null==K&&(K=[null].concat(A.shape));let g=A.dtype;return null==g&&(g="float32"),new T({batchInputShape:K,name:A.name,dtype:g,sparse:A.sparse}).inboundNodes[0].outputTensors[0]}b.JFn.registerClass(T)},48631:(I,$,l)=>{"use strict";l.d($,{Gx:()=>p});var b=l(10467),r=l(41653),k=l(29887),H=l(91806),T=l(22919),B=l(31542),A=l(42946),K=l(53340),g=l(24503),nt=(l(13425),l(71936));let p=(()=>{class z extends B.Gw{constructor(c){if(super({inputs:[],outputs:[]}),c=c||{},this.trainable=!0,this.built=!1,this.name=null!=c.name?c.name:(0,k.v)("sequential_"),null!=c.layers)for(const E of c.layers)this.add(E)}checkShape(c){if(c.inboundNodes[0].outputTensors[0].shape.some(M=>M<0))throw new A.Qp(`Negative dimension size caused by adding layer ${c.name} with input shape [${c.inboundNodes[0].inputTensors[0].shape}]`)}add(c){const E=c instanceof z||c instanceof B.Gw;let M;if(E){if(M=c,1!==M.outputs.length)throw new A.Qp("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==M.inputs.length)throw new A.Qp("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===c.inboundNodes.length){if(null==c.batchInputShape)throw new A.Qp("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const V=(0,H.p)({batchShape:c.batchInputShape,dtype:c.dtype,name:c.name+"_input"});c.apply(V)}if(E)this.outputs=M.outputs,this.inputs=M.inputs;else{if(1!==c.inboundNodes.length)throw new A.Qp(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${c.name} which has ${c.inboundNodes.length} pre-existing inbound connections.`);if(1!==c.inboundNodes[0].outputTensors.length)throw new A.Qp("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(c),this.outputs=[c.inboundNodes[0].outputTensors[0]],this.inputs=(0,T.X6)(this.outputs[0])}this.inboundNodes=[],new T.bP({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:g.fD(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map(V=>V.shape),outputShapes:this.outputs[0].shape})}else{const V=c.apply(this.outputs[0]);if(Array.isArray(V))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(c),this.outputs=[V],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(c),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const c=this.layers.length-1;this.layers[c].outboundNodes=[],this.outputs=[this.layers[c].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(c,E){return null==this.model&&this.build(),this.model.call(c,E)}build(c){if((0,nt.U$)(c),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new B.Gw({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(c,E,M=console.log){this.built||this.build(),super.summary(c,E,M)}setWeights(c){null==this.model&&this.build(),this.model.setWeights(c)}evaluate(c,E,M={}){if(!this.built)throw new A.bu("The model needs to be compiled before being used.");return this.model.evaluate(c,E,M)}evaluateDataset(c,E){var M=this;return(0,b.A)(function*(){if(!M.built)throw new A.bu("The model needs to be compiled before being used.");return M.model.evaluateDataset(c,E)})()}predict(c,E={}){return null==this.model&&this.build(),this.model.predict(c,E)}predictOnBatch(c){return null==this.model&&this.build(),this.model.predictOnBatch(c)}compile(c){this.build(),this.model.compile(c),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(c){this.model.optimizer=c}fit(c,E){var M=this;return(0,b.A)(function*(V,ot,tt={}){if(!M.built)throw new A.bu("The model needs to be compiled before being used.");return M.model.fit(V,ot,tt)}).apply(this,arguments)}fitDataset(c,E){var M=this;return(0,b.A)(function*(){if(!M.built)throw new A.bu("The model needs to be compiled before being used.");return M.model.fitDataset(c,E)})()}trainOnBatch(c,E){var M=this;return(0,b.A)(function*(){return M.model.trainOnBatch(c,E)})()}static fromConfig(c,E,M={},V=!1){let ot,tt={};if(E instanceof Array){if(null==E[0].className||"Merge"===E[0].className)throw new A.Qp("Legacy serialization format not supported yet.");ot=E}else r.ZSL.assert(null!=E.layers,()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field."),ot=E.layers,delete E.layers,tt=E;const ct=new c(tt);if(!(ct instanceof z))throw new A.EH(`Sequential.fromConfig called on non-Sequential input: ${ct}`);for(const et of ot){const L=(0,K.i)(et,void 0,V);V&&L.setFastWeightInitDuringBuild(!0),ct.add(L)}return ct}set stopTraining(c){if(null==this.model)throw new A.Qp("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=c}get stopTraining(){if(null==this.model)throw new A.Qp("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const c=[];for(const E of this.layers){const M={};M.className=E.getClassName(),M.config=E.getConfig(),c.push(M)}return{name:this.name,layers:c}}}return z.className="Sequential",z})();r.JFn.registerClass(p)},13425:(I,$,l)=>{"use strict";l.d($,{M:()=>H,w:()=>k});var b=l(24503);function r(T,B,A){return("inboundNodes"===T||"outputLayers"===T||"inputLayers"===T)&&0===B&&"string"==typeof A}function k(T,B){if(null===T)return null;if("string"==typeof T)return b.Cb(T);if("number"==typeof T||"boolean"==typeof T)return T;if(T instanceof Array){const A=[],K=T.length;for(let g=0;g<K;++g){const Z=T[g];r(B,g,Z)?A.push(Z):A.push(k(Z,B))}return A}{const A={};for(const K of Object.keys(T)){const g=T[K];if("name"===K&&"string"==typeof g)A[K]=g;else{const Z=b.Cb(K);A[Z]=k(g,Z)}}return A}}function H(T,B){if(null==T)return null;if("string"==typeof T)return b.uc(T);if("number"==typeof T||"boolean"==typeof T)return T;if(T instanceof Array){const A=[],K=T.length;for(let g=0;g<K;++g){const Z=T[g];r(B,g,Z)?A.push(Z):A.push(H(Z,B))}return A}{const A={};for(const K of Object.keys(T)){const g=T[K];A[b.uc(K)]="name"!==K&&"className"!==K||"string"!=typeof g?H(g,K):g}return A}}},48814:(I,$,l)=>{var b=l(2495),r=l(57850),k=l(85704),H=l(88114),T=l(79040),B=l(84478),A=l(97454);A.alea=b,A.xor128=r,A.xorwow=k,A.xorshift7=H,A.xor4096=T,A.tychei=B,I.exports=A},10467:(I,$,l)=>{"use strict";function b(k,H,T,B,A,K,g){try{var Z=k[K](g),nt=Z.value}catch(rt){return void T(rt)}Z.done?H(nt):Promise.resolve(nt).then(B,A)}function r(k){return function(){var H=this,T=arguments;return new Promise(function(B,A){var K=k.apply(H,T);function g(nt){b(K,B,A,g,Z,"next",nt)}function Z(nt){b(K,B,A,g,Z,"throw",nt)}g(void 0)})}}l.d($,{A:()=>r})}},Fs={};function W(I){var $=Fs[I];if(void 0!==$)return $.exports;var l=Fs[I]={id:I,loaded:!1,exports:{}};return Ns[I].call(l.exports,l,l.exports,W),l.loaded=!0,l.exports}W.m=Ns,W.x=()=>{var I=W.O(void 0,[6599],()=>W(18633));return W.O(I)},W.amdD=function(){throw new Error("define cannot be used indirect")},W.amdO={},I=[],W.O=($,l,b,r)=>{if(!l){var H=1/0;for(k=0;k<I.length;k++){for(var[l,b,r]=I[k],T=!0,B=0;B<l.length;B++)(!1&r||H>=r)&&Object.keys(W.O).every(rt=>W.O[rt](l[B]))?l.splice(B--,1):(T=!1,r<H&&(H=r));if(T){I.splice(k--,1);var A=b();void 0!==A&&($=A)}}return $}r=r||0;for(var k=I.length;k>0&&I[k-1][2]>r;k--)I[k]=I[k-1];I[k]=[l,b,r]},W.n=I=>{var $=I&&I.__esModule?()=>I.default:()=>I;return W.d($,{a:$}),$},W.d=(I,$)=>{for(var l in $)W.o($,l)&&!W.o(I,l)&&Object.defineProperty(I,l,{enumerable:!0,get:$[l]})},W.f={},W.e=I=>Promise.all(Object.keys(W.f).reduce(($,l)=>(W.f[l](I,$),$),[])),W.u=I=>I+".d5ff5767ff411a13.js",W.miniCssF=I=>{},W.o=(I,$)=>Object.prototype.hasOwnProperty.call(I,$),W.r=I=>{typeof Symbol<"u"&&Symbol.toStringTag&&Object.defineProperty(I,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(I,"__esModule",{value:!0})},W.nmd=I=>(I.paths=[],I.children||(I.children=[]),I),(()=>{var I;W.tt=()=>(void 0===I&&(I={createScriptURL:$=>$},typeof trustedTypes<"u"&&trustedTypes.createPolicy&&(I=trustedTypes.createPolicy("angular#bundler",I))),I)})(),W.tu=I=>W.tt().createScriptURL(I),W.p="",(()=>{var I={8633:1};W.f.i=(r,k)=>{I[r]||importScripts(W.tu(W.p+W.u(r)))};var l=self.webpackChunkapp=self.webpackChunkapp||[],b=l.push.bind(l);l.push=r=>{var[k,H,T]=r;for(var B in H)W.o(H,B)&&(W.m[B]=H[B]);for(T&&T(W);k.length;)I[k.pop()]=1;b(r)}})(),(()=>{var I=W.x;W.x=()=>W.e(6599).then(I)})(),W.x()})();